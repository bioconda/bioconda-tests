Updating build index: /home/vsts/conda/conda-bld

TEST START: qiime-1.9.1-py_3.tar.bz2
Adding in variants from internal_defaults
Adding in variants from /tmp/tmpj49wmwo8/info/recipe/conda_build_config.yaml
  Package                                  Version  Build                Channel                                   Size
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
  Install:
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

  + _libgcc_mutex                              0.1  conda_forge          conda-forge/linux-64                    Cached
  + _openmp_mutex                              4.5  1_gnu                conda-forge/linux-64                    Cached
  + _r-mutex                                 1.0.1  anacondar_1          conda-forge/noarch                        3 KB
  + backports                                  1.0  py27_1               conda-forge/linux-64                      4 KB
  + backports.functools_lru_cache            1.6.1  py_0                 conda-forge/noarch                        8 KB
  + backports.shutil_get_terminal_size       1.0.0  py27_1               conda-forge/linux-64                      7 KB
  + backports_abc                              0.5  py27_0               conda-forge/linux-64                      6 KB
  + biom-format                              2.1.7  py27hc1659b7_1002    conda-forge/linux-64                     11 MB
  + burrito                                  0.9.1  py27r3.4.1_1         bioconda/linux-64                        63 KB
  + burrito-fillings                         0.1.1  py27_1               bioconda/linux-64                       356 KB
  + bzip2                                    1.0.8  h7f98852_4           conda-forge/linux-64                    Cached
  + ca-certificates                      2021.5.30  ha878542_0           conda-forge/linux-64                    Cached
  + cairo                                  1.14.12  ha4e643d_1006        conda-forge/linux-64                      1 MB
  + certifi                             2019.11.28  py27h8c360ce_1       conda-forge/linux-64                    149 KB
  + click                                    7.1.2  pyh9f0ad1d_0         conda-forge/noarch                       64 KB
  + cogent                                   1.5.3  py27h1341992_2       bioconda/linux-64                         2 MB
  + curl                                    7.61.0  h93b3f91_2           conda-forge/linux-64                    859 KB
  + cycler                                  0.10.0  py27_0               conda-forge/linux-64                     13 KB
  + dbus                                    1.13.6  he372182_0           conda-forge/linux-64                    602 KB
  + decorator                                4.4.2  py_0                 conda-forge/noarch                       11 KB
  + emperor                                 0.9.51  py27_0               bioconda/linux-64                       401 KB
  + enum34                                  1.1.10  py27h8c360ce_1       conda-forge/linux-64                     49 KB
  + expat                                    2.4.1  h9c3ff4c_0           conda-forge/linux-64                    182 KB
  + fontconfig                              2.13.1  hba837de_1005        conda-forge/linux-64                    357 KB
  + freetype                                2.10.4  h0708190_1           conda-forge/linux-64                    890 KB
  + functools32                            3.2.3.2  py27_2               conda-forge/linux-64                     22 KB
  + future                                  0.18.2  py27h8c360ce_1       conda-forge/linux-64                    722 KB
  + futures                                  3.3.0  py27h8c360ce_1       conda-forge/linux-64                     26 KB
  + gdata                                   2.0.18  py27_0               pkgs/main/linux-64                      720 KB
  + gettext                               0.19.8.1  hf34092f_1004        conda-forge/linux-64                      4 MB
  + glib                                    2.58.3  py27he9b9f4b_1003    conda-forge/linux-64                      3 MB
  + graphite2                               1.3.13  h58526e2_1001        conda-forge/linux-64                    102 KB
  + gsl                                      2.2.1  h0c605f7_3           pkgs/main/linux-64                        2 MB
  + gst-plugins-base                        1.14.5  h0935bb2_2           conda-forge/linux-64                      7 MB
  + gstreamer                               1.14.5  h36ae1b5_2           conda-forge/linux-64                      4 MB
  + harfbuzz                                 1.8.8  hffaf4a1_0           pkgs/main/linux-64                      507 KB
  + icu                                       58.2  hf484d3e_1000        conda-forge/linux-64                     23 MB
  + ipython                                  5.8.0  py27_1               conda-forge/linux-64                      1 MB
  + ipython_genutils                         0.2.0  py27_0               conda-forge/linux-64                     35 KB
  + jbig                                       2.1  h7f98852_2003        conda-forge/linux-64                     43 KB
  + jpeg                                        9d  h36c2ea0_0           conda-forge/linux-64                    264 KB
  + kiwisolver                               1.1.0  py27h9e3301b_1       conda-forge/linux-64                     88 KB
  + krb5                                    1.14.6  0                    conda-forge/linux-64                      4 MB
  + lerc                                     2.2.1  h9c3ff4c_0           conda-forge/linux-64                    213 KB
  + libblas                                  3.9.0  8_openblas           conda-forge/linux-64                     11 KB
  + libcblas                                 3.9.0  8_openblas           conda-forge/linux-64                     11 KB
  + libdeflate                                 1.7  h7f98852_5           conda-forge/linux-64                     67 KB
  + libffi                                   3.2.1  he1b5a44_1007        conda-forge/linux-64                     47 KB
  + libgcc-ng                               11.1.0  hc902ee8_8           conda-forge/linux-64                    Cached
  + libgfortran                              3.0.0  1                    conda-forge/linux-64                    281 KB
  + libgfortran-ng                           7.5.0  h14aa051_19          conda-forge/linux-64                     22 KB
  + libgfortran4                             7.5.0  h14aa051_19          conda-forge/linux-64                      1 MB
  + libgomp                                 11.1.0  hc902ee8_8           conda-forge/linux-64                    Cached
  + libiconv                                  1.16  h516909a_0           conda-forge/linux-64                    Cached
  + liblapack                                3.9.0  8_openblas           conda-forge/linux-64                     11 KB
  + libopenblas                             0.3.12  pthreads_hb3c22a3_1  conda-forge/linux-64                      8 MB
  + libpng                                  1.6.37  h21135ba_2           conda-forge/linux-64                    306 KB
  + libssh2                                  1.8.0  h1ad7b7a_1003        conda-forge/linux-64                    246 KB
  + libstdcxx-ng                            11.1.0  h56837e0_8           conda-forge/linux-64                    Cached
  + libtiff                                  4.3.0  hf544144_1           conda-forge/linux-64                    668 KB
  + libuuid                                 2.32.1  h7f98852_1000        conda-forge/linux-64                     28 KB
  + libwebp-base                             1.2.1  h7f98852_0           conda-forge/linux-64                    845 KB
  + libxcb                                    1.13  h7f98852_1003        conda-forge/linux-64                    395 KB
  + libxml2                                 2.9.12  h03d6c58_0           pkgs/main/linux-64                        1 MB
  + lz4-c                                    1.9.3  h9c3ff4c_1           conda-forge/linux-64                    Cached
  + matplotlib                               2.2.4  py27_0               conda-forge/linux-64                      7 KB
  + matplotlib-base                          2.2.4  py27hfd891ef_0       conda-forge/linux-64                      7 MB
  + mpi                                        1.0  mpich                conda-forge/linux-64                      4 KB
  + mpi4py                                   3.0.3  py27hf423c55_1       conda-forge/linux-64                    632 KB
  + mpich                                    3.3.2  h846660c_5           conda-forge/linux-64                      6 MB
  + mysql-connector-c                       6.1.11  hab6429c_1002        conda-forge/linux-64                      4 MB
  + mysql-python                             1.2.5  py27h7b6447c_0       pkgs/main/linux-64                      217 KB
  + natsort                                  4.0.4  py27_0               conda-forge/linux-64                     45 KB
  + ncurses                                    6.2  h58526e2_4           conda-forge/linux-64                    Cached
  + numpy                                   1.16.5  py27h95a1406_0       conda-forge/linux-64                      4 MB
  + openssl                                 1.0.2u  h516909a_0           conda-forge/linux-64                      3 MB
  + pandas                                  0.24.2  py27hb3f55d8_0       conda-forge/linux-64                     11 MB
  + pango                                  1.40.14  he752989_2           conda-forge/linux-64                    527 KB
  + pathlib2                                 2.3.5  py27h8c360ce_1       conda-forge/linux-64                     33 KB
  + pcre                                      8.45  h9c3ff4c_0           conda-forge/linux-64                    253 KB
  + pexpect                                  4.8.0  py27h8c360ce_1       conda-forge/linux-64                     77 KB
  + pickleshare                              0.7.5  py27h8c360ce_1001    conda-forge/linux-64                     12 KB
  + pip                                     20.1.1  pyh9f0ad1d_0         conda-forge/noarch                        1 MB
  + pixman                                  0.34.0  h14c3975_1003        conda-forge/linux-64                    595 KB
  + prompt_toolkit                          1.0.15  py27_0               conda-forge/linux-64                    332 KB
  + pthread-stubs                              0.4  h36c2ea0_1001        conda-forge/linux-64                      5 KB
  + ptyprocess                               0.7.0  pyhd3deb0d_0         conda-forge/noarch                       16 KB
  + pygments                                 2.5.2  py_0                 conda-forge/noarch                      669 KB
  + pynast                                   1.2.2  py27_1               bioconda/linux-64                        31 KB
  + pyparsing                                2.4.7  pyh9f0ad1d_0         conda-forge/noarch                       60 KB
  + pyqi                                     0.3.2  py_0                 conda-forge/noarch                       36 KB
  + pyqt                                     5.9.2  py27h05f1152_2       pkgs/main/linux-64                        4 MB
  + python                                  2.7.15  h938d71a_1006        conda-forge/linux-64                     12 MB
  + python-dateutil                          2.8.1  py_0                 conda-forge/noarch                      220 KB
  + python_abi                                 2.7  1_cp27mu             conda-forge/linux-64                      4 KB
  + pytz                                    2020.1  pyh9f0ad1d_0         conda-forge/noarch                      227 KB
  + qcli                                     0.1.1  py27_2               bioconda/linux-64                        21 KB
  + qiime                                    1.9.1  py_3                 /home/vsts/conda/conda-bld/linux-64       8 MB
  + qiime-default-reference                  0.1.3  py27_0               bioconda/linux-64                        21 MB
  + qt                                       5.9.6  h8703b6f_2           pkgs/main/linux-64                       68 MB
  + r-base                                   3.4.1  h4fe35fd_8           conda-forge/linux-64                     23 MB
  + readline                                   7.0  hf8c457e_1001        conda-forge/linux-64                    391 KB
  + scandir                                 1.10.0  py27hdf8410d_1       conda-forge/linux-64                     28 KB
  + scikit-bio                               0.2.3  py27_0               bioconda/linux-64                       981 KB
  + scipy                                    1.2.1  py27h921218d_2       conda-forge/linux-64                     18 MB
  + setuptools                              44.0.0  py27_0               conda-forge/linux-64                    663 KB
  + simplegeneric                            0.8.1  py27_0               conda-forge/linux-64                      6 KB
  + singledispatch                           3.6.1  pyh44b312d_0         conda-forge/noarch                       12 KB
  + sip                                     4.19.8  py27hf484d3e_1000    conda-forge/linux-64                    290 KB
  + six                                     1.16.0  pyh6c4a22f_0         conda-forge/noarch                      Cached
  + sqlalchemy                              1.3.15  py27hdf8410d_1       conda-forge/linux-64                      2 MB
  + sqlite                                  3.28.0  h8b20d00_0           conda-forge/linux-64                      2 MB
  + subprocess32                             3.5.4  py27h516909a_0       conda-forge/linux-64                     46 KB
  + tk                                      8.6.11  h21135ba_0           conda-forge/linux-64                    Cached
  + tornado                                  5.1.1  py27h14c3975_1000    conda-forge/linux-64                    642 KB
  + traitlets                                4.3.3  py27h8c360ce_1       conda-forge/linux-64                    128 KB
  + wcwidth                                  0.2.5  pyh9f0ad1d_2         conda-forge/noarch                      Cached
  + wheel                                   0.37.0  pyhd8ed1ab_1         conda-forge/noarch                      Cached
  + xorg-kbproto                             1.0.7  h7f98852_1002        conda-forge/linux-64                     27 KB
  + xorg-libice                             1.0.10  h7f98852_0           conda-forge/linux-64                     58 KB
  + xorg-libsm                               1.2.3  hd9c2040_1000        conda-forge/linux-64                     26 KB
  + xorg-libx11                              1.7.2  h7f98852_0           conda-forge/linux-64                    941 KB
  + xorg-libxau                              1.0.9  h7f98852_0           conda-forge/linux-64                     13 KB
  + xorg-libxdmcp                            1.1.3  h7f98852_0           conda-forge/linux-64                     19 KB
  + xorg-libxext                             1.3.4  h7f98852_1           conda-forge/linux-64                     54 KB
  + xorg-libxrender                         0.9.10  h7f98852_1003        conda-forge/linux-64                     32 KB
  + xorg-libxt                               1.2.1  h7f98852_2           conda-forge/linux-64                    375 KB
  + xorg-renderproto                        0.11.1  h7f98852_1002        conda-forge/linux-64                      9 KB
  + xorg-xextproto                           7.3.0  h7f98852_1002        conda-forge/linux-64                     28 KB
  + xorg-xproto                             7.0.31  h7f98852_1007        conda-forge/linux-64                     73 KB
  + xz                                       5.2.5  h516909a_1           conda-forge/linux-64                    Cached
  + zlib                                    1.2.11  h516909a_1010        conda-forge/linux-64                    Cached
  + zstd                                     1.5.0  ha95c52a_0           conda-forge/linux-64                    Cached

  Summary:

  Install: 133 packages

  Total download: 285 MB

─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────


## Package Plan ##

  environment location: /home/vsts/conda/conda-bld/qiime_1630456709534/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh


The following NEW packages will be INSTALLED:

    _libgcc_mutex:                      0.1-conda_forge            conda-forge
    _openmp_mutex:                      4.5-1_gnu                  conda-forge
    _r-mutex:                           1.0.1-anacondar_1          conda-forge
    backports:                          1.0-py27_1                 conda-forge
    backports.functools_lru_cache:      1.6.1-py_0                 conda-forge
    backports.shutil_get_terminal_size: 1.0.0-py27_1               conda-forge
    backports_abc:                      0.5-py27_0                 conda-forge
    biom-format:                        2.1.7-py27hc1659b7_1002    conda-forge
    burrito:                            0.9.1-py27r3.4.1_1         bioconda   
    burrito-fillings:                   0.1.1-py27_1               bioconda   
    bzip2:                              1.0.8-h7f98852_4           conda-forge
    ca-certificates:                    2021.5.30-ha878542_0       conda-forge
    cairo:                              1.14.12-ha4e643d_1006      conda-forge
    certifi:                            2019.11.28-py27h8c360ce_1  conda-forge
    click:                              7.1.2-pyh9f0ad1d_0         conda-forge
    cogent:                             1.5.3-py27h1341992_2       bioconda   
    curl:                               7.61.0-h93b3f91_2          conda-forge
    cycler:                             0.10.0-py27_0              conda-forge
    dbus:                               1.13.6-he372182_0          conda-forge
    decorator:                          4.4.2-py_0                 conda-forge
    emperor:                            0.9.51-py27_0              bioconda   
    enum34:                             1.1.10-py27h8c360ce_1      conda-forge
    expat:                              2.4.1-h9c3ff4c_0           conda-forge
    fontconfig:                         2.13.1-hba837de_1005       conda-forge
    freetype:                           2.10.4-h0708190_1          conda-forge
    functools32:                        3.2.3.2-py27_2             conda-forge
    future:                             0.18.2-py27h8c360ce_1      conda-forge
    futures:                            3.3.0-py27h8c360ce_1       conda-forge
    gdata:                              2.0.18-py27_0                         
    gettext:                            0.19.8.1-hf34092f_1004     conda-forge
    glib:                               2.58.3-py27he9b9f4b_1003   conda-forge
    graphite2:                          1.3.13-h58526e2_1001       conda-forge
    gsl:                                2.2.1-h0c605f7_3                      
    gst-plugins-base:                   1.14.5-h0935bb2_2          conda-forge
    gstreamer:                          1.14.5-h36ae1b5_2          conda-forge
    harfbuzz:                           1.8.8-hffaf4a1_0                      
    icu:                                58.2-hf484d3e_1000         conda-forge
    ipython:                            5.8.0-py27_1               conda-forge
    ipython_genutils:                   0.2.0-py27_0               conda-forge
    jbig:                               2.1-h7f98852_2003          conda-forge
    jpeg:                               9d-h36c2ea0_0              conda-forge
    kiwisolver:                         1.1.0-py27h9e3301b_1       conda-forge
    krb5:                               1.14.6-0                   conda-forge
    lerc:                               2.2.1-h9c3ff4c_0           conda-forge
    libblas:                            3.9.0-8_openblas           conda-forge
    libcblas:                           3.9.0-8_openblas           conda-forge
    libdeflate:                         1.7-h7f98852_5             conda-forge
    libffi:                             3.2.1-he1b5a44_1007        conda-forge
    libgcc-ng:                          11.1.0-hc902ee8_8          conda-forge
    libgfortran:                        3.0.0-1                    conda-forge
    libgfortran-ng:                     7.5.0-h14aa051_19          conda-forge
    libgfortran4:                       7.5.0-h14aa051_19          conda-forge
    libgomp:                            11.1.0-hc902ee8_8          conda-forge
    libiconv:                           1.16-h516909a_0            conda-forge
    liblapack:                          3.9.0-8_openblas           conda-forge
    libopenblas:                        0.3.12-pthreads_hb3c22a3_1 conda-forge
    libpng:                             1.6.37-h21135ba_2          conda-forge
    libssh2:                            1.8.0-h1ad7b7a_1003        conda-forge
    libstdcxx-ng:                       11.1.0-h56837e0_8          conda-forge
    libtiff:                            4.3.0-hf544144_1           conda-forge
    libuuid:                            2.32.1-h7f98852_1000       conda-forge
    libwebp-base:                       1.2.1-h7f98852_0           conda-forge
    libxcb:                             1.13-h7f98852_1003         conda-forge
    libxml2:                            2.9.12-h03d6c58_0                     
    lz4-c:                              1.9.3-h9c3ff4c_1           conda-forge
    matplotlib:                         2.2.4-py27_0               conda-forge
    matplotlib-base:                    2.2.4-py27hfd891ef_0       conda-forge
    mpi:                                1.0-mpich                  conda-forge
    mpi4py:                             3.0.3-py27hf423c55_1       conda-forge
    mpich:                              3.3.2-h846660c_5           conda-forge
    mysql-connector-c:                  6.1.11-hab6429c_1002       conda-forge
    mysql-python:                       1.2.5-py27h7b6447c_0                  
    natsort:                            4.0.4-py27_0               conda-forge
    ncurses:                            6.2-h58526e2_4             conda-forge
    numpy:                              1.16.5-py27h95a1406_0      conda-forge
    openssl:                            1.0.2u-h516909a_0          conda-forge
    pandas:                             0.24.2-py27hb3f55d8_0      conda-forge
    pango:                              1.40.14-he752989_2         conda-forge
    pathlib2:                           2.3.5-py27h8c360ce_1       conda-forge
    pcre:                               8.45-h9c3ff4c_0            conda-forge
    pexpect:                            4.8.0-py27h8c360ce_1       conda-forge
    pickleshare:                        0.7.5-py27h8c360ce_1001    conda-forge
    pip:                                20.1.1-pyh9f0ad1d_0        conda-forge
    pixman:                             0.34.0-h14c3975_1003       conda-forge
    prompt_toolkit:                     1.0.15-py27_0              conda-forge
    pthread-stubs:                      0.4-h36c2ea0_1001          conda-forge
    ptyprocess:                         0.7.0-pyhd3deb0d_0         conda-forge
    pygments:                           2.5.2-py_0                 conda-forge
    pynast:                             1.2.2-py27_1               bioconda   
    pyparsing:                          2.4.7-pyh9f0ad1d_0         conda-forge
    pyqi:                               0.3.2-py_0                 conda-forge
    pyqt:                               5.9.2-py27h05f1152_2                  
    python:                             2.7.15-h938d71a_1006       conda-forge
    python-dateutil:                    2.8.1-py_0                 conda-forge
    python_abi:                         2.7-1_cp27mu               conda-forge
    pytz:                               2020.1-pyh9f0ad1d_0        conda-forge
    qcli:                               0.1.1-py27_2               bioconda   
    qiime:                              1.9.1-py_3                 local      
    qiime-default-reference:            0.1.3-py27_0               bioconda   
    qt:                                 5.9.6-h8703b6f_2                      
    r-base:                             3.4.1-h4fe35fd_8           conda-forge
    readline:                           7.0-hf8c457e_1001          conda-forge
    scandir:                            1.10.0-py27hdf8410d_1      conda-forge
    scikit-bio:                         0.2.3-py27_0               bioconda   
    scipy:                              1.2.1-py27h921218d_2       conda-forge
    setuptools:                         44.0.0-py27_0              conda-forge
    simplegeneric:                      0.8.1-py27_0               conda-forge
    singledispatch:                     3.6.1-pyh44b312d_0         conda-forge
    sip:                                4.19.8-py27hf484d3e_1000   conda-forge
    six:                                1.16.0-pyh6c4a22f_0        conda-forge
    sqlalchemy:                         1.3.15-py27hdf8410d_1      conda-forge
    sqlite:                             3.28.0-h8b20d00_0          conda-forge
    subprocess32:                       3.5.4-py27h516909a_0       conda-forge
    tk:                                 8.6.11-h21135ba_0          conda-forge
    tornado:                            5.1.1-py27h14c3975_1000    conda-forge
    traitlets:                          4.3.3-py27h8c360ce_1       conda-forge
    wcwidth:                            0.2.5-pyh9f0ad1d_2         conda-forge
    wheel:                              0.37.0-pyhd8ed1ab_1        conda-forge
    xorg-kbproto:                       1.0.7-h7f98852_1002        conda-forge
    xorg-libice:                        1.0.10-h7f98852_0          conda-forge
    xorg-libsm:                         1.2.3-hd9c2040_1000        conda-forge
    xorg-libx11:                        1.7.2-h7f98852_0           conda-forge
    xorg-libxau:                        1.0.9-h7f98852_0           conda-forge
    xorg-libxdmcp:                      1.1.3-h7f98852_0           conda-forge
    xorg-libxext:                       1.3.4-h7f98852_1           conda-forge
    xorg-libxrender:                    0.9.10-h7f98852_1003       conda-forge
    xorg-libxt:                         1.2.1-h7f98852_2           conda-forge
    xorg-renderproto:                   0.11.1-h7f98852_1002       conda-forge
    xorg-xextproto:                     7.3.0-h7f98852_1002        conda-forge
    xorg-xproto:                        7.0.31-h7f98852_1007       conda-forge
    xz:                                 5.2.5-h516909a_1           conda-forge
    zlib:                               1.2.11-h516909a_1010       conda-forge
    zstd:                               1.5.0-ha95c52a_0           conda-forge

Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
export PREFIX=/home/vsts/conda/conda-bld/qiime_1630456709534/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh
export SRC_DIR=/home/vsts/conda/conda-bld/qiime_1630456709534/test_tmp
import: 'qiime'
import: 'qiime.util'
import: 'qiime.check_id_map'
import: 'qiime.sort'
import: 'qiime'
import: 'qiime.check_id_map'
import: 'qiime.sort'
import: 'qiime.util'
Usage: add_alpha_to_mapping_file.py [options] {-i/--alpha_fps ALPHA_FPS -m/--mapping_fp MAPPING_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Add alpha diversity data to a mapping file for use with other QIIME scripts, i. e. make_emperor. The resulting mapping file will contain three new columns per metric in the alpha diversity data; the first column being the raw value, the second being a normalized raw value and the third one a label classifying the bin where this value fits based on the normalized value.

Example usage: 
Print help message and exit
 add_alpha_to_mapping_file.py -h

Adding alpha diversity data: Add the alpha diversity values to a mapping file and classify the normalized values into 4 bins, where the limits will be  0 < x <= 0.25 for the first bin 0.25 < x <= 0.5 for the second bin, 0.5 < x <= 0.75 for the third bin and 0.75 < x <= 1 for the fourth bin.
 add_alpha_to_mapping_file.py -i adiv_pd.txt -m mapping.txt -b 4 -o alpha_mapping.txt

Adding alpha diversity data with the quantile method: Add the alpha diversity values to a mapping file and classify the normalized values using the quartiles of the distribution of this values.
 add_alpha_to_mapping_file.py -i adiv_pd.txt -m mapping.txt -b 4 -o alpha_mapping_quantile.txt --binning_method=quantile

Adding collated alpha diversity data: Add the mean of the alpha diversity values at a specified rarefaction depth, this case is for use with the output of collate_alpha.py. It is recommended that the filenames are the name of the metric used in each file.
 add_alpha_to_mapping_file.py -i 'shannon.txt,chao1.txt' -m mapping.txt -b 4 -o collated_alpha_mapping.txt --depth=49 --collated_input

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_MAPPING_FP, --output_mapping_fp=OUTPUT_MAPPING_FP
                        filepath for the modified mapping file [default:
                        mapping_file_with_alpha.txt]
  -b NUMBER_OF_BINS, --number_of_bins=NUMBER_OF_BINS
                        number of bins [default: 4].
  -x MISSING_VALUE_NAME, --missing_value_name=MISSING_VALUE_NAME
                        bin prefix name for the sample identifiers that exist
                        in the mapping file (mapping_fp) but not in the alpha
                        diversity file (alpha_fp) [default: N/A].
  --binning_method=BINNING_METHOD
                        Select the method name to create the bins, the options
                        are 'equal' and 'quantile'. Both methods work over the
                        normalized alpha diversity values. On the one hand
                        'equal' will assign the bins on equally spaced limits,
                        depending on the value of --number_of_bins i. e. if
                        you select 4 the limits will be [0.25, 0.50, 0.75]. On
                        the other hand 'quantile' will select the limits based
                        on the --number_of_bins i. e. the limits will be the
                        quartiles if 4 is selected [default: equal].
  --depth=DEPTH         Select the rarefaction depth to use when the alpha_fps
                        refers to collated alpha diversity file(s) i. e. the
                        output of collate_alpha.py. All the iterations
                        contained at this depth will be averaged to form a
                        single mean value [default: highest depth available].
  --collated_input      Use to specify that the -i option is composed of
                        collated alpha diversity data.

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i ALPHA_FPS, --alpha_fps=ALPHA_FPS
                        alpha diversity data with one or multiple metrics i.
                        e. the output of alpha_diversity.py. This can also be
                        a comma-separated list of collated alpha diversity
                        file paths i. e. the output of collate_alpha.py, when
                        using collated alpha diversity data the --depth option
                        is required [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        mapping file to modify by adding the alpha diversity
                        data [REQUIRED]
Usage: add_qiime_labels.py [options] {-m/--mapping_fp MAPPING_FP -i/--fasta_dir FASTA_DIR -c/--filename_column FILENAME_COLUMN}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

A metadata mapping file with SampleIDs
and fasta file names (just the file name itself, not the full or relative
filepath) is used to generate a combined fasta file with valid
QIIME labels based upon the SampleIDs specified in the mapping file.

See: http://qiime.org/documentation/file_formats.html#metadata-mapping-files
for details about the metadata file format.

Example mapping file:
#SampleID	BarcodeSequence	LinkerPrimerSequence	InputFileName	Description
Sample.1	AAAACCCCGGGG	CTACATAATCGGRATT	seqs1.fna	sample.1
Sample.2	TTTTGGGGAAAA	CTACATAATCGGRATT	seqs2.fna	sample.2

This script is to handle situations where fasta data comes already
demultiplexed into a one fasta file per sample basis.  Only alters
the fasta label to add a QIIME compatible label at the beginning.

Example:
With the metadata mapping file above, and an specified directory containing the
files seqs1.fna and seqs2.fna, the first line from the seqs1.fna file might
look like this:
>FLP3FBN01ELBSX length=250 xy=1766_0111 region=1 run=R_2008_12_09_13_51_01_
AACAGATTAGACCAGATTAAGCCGAGATTTACCCGA

and in the output combined fasta file would be written like this
>Sample.1_0 FLP3FBN01ELBSX length=250 xy=1766_0111 region=1 run=R_2008_12_09_13_51_01_
AACAGATTAGACCAGATTAAGCCGAGATTTACCCGA

No changes are made to the sequences.


Example usage: 
Print help message and exit
 add_qiime_labels.py -h

Example: Specify fasta_dir as the input directory of fasta files, use the metadata mapping file example_mapping.txt, with the metadata fasta file name column specified as InputFileName, start enumerating with 1000000, and output the data to the directory combined_fasta
 add_qiime_labels.py -i fasta_dir -m example_mapping.txt -c InputFileName -n 1000000 -o combined_fasta

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Required output directory for log file and corrected
                        mapping file, log file, and html file. [default: .]
  -n COUNT_START, --count_start=COUNT_START
                        Specify the number to start enumerating sequence
                        labels with. [default: 0]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        SampleID to fasta file name mapping file filepath
                        [REQUIRED]
    -i FASTA_DIR, --fasta_dir=FASTA_DIR
                        Directory of fasta files to combine and label.
                        [REQUIRED]
    -c FILENAME_COLUMN, --filename_column=FILENAME_COLUMN
                        Specify column used in metadata mapping file for fasta
                        file names. [REQUIRED]
Usage: adjust_seq_orientation.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Write the reverse complement of all seqs in seqs.fasta (-i) to seqs_rc.fasta (default, change output_fp with -o). Each sequence description line will have ' RC' appended to the end of it (default,
leave sequence description lines untouched by passing -r):

Example usage: 
Print help message and exit
 adjust_seq_orientation.py -h

Example: Reverse complement all sequences in seqs.fna and write result to seqs_rc.fna
 adjust_seq_orientation.py -i seqs.fna

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath
  -r, --retain_seq_id   leave seq description lines untouched [default: append
                        " RC" to seq description lines]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
Usage: align_seqs.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script aligns the sequences in a FASTA file to each other or to a template sequence alignment, depending on the method chosen. Currently, there are three methods which can be used by the user:

1. PyNAST (Caporaso et al., 2009) - The default alignment method is PyNAST, a python implementation of the NAST alignment algorithm.  The NAST algorithm aligns each provided sequence (the "candidate" sequence) to the best-matching sequence in a pre-aligned database of sequences (the "template" sequence).  Candidate sequences are not permitted to introduce new gap characters into the template database, so the algorithm introduces local mis-alignments to preserve the existing template sequence.

2. MUSCLE (Edgar, 2004) - MUSCLE is an alignment method which stands for MUltiple Sequence Comparison by Log-Expectation.

3. INFERNAL (Nawrocki, Kolbe, & Eddy, 2009) - Infernal ("INFERence of RNA ALignment") is for an alignment method for using RNA structure and sequence similarities.


Example usage: 
Print help message and exit
 align_seqs.py -h

Alignment with PyNAST: The default alignment method is PyNAST, a python implementation of the NAST alignment algorithm. The NAST algorithm aligns each provided sequence (the "candidate" sequence) to the best-matching sequence in a pre-aligned database of sequences (the "template" sequence). Candidate sequences are not permitted to introduce new gap characters into the template database, so the algorithm introduces local mis-alignments to preserve the existing template sequence. The quality thresholds are the minimum requirements for matching between a candidate sequence and a template sequence. The set of matching template sequences will be searched for a match that meets these requirements, with preference given to the sequence length. By default, the minimum sequence length is 150 and the minimum percent id is 75%. The minimum sequence length is much too long for typical pyrosequencing reads, but was chosen for compatibility with the original NAST tool.

The following command can be used for aligning sequences using the PyNAST method, where we supply the program with a FASTA file of unaligned sequences (i.e. resulting FASTA file from pick_rep_set.py, a FASTA file of pre-aligned sequences (this is the template file, which is typically the Greengenes core set - available from http://greengenes.lbl.gov/), and the results will be written to the directory "pynast_aligned/"
 align_seqs.py -i $PWD/unaligned.fna -t $PWD/core_set_aligned.fasta.imputed -o $PWD/pynast_aligned_defaults/

Alternatively, one could change the minimum sequence length ("-e") requirement and minimum sequence identity ("-p"), using the following command
 align_seqs.py -i $PWD/unaligned.fna -t core_set_aligned.fasta.imputed -o $PWD/pynast_aligned/ -e 500 -p 95.0

Alignment with MUSCLE: One could also use the MUSCLE algorithm. The following command can be used to align sequences (i.e. the resulting FASTA file from pick_rep_set.py), where the output is written to the directory "muscle_alignment/"
 align_seqs.py -i $PWD/unaligned.fna -m muscle -o $PWD/muscle_alignment/

Alignment with Infernal: An alternative alignment method is to use Infernal. Infernal is similar to the PyNAST method, in that you supply a template alignment, although Infernal has several distinct differences. Infernal takes a multiple sequence alignment with a corresponding secondary structure annotation. This input file must be in Stockholm alignment format. There is a fairly good description of the Stockholm format rules at: http://en.wikipedia.org/wiki/Stockholm_format. Infernal will use the sequence and secondary structural information to align the candidate sequences to the full reference alignment. Similar to PyNAST, Infernal will not allow for gaps to be inserted into the reference alignment. Using Infernal is slower than other methods, and therefore is best used with sequences that do not align well using PyNAST.

The following command can be used for aligning sequences using the Infernal method, where we supply the program with a FASTA file of unaligned sequences, a STOCKHOLM file of pre-aligned sequences and secondary structure (this is the template file - an example file can be obtained from: ftp://ftp.microbio.me/qiime/seed.16s.reference_model.sto.zip), and the results will be written to the directory "infernal_aligned/"
 align_seqs.py -m infernal -i $PWD/unaligned.fna -t $PWD/seed.16s.reference_model.sto -o $PWD/infernal_aligned/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m ALIGNMENT_METHOD, --alignment_method=ALIGNMENT_METHOD
                        Method for aligning sequences. Valid choices are:
                        pynast, infernal, clustalw, muscle, infernal, mafft
                        [default: pynast]
  -a PAIRWISE_ALIGNMENT_METHOD, --pairwise_alignment_method=PAIRWISE_ALIGNMENT_METHOD
                        method for performing pairwise alignment in PyNAST.
                        Valid choices are muscle, pair_hmm, clustal, blast,
                        uclust, mafft [default: uclust]
  -t TEMPLATE_FP, --template_fp=TEMPLATE_FP
                        Filepath for template alignment [default:
                        /home/vsts/conda/conda-bld/qiime_1630456709534/_test_e
                        nv_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placeh/lib/python2.7
                        /site-packages/qiime_default_reference/gg_13_8_otus/re
                        p_set_aligned/85_otus.pynast.fasta]
  -e MIN_LENGTH, --min_length=MIN_LENGTH
                        Minimum sequence length to include in alignment
                        [default: 75% of the median input sequence length]
  -p MIN_PERCENT_ID, --min_percent_id=MIN_PERCENT_ID
                        Minimum percent sequence identity to closest blast hit
                        to include sequence in alignment [default: 0.75]
  -d BLAST_DB, --blast_db=BLAST_DB
                        Database to blast against when -m pynast [default:
                        created on-the-fly from template_alignment]
  --muscle_max_memory=MUSCLE_MAX_MEMORY
                        Maximum memory allocation for the muscle alignment
                        method (MB) [default: 80% of available memory, as
                        detected by MUSCLE]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Path to store result file [default:
                        <ALIGNMENT_METHOD>_aligned]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
Usage: alpha_diversity.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script calculates alpha diversity, or within-sample diversity, using an
OTU table. The QIIME pipeline allows users to conveniently calculate more than
two dozen different diversity metrics. The full list of available metrics is
available by passing the -s option to this script.

Documentation of the metrics can be found at
http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.
Every metric has different strengths and limitations - technical discussion of
each metric is readily available online and in ecology textbooks, but is beyond
the scope of this document.


Example usage: 
Print help message and exit
 alpha_diversity.py -h

Single File Alpha Diversity Example (non-phylogenetic): To perform alpha diversity (e.g. chao1) on a single OTU table, where the results are output to "alpha_div.txt", you can use the following command
 alpha_diversity.py -i otu_table.biom -m chao1 -o adiv_chao1.txt

Single File Alpha Diversity Example (phylogenetic): In the case that you would like to perform alpha diversity using a phylogenetic metric (e.g. PD_whole_tree), you can use the following command
 alpha_diversity.py -i otu_table.biom -m PD_whole_tree -o adiv_pd.txt -t rep_set.tre

Single File Alpha Diversity Example with multiple metrics: You can use the following idiom to run multiple metrics at once (comma-separated)
 alpha_diversity.py -i otu_table.biom -m chao1,PD_whole_tree -o adiv_chao1_pd.txt -t rep_set.tre

Multiple File (batch) Alpha Diversity: To perform alpha diversity on multiple OTU tables (e.g.: rarefied otu tables resulting from multiple_rarefactions.py), specify an input directory instead of a single otu table, and an output directory (e.g. "alpha_div_chao1_PD/") as shown by the following command
 alpha_diversity.py -i otu_tables/ -m chao1,PD_whole_tree -o adiv_chao1_pd/ -t rep_set.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -i INPUT_PATH, --input_path=INPUT_PATH
                        Input OTU table filepath or input directory containing
                        OTU tables for batch processing. [default: none]
  -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        Output filepath to store alpha diversity metric(s) for
                        each sample in a tab-separated format or output
                        directory when batch processing. [default: none]
  -m METRICS, --metrics=METRICS
                        Alpha-diversity metric(s) to use. A comma-separated
                        list should be provided when multiple metrics are
                        specified. [default:
                        PD_whole_tree,chao1,observed_otus]
  -s, --show_metrics    Show the available alpha-diversity metrics and exit.
  -t TREE_PATH, --tree_path=TREE_PATH
                        Input newick tree filepath. [default: none; REQUIRED
                        for phylogenetic metrics]
Usage: alpha_rarefaction.py [options] {-i/--otu_table_fp OTU_TABLE_FP -m/--mapping_fp MAPPING_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


The steps performed by this script are: Generate rarefied OTU tables; compute alpha diversity metrics for each rarefied OTU table; collate alpha diversity results; and generate alpha rarefaction plots.

Example usage: 
Print help message and exit
 alpha_rarefaction.py -h

Example: Given an OTU table, a phylogenetic tree, a mapping file, and a max sample depth, compute alpha rarefaction plots for the PD, observed species and chao1 metrics. To specify alternative metrics pass a parameter file via -p. We generally recommend that the max depth specified here (-e) is the same as the even sampling depth provided to beta_diversity_through_plots (also -e).
 alpha_rarefaction.py -i otu_table.biom -o arare_max100/ -t rep_set.tre -m Fasting_Map.txt -e 100

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters . [if omitted, default values will
                        be used]
  -n NUM_STEPS, --num_steps=NUM_STEPS
                        number of steps (or rarefied OTU table sizes) to make
                        between min and max counts [default: 10]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]
  -a, --parallel        Run in parallel where available [default: False]
  -t TREE_FP, --tree_fp=TREE_FP
                        path to the tree file [default: none; REQUIRED for
                        phylogenetic measures]
  --min_rare_depth=MIN_RARE_DEPTH
                        the lower limit of rarefaction depths [default: 10]
  -e MAX_RARE_DEPTH, --max_rare_depth=MAX_RARE_DEPTH
                        the upper limit of rarefaction depths [default: median
                        sequence/sample count]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start. NOTE: you must also pass -a
                        to run in parallel, this defines the number of jobs to
                        be started if and only if -a is passed [default: 1]
  --retain_intermediate_files
                        retain intermediate files: rarefied OTU tables
                        (rarefaction) and alpha diversity results (alpha_div).
                        By default these will be erased [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        the input otu table [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to the mapping file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: ampliconnoise.py [options] {-m/--mapping_fp MAPPING_FP -i/--sff_filepath SFF_FILEPATH -o/--output_filepath OUTPUT_FILEPATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


The steps performed by this script are:

1. Split input sff.txt file into one file per sample

2. Run scripts required for PyroNoise

3. Run scripts required for SeqNoise

4. Run scripts requred for Perseus (chimera removal)

5. Merge output files into one file similar to the output of split_libraries.py

This script produces a denoised fasta sequence file such as:
>PC.355_41
CATGCTGCCTC...
...
>PC.636_23
CATGCTGCCTC...
...

Additionally, the intermediate results of the ampliconnoise pipeline are
written to an output directory.

Ampliconnoise must be installed and correctly configured, and parallelized
steps will be called with mpirun, not qiime's start_parallel_jobs_torque.py script.



Example usage: 
Print help message and exit
 ampliconnoise.py -h

Run ampliconnoise, write output to anoise_out.fna, compatible with output of split_libraries.py
 ampliconnoise.py -i Fasting_Example.sff.txt -m Fasting_Map.txt -o anoise_out.fna

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NP, --np=NP        number of processes to use for mpi steps. Default: 2
  --chimera_alpha=CHIMERA_ALPHA
                        alpha value to Class.pl used for chimera removal
                        Default: -3.8228
  --chimera_beta=CHIMERA_BETA
                        beta value to Class.pl used for chimera removal
                        Default: 0.62
  --seqnoise_resolution=SEQNOISE_RESOLUTION
                        -s parameter passed to seqnoise. Default is 25.0 for
                        titanium, 30.0 for flx
  -d OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory for ampliconnoise intermediate results.
                        Default is output_filepath_dir
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters. [if omitted, default values will be
                        used]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: False]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]
  --suppress_perseus    omit perseus from ampliconnoise workflow
  --platform=PLATFORM   sequencing technology, options are 'titanium','flx'.
                        [default: flx]
  --truncate_len=TRUNCATE_LEN
                        Specify a truncation length for ampliconnoise.  Note
                        that is this is not specified, the truncate length is
                        chosen by the --platform option (220 for FLX, 400 for
                        Titanium) [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath [REQUIRED]
    -i SFF_FILEPATH, --sff_filepath=SFF_FILEPATH
                        sff.txt filepath [REQUIRED]
    -o OUTPUT_FILEPATH, --output_filepath=OUTPUT_FILEPATH
                        the output file [REQUIRED]
Usage: assign_taxonomy.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Contains code for assigning taxonomy, using several techniques.

Given a set of sequences, assign_taxonomy.py attempts to assign the taxonomy of each sequence. Currently the methods implemented are assignment with BLAST, the RDP classifier, RTAX, mothur, and uclust. The output of this step is an observation metadata mapping file of input sequence identifiers (1st column of output file) to taxonomy (2nd column) and quality score (3rd column). There may be method-specific information in subsequent columns.

Reference data sets and id-to-taxonomy maps for 16S rRNA sequences can be found in the Greengenes reference OTU builds. To get the latest build of the Greengenes OTUs (and other marker gene OTU collections), follow the "Resources" link from http://qiime.org. After downloading and unzipping you can use the following files as -r and -t, where <otus_dir> is the name of the new directory after unzipping the reference OTUs tgz file.

-r <otus_dir>/rep_set/97_otus.fasta
-t <otus_dir>/taxonomy/97_otu_taxonomy.txt


Example usage: 
Print help message and exit
 assign_taxonomy.py -h

Assign taxonomy with the uclust consensus taxonomy assigner (default): Perform database search with uclust to retrive up to uclust_max_accepts hits for each query sequence. Then assign the most specific taxonomic label that is associated with at least min_consensus_fraction of the hits.
 assign_taxonomy.py -i repr_set_seqs.fasta -r ref_seq_set.fna -t id_to_taxonomy.txt

Assignment with SortMeRNA: Perform database search with sortmerna to retrieve up to sortmerna_best_N_alignments hits for each query sequence. Then assign the most specific taxonomic label that is associated with at least min_consensus_fraction of the hits.
 assign_taxonomy.py -i repr_set_seqs.fasta -r ref_seq_set.fna -t id_to_taxonomy.txt -m sortmerna

Assignment with BLAST: Taxonomy assignments are made by searching input sequences against a blast database of pre-assigned reference sequences. If a satisfactory match is found, the reference assignment is given to the input sequence. This method does not take the hierarchical structure of the taxonomy into account, but it is very fast and flexible. If a file of reference sequences is provided, a temporary blast database is built on-the-fly. The quality scores assigned by the BLAST taxonomy assigner are e-values.

To assign the sequences to the representative sequence set, using a reference set of sequences and a taxonomy to id assignment text file, where the results are output to default directory "blast_assigned_taxonomy", you can run the following command
 assign_taxonomy.py -i repr_set_seqs.fasta -r ref_seq_set.fna -t id_to_taxonomy.txt -m blast

Optionally, the user could changed the E-value ("-e"), using the following command
 assign_taxonomy.py -i repr_set_seqs.fasta -r ref_seq_set.fna -t id_to_taxonomy.txt -e 0.01 -m blast

Assignment with the RDP Classifier: The RDP Classifier (Wang, Garrity, Tiedje, & Cole, 2007) assigns taxonomies using Naive Bayes classification. By default, the classifier is retrained using the values provided for --id_to_taxonomy_fp and --reference_seqs_fp.
 assign_taxonomy.py -i repr_set_seqs.fasta -m rdp

Assignment with the RDP Classifier using an alternative minimum confidence score by passing -c
 assign_taxonomy.py -i repr_set_seqs.fasta -m rdp -c 0.80

Assignment with RTAX: Taxonomy assignments are made by searching input sequences against a fasta database of pre-assigned reference sequences. All matches are collected which match the query within 0.5% identity of the best match.  A taxonomy assignment is made to the lowest rank at which more than half of these hits agree.  Note that both unclustered read fasta files are required as inputs in addition to the representative sequence file.

To make taxonomic classifications of the representative sequences, using a reference set of sequences and a taxonomy to id assignment text file, where the results are output to default directory "rtax_assigned_taxonomy", you can run the following command
 assign_taxonomy.py -i rtax_repr_set_seqs.fasta -m rtax --read_1_seqs_fp read_1.seqs.fna --read_2_seqs_fp read_2.seqs.fna -r rtax_ref_seq_set.fna -t rtax_id_to_taxonomy.txt

Assignment with Mothur: The Mothur software provides a naive bayes classifier similar to the RDP Classifier.A set of training sequences and id-to-taxonomy assignments must be provided.  Unlike the RDP Classifier, sequences in the training set may be assigned at any level of the taxonomy.

To make taxonomic classifications of the representative sequences, where the results are output to default directory "mothur_assigned_taxonomy", you can run the following command
 assign_taxonomy.py -i mothur_repr_set_seqs.fasta -m mothur -r mothur_ref_seq_set.fna -t mothur_id_to_taxonomy.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t ID_TO_TAXONOMY_FP, --id_to_taxonomy_fp=ID_TO_TAXONOMY_FP
                        Path to tab-delimited file mapping sequences to
                        assigned taxonomy. Each assigned taxonomy is provided
                        as a semicolon-separated list. For assignment with
                        rdp, each assigned taxonomy must be exactly 6 levels
                        deep. [default: /home/vsts/conda/conda-bld/qiime_16304
                        56709534/_test_env_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placehold_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeh/
                        lib/python2.7/site-packages/qiime_default_reference/gg
                        _13_8_otus/taxonomy/97_otu_taxonomy.txt]
  -r REFERENCE_SEQS_FP, --reference_seqs_fp=REFERENCE_SEQS_FP
                        Path to reference sequences.  For assignment with
                        blast, these are used to generate a blast database.
                        For assignment with rdp, they are used as training
                        sequences for the classifier. [default:
                        /home/vsts/conda/conda-bld/qiime_1630456709534/_test_e
                        nv_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placeh/lib/python2.7
                        /site-packages/qiime_default_reference/gg_13_8_otus/re
                        p_set/97_otus.fasta]
  -p TRAINING_DATA_PROPERTIES_FP, --training_data_properties_fp=TRAINING_DATA_PROPERTIES_FP
                        Path to ".properties" file in pre-compiled training
                        data for the RDP Classifier.  This option is
                        overridden by the -t and -r options. [default: none]
  --read_1_seqs_fp=READ_1_SEQS_FP
                        Path to fasta file containing the first read from
                        paired-end sequencing, prior to OTU clustering (used
                        for RTAX only). [default: none]
  --read_2_seqs_fp=READ_2_SEQS_FP
                        Path to fasta file containing a second read from
                        paired-end sequencing, prior to OTU clustering (used
                        for RTAX only). [default: none]
  --single_ok           When classifying paired ends, allow fallback to
                        single-ended classification when the mate pair is
                        lacking (used for RTAX only). [default: False]
  --no_single_ok_generic
                        When classifying paired ends, do not allow fallback to
                        single-ended classification when the mate pair is
                        overly generic (used for RTAX only). [default: False]
  --read_id_regex=READ_ID_REGEX
                        Used to parse the result of OTU clustering, to get the
                        read_1_id for each clusterID.  The clusterID itself is
                        assumed to be the first field, and is not captured by
                        the regex.  (used for RTAX only). [default:
                        \S+\s+(\S+)]
  --amplicon_id_regex=AMPLICON_ID_REGEX
                        Used to parse the result of split_libraries, to get
                        the ampliconID for each read_1_id.  Two groups capture
                        read_1_id and ampliconID, respectively.  (used for
                        RTAX only). [default: (\S+)\s+(\S+?)\/]
  --header_id_regex=HEADER_ID_REGEX
                        Used to parse the result of split_libraries, to get
                        the portion of the header that RTAX uses to match mate
                        pairs.  The default uses the amplicon ID, not
                        including /1 or /3, as the primary key for the query
                        sequences.  Typically this regex will be the same as
                        amplicon_id_regex, except that only the second group
                        is captured.  (used for RTAX only). [default:
                        \S+\s+(\S+?)\/]
  -m ASSIGNMENT_METHOD, --assignment_method=ASSIGNMENT_METHOD
                        Taxon assignment method, must be one of rdp, blast,
                        rtax, mothur, uclust, sortmerna [default: uclust]
  --sortmerna_db=SORTMERNA_DB
                        Pre-existing database to search against when using
                        sortmerna [default: none]
  --sortmerna_e_value=SORTMERNA_E_VALUE
                        Maximum E-value when clustering [default = 1.0]
  --sortmerna_coverage=SORTMERNA_COVERAGE
                        Mininum percent query coverage (of an alignment) to
                        consider a hit, expressed as a fraction between 0 and
                        1 [default: 0.9]
  --sortmerna_best_N_alignments=SORTMERNA_BEST_N_ALIGNMENTS
                        This option specifies how many best alignments per
                        read will be written [default: 5]
  --sortmerna_threads=SORTMERNA_THREADS
                        Specify number of threads to be used for sortmerna
                        mapper which utilizes multithreading. [default: 1]
  -b BLAST_DB, --blast_db=BLAST_DB
                        Database to blast against.  Must provide either
                        --blast_db or --reference_seqs_db for assignment with
                        blast [default: none]
  -c CONFIDENCE, --confidence=CONFIDENCE
                        Minimum confidence to record an assignment, only used
                        for rdp and mothur methods [default: 0.5]
  --min_consensus_fraction=MIN_CONSENSUS_FRACTION
                        Minimum fraction of database hits that must have a
                        specific taxonomic assignment to assign that taxonomy
                        to a query, only used for sortmerna and uclust methods
                        [default: 0.51]
  --similarity=SIMILARITY
                        Minimum percent similarity (expressed as a fraction
                        between 0 and 1) to consider a database match a hit,
                        only used for sortmerna and uclust methods [default:
                        0.9]
  --uclust_max_accepts=UCLUST_MAX_ACCEPTS
                        Number of database hits to consider when making an
                        assignment, only used for uclust method [default: 3]
  --rdp_max_memory=RDP_MAX_MEMORY
                        Maximum memory allocation, in MB, for Java virtual
                        machine when using the rdp method.  Increase for large
                        training sets [default: 4000]
  -e BLAST_E_VALUE, --blast_e_value=BLAST_E_VALUE
                        Maximum e-value to record an assignment, only used for
                        blast method [default: 0.001]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Path to store result file [default:
                        <ASSIGNMENT_METHOD>_assigned_taxonomy]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
Usage: beta_diversity.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The input for this script is the OTU table containing the number of sequences observed in each OTU (rows) for each sample (columns). For more information pertaining to the OTU table refer to the documentation for make_otu_table. If the user would like phylogenetic beta diversity metrics using UniFrac, a phylogenetic tree must also be passed as input (see make_phylogeny.py). The output of this script is a distance matrix containing a dissimilarity value for each pairwise comparison.

A number of metrics are currently supported, including unweighted and weighted UniFrac (pass the -s option to see available metrics). In general, because unifrac uses phylogenetic information, one of the unifrac metrics is recommended, as results can be vastly more useful (Hamady & Knight, 2009). Quantitative measures (e.g. weighted unifrac) are ideally suited to revealing community differences that are due to changes in relative taxon abundance (e.g., when a particular set of taxa flourish because a limiting nutrient source becomes abundant). Qualitative measures (e.g. unweighted unifrac) are most informative when communities differ primarily by what can live in them (e.g., at high temperatures), in part because abundance information can obscure significant patterns of variation in which taxa are present (Lozupone et al., 2007). Most qualitative measures are referred to here e.g. "binary_jaccard". Typically both weighted and unweighted unifrac are used.

Example usage: 
Print help message and exit
 beta_diversity.py -h

Single File Beta Diversity (non-phylogenetic): To perform beta diversity (using e.g. euclidean distance) on a single OTU table, where the results are output to beta_div/, use the following command
 beta_diversity.py -i otu_table.biom -m euclidean -o beta_div

Single File Beta Diversity (phylogenetic): In the case that you would like to perform beta diversity using a phylogenetic metric (e.g. weighted_unifrac), you can use the following command
 beta_diversity.py -i otu_table.biom -m weighted_unifrac -o beta_div/ -t rep_set.tre

Multiple File (batch) Beta Diversity (phylogenetic): To perform beta diversity on multiple OTU tables (e.g., resulting files from multiple_rarefactions.py), specify an input directory (e.g. otu_tables/) as shown by the following command
 beta_diversity.py -i otu_tables/ -m weighted_unifrac -o beta_div/ -t rep_set.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -i INPUT_PATH, --input_path=INPUT_PATH
                        Input OTU table in biom format or input directory
                        containing OTU tables in biom format for batch
                        processing.
  -r ROWS, --rows=ROWS  Compute for only these rows of the distance matrix.
                        User should pass a list of sample names (e.g. "s1,s3")
                        [default: none; full n x n matrix is generated]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Output directory. One will be created if it doesn't
                        exist.
  -m METRICS, --metrics=METRICS
                        Beta-diversity metric(s) to use. A comma-separated
                        list should be provided when multiple metrics are
                        specified. [default:
                        unweighted_unifrac,weighted_unifrac]
  -s, --show_metrics    Show the available beta-diversity metrics and exit.
                        Metrics starting with "binary..." specifies that a
                        metric is qualitative, and considers only the presence
                        or absence of each taxon [default: False]
  -t TREE_PATH, --tree_path=TREE_PATH
                        Input newick tree filepath, which is required when
                        phylogenetic metrics are specified. [default: none]
  -f, --full_tree       By default, tips not corresponding to OTUs in the OTU
                        table are removed from the tree for diversity
                        calculations. Pass to skip this step if you're already
                        passing a minimal tree. Beware with "full_tree"
                        metrics, as extra tips in the tree change the result
Usage: beta_diversity_through_plots.py [options] {-i/--otu_table_fp OTU_TABLE_FP -m/--mapping_fp MAPPING_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script will perform beta diversity, principal coordinate analysis, and generate a preferences file along with 3D PCoA Plots.


Example usage: 
Print help message and exit
 beta_diversity_through_plots.py -h

Example: Given an OTU table, a phylogenetic tree, an even sampling depth, and a mapping file, perform the following steps: 1. Randomly subsample otu_table.biom to even number of sequences per sample (100 in this case); 2. Compute a weighted and unweighted unifrac distance matrices (can add additional metrics by passing a parameters file via -p); 3. Peform a principal coordinates analysis on the result of Step 2; 4. Generate a 2D and 3D plots for all mapping fields.
 beta_diversity_through_plots.py -i otu_table.biom -o bdiv_even100/ -t rep_set.tre -m Fasting_Map.txt -e 100

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t TREE_FP, --tree_fp=TREE_FP
                        path to the tree file [default: none; REQUIRED for
                        phylogenetic measures]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters . [if omitted, default values will
                        be used]
  --color_by_all_fields
                        plots will have coloring for all mapping fields
                        [default: False; only include fields with greater than
                        one value and fewer values than the number of samples]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]
  -a, --parallel        Run in parallel where available [default: False]
  -e SEQS_PER_SAMPLE, --seqs_per_sample=SEQS_PER_SAMPLE
                        depth of coverage for even sampling [default: none]
  --suppress_emperor_plots
                        Do not generate emperor plots [default: False]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start. NOTE: you must also pass -a
                        to run in parallel, this defines the number of jobs to
                        be started if and only if -a is passed [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        the input biom table [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to the mapping file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: beta_significance.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH -s/--significance_test SIGNIFICANCE_TEST -t/--tree_path TREE_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The tests are conducted on each pair of samples present in the input otu table. See the unifrac tutorial online for more details (http://unifrac.colorado.edu/)

Example usage: 
Print help message and exit
 beta_significance.py -h

Example: Perform 100 randomizations of sample/sequence assignments, and record the probability that sample 1 is phylogenetically different from sample 2, using the unifrac monte carlo significance test. The test is run for all pairs of samples.
 beta_significance.py -i otu_table.biom -t rep_set.tre -s unweighted_unifrac -o unw_sig.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NUM_ITERS, --num_iters=NUM_ITERS
                        number of monte carlo randomizations [default: 100]
  -k TYPE_OF_TEST, --type_of_test=TYPE_OF_TEST
                        type of significance test to perform, options are
                        'all_together', 'each_pair' or 'each_sample'.
                        [default: each_pair]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input otu table in biom format [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output results path [REQUIRED]
    -s SIGNIFICANCE_TEST, --significance_test=SIGNIFICANCE_TEST
                        significance test to use, options are
                        'unweighted_unifrac', 'weighted_unifrac',
                        'weighted_normalized_unifrac', or 'p-test' [REQUIRED]
    -t TREE_PATH, --tree_path=TREE_PATH
                        path to newick tree file [REQUIRED]
Usage: blast_wrapper.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -r/--refseqs_fp REFSEQS_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script is a functionally-limited interface to the qiime.util.qiime_blast_seqs function, primarily useful for testing purposes. Once that function has been integrated into qiime as the primary blast interface it will move to PyCogent. An expanded version of this command line interface may replace the script functionality of cogent.app.blast at that point.

Example usage: 
Print help message and exit
 blast_wrapper.py -h

Example: Blast all sequences in inseqs.fasta (-i) against a BLAST db constructed from refseqs.fasta (-r).
 blast_wrapper.py -i $PWD/inseqs.fasta -r $PWD/refseqs.fasta

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NUM_SEQS_PER_BLAST_RUN, --num_seqs_per_blast_run=NUM_SEQS_PER_BLAST_RUN
                        number of sequences passed to each blast call - useful
                        for very large sequence collections [default: 1000]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
    -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        path to blast database as a fasta file [REQUIRED]
Usage: categorized_dist_scatterplot.py [options] {-m/--map MAP -d/--distance_matrix DISTANCE_MATRIX -p/--primary_state PRIMARY_STATE -a/--axis_category AXIS_CATEGORY -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Create a figure representing average distances between samples, broken down by categories. I call it a 'categorized distance scatterplot'. See script usage for more details. The mapping file specifies the relevant data - if you have e.g. 'N/A' values or samples you don't want included, first use filter_samples_from_otu_table.py to remove unwanted samples from the mapping file, and thus the analysis. Note that the resulting plot will include only samples in both the mapping file AND the distance matrix.

Example usage: 
Print help message and exit
 categorized_dist_scatterplot.py -h

Canonical Example: Split samples by country. Within each country compare each child to all adults. Plot the average distance from that child to all adults, vs. the age of that child
 python categorized_dist_scatterplot.py -m map.txt -d unifrac_distance.txt -c Country -p AgeCategory:Child -s AgeCategory:Adult -a AgeYears -o fig1.png

Example 2: Same as above, but compares Child with all other categories (e.g.: NA, Infant, etc.)
 python categorized_dist_scatterplot.py -m map.txt -d unifrac_distance.txt -c Country -p AgeCategory:Child -a AgeYears -o fig1.svg

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -c COLORBY, --colorby=COLORBY
                        samples will first be separated by this column of the
                        mapping file. They will be colored by this column of
                        the mapping file, and all comparisons will be done
                        only among samples with the same value in this column.
                        e.g.: Country. You may omit -c, and the samples will
                        not be separated
  -s SECONDARY_STATE, --secondary_state=SECONDARY_STATE
                        all samples matching the primary state will be
                        compared to samples matcthing this secondary state.
                        E.g.: AgeCategory:Adult

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAP, --map=MAP   mapping file [REQUIRED]
    -d DISTANCE_MATRIX, --distance_matrix=DISTANCE_MATRIX
                        distance matrix [REQUIRED]
    -p PRIMARY_STATE, --primary_state=PRIMARY_STATE
                        Samples matching this state will be plotted. E.g.:
                        AgeCategory:Child . See qiime's
                        filter_samples_from_otu_table.py for more syntax
                        options [REQUIRED]
    -a AXIS_CATEGORY, --axis_category=AXIS_CATEGORY
                        this will form the horizontal axis of the figure,
                        e.g.: AgeYears . Must be numbers [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output figure, filename extention determines format.
                        E.g.: "fig1.png" or similar. A "fig1.txt" or similar
                        will also be created with the data underlying the
                        figure [REQUIRED]

This script has been renamed validate_mapping_file.py for clarity. For help, call validate_mapping_file.py -h

Usage: clean_raxml_parsimony_tree.py [options] {-i/--input_tree INPUT_TREE -t/--tips_to_keep TIPS_TO_KEEP -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script allows the user to remove specific duplicate tips from a Raxml tree.

Example usage: 
Print help message and exit
 clean_raxml_parsimony_tree.py -h

Example (depth): For this case the user can pass in input Raxml tree, duplicate tips, and define an output filepath. When using the depth option, only the deepest replicate is kept.
 clean_raxml_parsimony_tree.py -i raxml_v730_final_placement.tre -t 6 -o raxml_v730_final_placement_depth.tre

Example (numtips): For this case the user can pass in input Raxml tree, duplicate tips, and define an output filepath. When using the numtips option, the replicate with the fewest siblings is kept.
 clean_raxml_parsimony_tree.py -i raxml_v730_final_placement.tre -t 6 -o raxml_v730_final_placement_numtips.tre -s numtips

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s SCORING_METHOD, --scoring_method=SCORING_METHOD
                        the scoring method either depth or numtips [default:
                        depth]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_TREE, --input_tree=INPUT_TREE
                        the input raxml parsimony tree [REQUIRED]
    -t TIPS_TO_KEEP, --tips_to_keep=TIPS_TO_KEEP
                        the input tips to score and retain (comma-separated
                        list) [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath [REQUIRED]
Usage: cluster_quality.py [options] {-i/--input_path INPUT_PATH -m/--map MAP -c/--category CATEGORY}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The input is a distance matrix (i.e. resulting file from beta_diversity.py).

Example usage: 
Print help message and exit
 cluster_quality.py -h

cluster quality based on the treatment category: to compute the quality of clusters, and print to stdout, use the following idiom
 cluster_quality.py -i weighted_unifrac_otu_table.txt -m Fasting_Map.txt -c Treatment

cluster quality based on the DOB category: to compute the quality of clusters, and print to stdout, use the following idiom
 cluster_quality.py -i weighted_unifrac_otu_table.txt -m Fasting_Map.txt -c DOB

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path, prints to stdout if omitted
  -s, --short           print only the ratio of mean dissimilarities
                        between/within clusters instead of more detailed
                        output
  --metric=METRIC       choice of quality metric to apply. Currently only one
                        option exists, the ratio of mean(distances between
                        samples from different clusters) to mean(distances
                        between samples from the same cluster) Default: ratio

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input distance matrix file [REQUIRED]
    -m MAP, --map=MAP   mapping file [REQUIRED]
    -c CATEGORY, --category=CATEGORY
                        column of mapping file delimiting clusters [REQUIRED]
Usage: collapse_samples.py [options] {-b/--input_biom_fp INPUT_BIOM_FP -m/--mapping_fp MAPPING_FP --output_biom_fp OUTPUT_BIOM_FP --output_mapping_fp OUTPUT_MAPPING_FP --collapse_fields COLLAPSE_FIELDS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Collapse samples in a BIOM table and mapping file. Values in the BIOM table are collapsed in one of several different ways; see the available options for --collapse_mode. Values in the mapping file are collapsed by grouping the values if they differ for the grouped samples, and by providing the single value if they don't differ for the grouped samples.

Example usage: 
Print help message and exit
 collapse_samples.py -h

Collapse samples in biom table and mapping file: Collapse samples by taking the median value for each observation in each group, where group is defined by having the same values for subject in the mapping file.
 collapse_samples.py -b table.biom -m map.txt --output_biom_fp collapsed.biom --output_mapping_fp collapsed_map.txt --collapse_mode median --collapse_fields subject

Collapse samples in biom table and mapping file: Collapse samples by taking the median value for each observation in each group, where group is defined by having the same values for both subject and replicate-group in the mapping file.
 collapse_samples.py -b table.biom -m map.txt --output_biom_fp collapsed.biom --output_mapping_fp collapsed_map.txt --collapse_mode median --collapse_fields replicate-group,subject

Collapse samples in biom table and mapping file, and normalize the table: Collapse samples by taking the median value for each observation in each group, where group is defined by having the same values for both subject and replicate-group in the mapping file. Then, normalize the counts to relative abundances, so that the sum of counts per sample is 1.0.
 collapse_samples.py -b table.biom -m map.txt --output_biom_fp collapsed-normed.biom --output_mapping_fp collapsed_map.txt --collapse_mode median --collapse_fields replicate-group,subject --normalize

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --collapse_mode=COLLAPSE_MODE
                        the mechanism for collapsing counts within groups;
                        valid options are: mean, sum, random, median, first.
                        [default: sum]
  --normalize           Normalize observation counts to relative abundances,
                        so the counts within each sample sum to 1.0. [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -b INPUT_BIOM_FP, --input_biom_fp=INPUT_BIOM_FP
                        the biom table containing the samples to be collapsed
                        [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the sample metdata mapping file [REQUIRED]
    --output_biom_fp=OUTPUT_BIOM_FP
                        path where collapsed biom table should be written
                        [REQUIRED]
    --output_mapping_fp=OUTPUT_MAPPING_FP
                        path where collapsed mapping file should be written
                        [REQUIRED]
    --collapse_fields=COLLAPSE_FIELDS
                        comma-separated list of fields to collapse on
                        [REQUIRED]
Usage: collate_alpha.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

When performing batch analyses on the OTU table (e.g. rarefaction followed by alpha diversity), the result of alpha_diversity.py comprises many files, which need to be concatenated into a single file for generating rarefaction curves.  This script joins those files.
Input files are:
each file represents one (rarefied) otu table
each row in a file represents one sample
each column in a file represents one diversity metric

Output files are:
each file represents one diversity metric
each row in a file represents one (rarefied) otu table
each column in a file represents one sample

The input directory should contain only otu tables. The output directory should be empty or nonexistant and the example file is optional.

If you have a set of rarefied OTU tables, make sure the example file contains every sample present in the otu talbes. You should typically choose the file with the fewest sequences per sample, to avoid files with sparse samples omitted.


Example usage: 
Print help message and exit
 collate_alpha.py -h

Example: The user inputs the results from batch alpha diversity (e.g. alpha_div/) and the location where the results should be written (e.g. collated_alpha/), as shown by the following command
 collate_alpha.py -i alpha_div/ -o collated_alpha/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -e EXAMPLE_PATH, --example_path=EXAMPLE_PATH
                        example alpha_diversity analysis file, containing all
                        samples and all metrics to be included in the collated
                        result[Default: chosen automatically (see usage
                        string)]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input path (a directory) [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path (a directory).  will be created if needed
                        [REQUIRED]
Usage: compare_alpha_diversity.py [options] {-i/--alpha_diversity_fp ALPHA_DIVERSITY_FP -m/--mapping_fp MAPPING_FP -c/--categories CATEGORIES -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script compares the alpha diversity of samples found in a collated alpha
diversity file. The comparison is done not between samples, but between groups
of samples. The groupings are created via the input category passed via
-c/--category. Any samples which have the same value under the catgory will be
grouped.

For example, if your mapping file had a category called 'Treatment' that
separated your samples into three groups (Treatment='Control', Treatment='Drug',
Treatment='2xDose'), passing 'Treatment' to this script would cause it to
compare (Control,Drug), (Control,2xDose), (2xDose, Drug) alpha diversity
values. By default the two-sample t-test will be nonparametric (i.e. using
Monte Carlo permutations to calculate the p-value), though the user has the
option to make the test a parametric t-test.

The script creates an output file in tab-separated format where each row is a
different group comparison. The columns in each row denote which two groups of
samples are being compared, as well as the mean and standard deviation of each
group's alpha diversity. Finally, the t-statistic and p-value are reported for
the comparison. This file can be most easily viewed in a spreadsheet program
such as Excel.

Note: Any iterations of a rarefaction at a given depth will be averaged. For
instance, if your collated_alpha file had 10 iterations of the rarefaction at
depth 480, the scores for the alpha diversity metrics of those 10 iterations
would be averaged (within sample). The iterations are not controlled by this
script; when multiple_rarefactions.py is called, the -n option specifies the
number of iterations that have occurred. The multiple comparison correction
takes into account the number of between group comparisons. If you do not know
the rarefaction depth available or you want to use the deepest rarefaction
level available then do not pass -d/--depth and it will default to using the
deepest available.

If t-statistics and/or p-values are None for any of your comparisons, there are
three possible reasons. The first is that there were undefined values in your
collated alpha diversity input file. This occurs if there were too few
sequences in one or more of the samples in the groups involved in those
comparisons to compute alpha diversity at that depth. You can either rerun
compare_alpha_diversity.py passing a lower value for --depth, or you can re-run alpha diversity
after filtering samples with too few sequences. The second is that you had some
comparison where each treatment was represented by only a single sample. It is
not possible to perform a two-sample t-test on two samples each of length 1, so
None will be reported instead. The third possibility occurs when using the
nonparamteric t-test with small datasets where the Monte Carlo permutations
don't return a p-value because the distribution of the data has no variance.
The multiple comparisons correction will not penalize you for comparisons that
return as None regardless of origin.

If the means/standard deviations are None for any treatment group, the likely
cause is that there is an 'n\a' value in the collated_alpha file that was
passed.


Example usage: 
Print help message and exit
 compare_alpha_diversity.py -h

Comparing alpha diversities: The following command takes the following input: a mapping file (which associaties each sample with a number of characteristics), alpha diversity metric (the results of collate_alpha for an alpha diverity metric, like PD_whole_tree), depth (the rarefaction depth to use for comparison), category (the category in the mapping file to determine which samples to compare to each other), and output filepath (a path to the output file to be created). A nonparametric two sample t-test is run to compare the alpha diversities using the default number of Monte Carlo permutations (999).
 compare_alpha_diversity.py -i PD_whole_tree.txt -m mapping.txt -c Treatment -d 100 -o Treatment_PD100

Comparing alpha diversities: Similar to above, but performs comparisons for two categories.
 compare_alpha_diversity.py -i PD_whole_tree.txt -m mapping.txt -c Treatment,DOB -d 100 -o Treatment_DOB_PD100

Parametric t-test: The following command runs a parametric two sample t-test using the t-distribution instead of Monte Carlo permutations at rarefaction depth 100.
 compare_alpha_diversity.py -i PD_whole_tree.txt -m mapping.txt -c Treatment -d 100 -o PD_d100_parametric -t parametric

Parametric t-test: The following command runs a parametric two sample t-test using the t-distribution instead of Monte Carlo permutations at the greatest depth available.
 compare_alpha_diversity.py -i PD_whole_tree.txt -m mapping.txt -c Treatment -o PD_dmax_parametric -t parametric

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t TEST_TYPE, --test_type=TEST_TYPE
                        the type of test to perform when calculating the
                        p-values. Valid choices: parametric, nonparametric. If
                        test_type is nonparametric, Monte Carlo permutations
                        will be used to determine the p-value. If test_type is
                        parametric, the num_permutations option will be
                        ignored and the t-distribution will be used instead
                        [default: nonparametric]
  -n NUM_PERMUTATIONS, --num_permutations=NUM_PERMUTATIONS
                        the number of permutations to perform when calculating
                        the p-value. Must be greater than 10. Only applies if
                        test_type is nonparametric [default: 999]
  -p CORRECTION_METHOD, --correction_method=CORRECTION_METHOD
                        method to use for correcting multiple comparisons.
                        Available methods are bonferroni, fdr, or none.
                        [default: bonferroni]
  -d DEPTH, --depth=DEPTH
                        depth of rarefaction file to use [default: greatest
                        depth]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i ALPHA_DIVERSITY_FP, --alpha_diversity_fp=ALPHA_DIVERSITY_FP
                        path to collated alpha diversity file (as generated by
                        collate_alpha.py) [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to the mapping file [REQUIRED]
    -c CATEGORIES, --categories=CATEGORIES
                        comma-separated list of categories for comparison
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory where output files should be stored
                        [REQUIRED]
Usage: compare_categories.py [options] {--method METHOD -i/--input_dm INPUT_DM -m/--mapping_file MAPPING_FILE -c/--categories CATEGORIES -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script allows for the analysis of the strength and statistical
significance of sample groupings using a distance matrix as the primary input.
Several statistical methods are available: adonis, ANOSIM, BIO-ENV, Moran's I,
MRPP, PERMANOVA, PERMDISP, and db-RDA.

Note: R's vegan and ape packages are used to compute many of these methods, and
for the ones that are not, their implementations are based on the
implementations found in those packages. It is recommended to read through the
detailed descriptions provided by the authors (they are not reproduced here)
and to refer to the primary literature for complete details, including the
methods' assumptions. To view the documentation of a method in R, prepend a
question mark before the method name. For example:

?vegan::adonis

The following are brief descriptions of the available methods:

adonis - Partitions a distance matrix among sources of variation in order to
describe the strength and significance that a categorical or continuous
variable has in determining variation of distances. This is a nonparametric
method and is nearly equivalent to db-RDA (see below) except when distance
matrices constructed with semi-metric or non-metric dissimilarities are
provided, which may result in negative eigenvalues. adonis is very similar to
PERMANOVA, though it is more robust in that it can accept either categorical or
continuous variables in the metadata mapping file, while PERMANOVA can only
accept categorical variables. See vegan::adonis for more details.

ANOSIM - Tests whether two or more groups of samples are significantly
different based on a categorical variable found in the metadata mapping file.
You can specify a category in the metadata mapping file to separate
samples into groups and then test whether there are significant differences
between those groups. For example, you might test whether 'Control' samples are
significantly different from 'Fast' samples. Since ANOSIM is nonparametric,
significance is determined through permutations. See vegan::anosim for more
details.

BIO-ENV - Finds subsets of variables whose Euclidean distances (after scaling
the variables) are maximally rank-correlated with the distance matrix. For
example, the distance matrix might contain UniFrac distances between
communities, and the variables might be numeric environmental variables (e.g.,
pH and latitude). Correlation between the community distance matrix and
Euclidean environmental distance matrix is computed using Spearman's rank
correlation coefficient (rho). This method will only accept categories that are
numerical (continuous or discrete). This is currently the only method in the
script that accepts more than one category (via -c). See vegan::bioenv for more
details. This method is also known as BEST (previously called BIO-ENV) in the
PRIMER-E software package.

Moran's I - This method uses the numerical (e.g. geographical) data supplied to
identify what type of spatial configuration occurs in the samples. For example,
are they dispersed, clustered, or of no distinctly noticeable configuration
when compared to each other? This method will only accept a category that is
numerical. See ape::Moran.I for more details.

MRPP - This method tests whether two or more groups of samples are
significantly different based on a categorical variable found in the metadata
mapping file. You can specify a category in the metadata mapping file to
separate samples into groups and then test whether there are significant
differences between those groups. For example, you might test whether 'Control'
samples are significantly different from 'Fast' samples. Since MRPP is
nonparametric, significance is determined through permutations. See
vegan::mrpp for more details.

PERMANOVA - This method is very similar to adonis except that it only accepts a
categorical variable in the metadata mapping file. It uses an ANOVA
experimental design and returns a pseudo-F value and a p-value. Since PERMANOVA
is nonparametric, significance is determined through permutations.

PERMDISP - This method analyzes the multivariate homogeneity of group
dispersions (variances). In essence, it determines whether the variances of
groups of samples are significantly different. The results of both parametric
and nonparametric significance tests are provided in the output. This method is
generally used as a companion to PERMANOVA. See vegan::betadisper for more
details.

db-RDA - This method is very similar to adonis and will only differ if certain
non-Euclidean semi- or non-metrics are used to generate the input distance
matrix, and negative eigenvalues are encountered. The only difference then will
be in the p-values, not the R^2 values. As part of the output, an ordination
plot is also generated that shows grouping/clustering of samples based on a
category in the metadata mapping file. This category is used to explain the
variability between samples. Thus, the ordination output of db-RDA is similar
to PCoA except that it is constrained, while PCoA is unconstrained (i.e. with
db-RDA, you must specify which category should be used to explain the
variability in your data). See vegan::capscale for more details.

For more information and examples pertaining to this script, please refer to
the accompanying tutorial, which can be found at
http://qiime.org/tutorials/category_comparison.html.


Example usage: 
Print help message and exit
 compare_categories.py -h

adonis example: Runs the adonis statistical method on a distance matrix and mapping file using the Treatment category and 999 permutations, writing the output to the 'adonis_out' directory.
 compare_categories.py --method adonis -i unweighted_unifrac_dm.txt -m Fasting_Map.txt -c Treatment -o adonis_out -n 999

ANOSIM example: Runs the ANOSIM statistical method on a distance matrix and mapping file using the Treatment category and 99 permutations, writing the output to the 'anosim_out' directory.
 compare_categories.py --method anosim -i unweighted_unifrac_dm.txt -m Fasting_Map.txt -c Treatment -o anosim_out -n 99

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NUM_PERMUTATIONS, --num_permutations=NUM_PERMUTATIONS
                        the number of permutations to use when calculating
                        statistical significance. Only applies to adonis,
                        ANOSIM, MRPP, PERMANOVA, PERMDISP, and db-RDA. Must be
                        greater than or equal to zero [default: 999]

  REQUIRED options:
    The following options must be provided under all circumstances.

    --method=METHOD     the statistical method to use. Valid options: adonis,
                        anosim, bioenv, morans_i, mrpp, permanova, permdisp,
                        dbrda [REQUIRED]
    -i INPUT_DM, --input_dm=INPUT_DM
                        the input distance matrix. WARNING: Only symmetric,
                        hollow distance matrices may be used as input.
                        Asymmetric distance matrices, such as those obtained
                        by the UniFrac Gain metric (i.e. beta_diversity.py -m
                        unifrac_g), should not be used as input [REQUIRED]
    -m MAPPING_FILE, --mapping_file=MAPPING_FILE
                        the metadata mapping file [REQUIRED]
    -c CATEGORIES, --categories=CATEGORIES
                        a comma-delimited list of categories from the mapping
                        file. Note: all methods except for BIO-ENV accept just
                        a single category. If multiple categories are
                        provided, only the first will be used [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
Usage: compare_distance_matrices.py [options] {--method METHOD -i/--input_dms INPUT_DMS -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script compares two or more distance/dissimilarity matrices for correlation by providing the Mantel, partial Mantel, and Mantel correlogram matrix correlation tests.

The Mantel test will test the correlation between two matrices. The data often represents the "distance" between objects or samples.

The partial Mantel test is a first-order correlation analysis that utilizes three distance (dissimilarity) matrices. This test builds on the traditional Mantel test which is a procedure that tests the hypothesis that distances between the objects within a given matrix are linearly independent of the distances withing those same objects in a separate matrix. It builds on the traditional Mantel test by adding a third "control" matrix.

Mantel correlogram produces a plot of distance classes versus Mantel statistics. Briefly, an ecological distance matrix (e.g. UniFrac distance matrix) and a second distance matrix (e.g. spatial distances, pH distances, etc.) are provided. The second distance matrix has its distances split into a number of distance classes (the number of classes is determined by Sturge's rule). A Mantel test is run over these distance classes versus the ecological distance matrix. The Mantel statistics obtained from each of these tests are then plotted in a correlogram. A filled-in point on the plot indicates that the Mantel statistic was statistically significant (you may provide what alpha to use).

For more information and examples pertaining to this script, please refer to the accompanying tutorial, which can be found at http://qiime.org/tutorials/distance_matrix_comparison.html.


Example usage: 
Print help message and exit
 compare_distance_matrices.py -h

Partial Mantel: Performs a partial Mantel test on two distance matrices, using a third matrix as a control. Runs 99 permutations to calculate the p-value.
 compare_distance_matrices.py --method partial_mantel -i weighted_unifrac_dm.txt,unweighted_unifrac_dm.txt -c PH_dm.txt -o partial_mantel_out -n 99

Mantel: Performs a Mantel test on all pairs of four distance matrices, including 999 permutations for each test.
 compare_distance_matrices.py --method mantel -i weighted_unifrac_dm.txt,unweighted_unifrac_dm.txt,weighted_unifrac_even100_dm.txt,unweighted_unifrac_even100_dm.txt -o mantel_out -n 999

Mantel Correlogram: This example computes a Mantel correlogram on two distance matrices using 999 permutations in each Mantel test. Output is written to the mantel_correlogram_out directory.
 compare_distance_matrices.py --method mantel_corr -i unweighted_unifrac_dm.txt,PH_dm.txt -o mantel_correlogram_out -n 999

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NUM_PERMUTATIONS, --num_permutations=NUM_PERMUTATIONS
                        the number of permutations to perform when calculating
                        the p-value [default: 100]
  -s SAMPLE_ID_MAP_FP, --sample_id_map_fp=SAMPLE_ID_MAP_FP
                        Map of original sample ids to new sample ids [default:
                        none]
  -t TAIL_TYPE, --tail_type=TAIL_TYPE
                        the type of tail test to perform when calculating the
                        p-value. Valid options: [two-sided, less, greater].
                        "two-sided" is a two-tailed test, while "less" tests
                        for r statistics less than the observed r statistic,
                        and "greater" tests for r statistics greater than the
                        observed r statistic. Only applies when method is
                        mantel [default: two-sided]
  -a ALPHA, --alpha=ALPHA
                        the value of alpha to use when denoting significance
                        in the correlogram plot. Only applies when method is
                        mantel_corr
  -g IMAGE_TYPE, --image_type=IMAGE_TYPE
                        the type of image to produce. Valid options: [png,
                        svg, pdf]. Only applies when method is mantel_corr
                        [default: pdf]
  --variable_size_distance_classes
                        if this option is supplied, each distance class will
                        have an equal number of distances (i.e. pairwise
                        comparisons), which may result in variable sizes of
                        distance classes (i.e. each distance class may span a
                        different range of distances). If this option is not
                        supplied, each distance class will have the same
                        width, but may contain varying numbers of pairwise
                        distances in each class. This option can help maintain
                        statistical power if there are large differences in
                        the number of distances in each class. See Darcy et
                        al. 2011 (PLoS ONE) for an example of this type of
                        correlogram. Only applies when method is mantel_corr
                        [default: False]
  -c CONTROL_DM, --control_dm=CONTROL_DM
                        the control matrix. Only applies (and is *required*)
                        when method is partial_mantel. [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    --method=METHOD     matrix correlation method to use. Valid options:
                        [mantel, partial_mantel, mantel_corr] [REQUIRED]
    -i INPUT_DMS, --input_dms=INPUT_DMS
                        the input distance matrices, comma-separated. WARNING:
                        Only symmetric, hollow distance matrices may be used
                        as input. Asymmetric distance matrices, such as those
                        obtained by the UniFrac Gain metric (i.e.
                        beta_diversity.py -m unifrac_g), should not be used as
                        input [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
Usage: compare_taxa_summaries.py [options] {-i/--taxa_summary_fps TAXA_SUMMARY_FPS -o/--output_dir OUTPUT_DIR -m/--comparison_mode COMPARISON_MODE}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script compares two taxa summary files by computing the correlation
coefficient between pairs of samples. This is useful, for example, if you want
to compare the taxonomic composition of mock communities that were assigned
using different taxonomy assigners in order to see if they are correlated or
not. Another example use-case is to compare the taxonomic composition of
several mock community replicate samples to a single expected, or known, sample
community.

This script is also useful for sorting and filling taxa summary files so that
each sample has the same taxa listed in the same order (with missing taxa
reporting an abundance of zero). The sorted and filled taxa summary files can
then be passed to a script, such as plot_taxa_summary.py, to visually compare
the differences using the same taxa coloring scheme.

For more information and examples pertaining to this script, please refer to
the accompanying tutorial, which can be found at
http://qiime.org/tutorials/taxa_summary_comparison.html.


Example usage: 
Print help message and exit
 compare_taxa_summaries.py -h

Paired sample comparison: Compare all samples that have matching sample IDs between the two input taxa summary files using the pearson correlation coefficient. The first input taxa summary file is from the overview tutorial, using the RDP classifier with a confidence level of 0.60 and the gg_otus_4feb2011 97% representative set. The second input taxa summary file was generated the same way, except for using a confidence level of 0.80.
 compare_taxa_summaries.py -i ts_rdp_0.60.txt,ts_rdp_0.80.txt -m paired -o taxa_comp

Paired sample comparison with sample ID map: Compare samples based on the mappings in the sample ID map using the spearman correlation coefficient. The second input taxa summary file is simply the original ts_rdp_0.60.txt file with all sample IDs containing 'PC.' renamed to 'S.'.
 compare_taxa_summaries.py -i ts_rdp_0.80.txt,ts_rdp_0.60_renamed.txt -m paired -o taxa_comp_using_sample_id_map -s sample_id_map.txt -c spearman

Detailed paired sample comparison: Compare all samples that have matching sample IDs between the two input taxa summary files using the pearson correlation coefficient. Additionally, compute the correlation coefficient between each pair of samples individually.
 compare_taxa_summaries.py -i ts_rdp_0.60.txt,ts_rdp_0.80.txt -m paired -o taxa_comp_detailed --perform_detailed_comparisons

One-tailed test: Compare all samples that have matching sample IDs between the two input taxa summary files using the pearson correlation coefficient. Perform a one-tailed (negative association) test of significance for both parametric and nonparametric tests. Additionally, compute a 90% confidence interval for the correlation coefficient. Note that the confidence interval will still be two-sided.
 compare_taxa_summaries.py -i ts_rdp_0.60.txt,ts_rdp_0.80.txt -m paired -o taxa_comp_one_tailed -t low -l 0.90

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -c CORRELATION_TYPE, --correlation_type=CORRELATION_TYPE
                        the type of correlation coefficient to compute. Valid
                        choices: pearson or spearman [default: pearson]
  -t TAIL_TYPE, --tail_type=TAIL_TYPE
                        the type of tail test to compute when calculating the
                        p-values. "high" specifies a one-tailed test for
                        values greater than the observed correlation
                        coefficient (positive association), while "low"
                        specifies a one-tailed test for values less than the
                        observed correlation coefficient (negative
                        association). "two-sided" specifies a two-tailed test
                        for values greater in magnitude than the observed
                        correlation coefficient. Valid choices: low or high or
                        two-sided [default: two-sided]
  -n NUM_PERMUTATIONS, --num_permutations=NUM_PERMUTATIONS
                        the number of permutations to perform when calculating
                        the nonparametric p-value. Must be an integer greater
                        than or equal to zero. If zero, the nonparametric test
                        of significance will not be performed and the
                        nonparametric p-value will be reported as "N/A"
                        [default: 999]
  -l CONFIDENCE_LEVEL, --confidence_level=CONFIDENCE_LEVEL
                        the confidence level of the correlation coefficient
                        confidence interval. Must be a value between 0 and 1
                        (exclusive). For example, a 95% confidence interval
                        would be 0.95 [default: 0.95]
  -s SAMPLE_ID_MAP_FP, --sample_id_map_fp=SAMPLE_ID_MAP_FP
                        map of original sample IDs to new sample IDs. Use this
                        to match up sample IDs that should be compared between
                        the two taxa summary files. Each line should contain
                        an original sample ID, a tab, and the new sample ID.
                        All original sample IDs from the two input taxa
                        summary files must be mapped. This option only applies
                        if the comparison mode is "paired". If not provided,
                        only sample IDs that exist in both taxa summary files
                        will be compared [default: none]
  -e EXPECTED_SAMPLE_ID, --expected_sample_id=EXPECTED_SAMPLE_ID
                        the sample ID in the second "expected" taxa summary
                        file to compare all samples to. This option only
                        applies if the comparison mode is "expected". If not
                        provided, the second taxa summary file must have only
                        one sample [default: none]
  --perform_detailed_comparisons
                        Perform a comparison for each sample pair in addition
                        to the single overall comparison. The results will
                        include the Bonferroni-corrected p-values in addition
                        to the original p-values [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i TAXA_SUMMARY_FPS, --taxa_summary_fps=TAXA_SUMMARY_FPS
                        the two input taxa summary filepaths, comma-separated.
                        These will usually be the files that are output by
                        summarize_taxa.py. These taxa summary files do not
                        need to have the same taxa in the same order, as the
                        script will make them compatible before comparing them
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
    -m COMPARISON_MODE, --comparison_mode=COMPARISON_MODE
                        the type of comparison to perform. Valid choices:
                        paired or expected. "paired" will compare each sample
                        in the taxa summary files that match based on sample
                        ID, or that match given a sample ID map (see the
                        --sample_id_map_fp option for more information).
                        "expected" will compare each sample in the first taxa
                        summary file to an expected sample (contained in the
                        second taxa summary file). If "expected", the second
                        taxa summary file must contain only a single sample
                        that all other samples will be compared to (unless the
                        --expected_sample_id option is provided) [REQUIRED]
Usage: compare_trajectories.py [options] {-i/--input_fp INPUT_FP -m/--map_fp MAP_FP -c/--categories CATEGORIES -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script mainly allows performing analysis of volatility on time series data, but they can be applied to any data that contains a gradient. The methods available are RMS (either using 'avg' or 'trajectory'); or the first difference (using 'diff'), or 'wdiff' for a modified first difference algorithm. The trajectories are computed as follows. For 'avg' it calculates the average point within a group and then computes the norm of the distance of each sample from the averaged value. For 'trajectory' each component of the result trajectory is computed as taking the sorted list of samples in the group and taking the norm of the coordinates of the 2nd samples minus the 1st sample, 3rd sample minus 2nd sample and so on. For 'diff' it calculates the norm for all the time-points and then calculates the first difference for each resulting point. For 'wdiff', it calculates the norm for all the time-points and substracts the mean of the next number of elements, specified using the '--window_size' parameters, and the current element.

Example usage: 
Print help message and exit
 compare_trajectories.py -h

Average method: Execute the analysis of volatility using the average method, grouping the samples using the 'Treatment' category
 compare_trajectories.py -i pcoa_res.txt -m map.txt -c 'Treatment' -o avg_output

Trajectory method: Execute the analysis of volatility using the trajectory method, grouping the samples using the 'Treatment' category and sorting them using the 'time' category
 compare_trajectories.py -i pcoa_res.txt -m map.txt -c 'Treatment' --algorithm trajectory -o trajectory_output -s time

First difference method: Execute the analysis of volatility using the first difference method, grouping the samples using the 'Treatment' category, sorting them using the 'time' category and calculating the trajectory using the first four axes
 compare_trajectories.py -i pcoa_res.txt -m map.txt -c 'Treatment' --algorithm diff -o diff_output -s time --axes 4

Window difference method: Execute the analysis of volatility using the window difference method, grouping the samples using the 'Treatment' category, sorting them using the 'time' category, weighting the output by the space between samples in the 'time' category and using a window size of three.
 compare_trajectories.py -i pcoa_res.txt -m map.txt -c 'Treatment' --algorithm wdiff -o wdiff_output -s time --window_size 3 -w

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s SORT_BY, --sort_by=SORT_BY
                        Category name of the mapping file to use to sort
  --algorithm=ALGORITHM
                        The algorithm to use. Available methods: ['avg',
                        'trajectory', 'diff', 'wdiff']. [Default: avg]
  --axes=AXES           The number of axes to account while doing the
                        trajectory specific calculations. We suggest using 3
                        because those are the ones being displayed in the
                        plots but you could use any number between 1 and
                        number of samples - 1. To use all of them pass 0.
                        [default: 3]
  -w, --weight_by_vector
                        Use -w when you want the output to be weighted by the
                        space between samples in the --sort_by column, i. e.
                        days between samples [default: False]
  --window_size=WINDOW_SIZE
                        Use --window_size, when selecting the modified first
                        difference ('wdiff') option for --algorithm. This
                        integer determines the number of elements to be
                        averaged per element subtraction, the resulting
                        trajectory. [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        Input ordination results filepath [REQUIRED]
    -m MAP_FP, --map_fp=MAP_FP
                        Input metadata mapping filepath [REQUIRED]
    -c CATEGORIES, --categories=CATEGORIES
                        Comma-separated list of category names of the mapping
                        file to use to create the trajectories [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Name of the output directory to save the results
                        [REQUIRED]
Usage: compute_core_microbiome.py [options] {-i/--input_fp INPUT_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 compute_core_microbiome.py -h

Identify the core OTUs in otu_table.biom, defined as the OTUs that are present in at least 50% of the samples. Write the list of core OTUs to a text file, and a new BIOM file containing only the core OTUs.
 compute_core_microbiome.py -i otu_table.biom -o otu_table_core

Identify the core OTUs in otu_table.biom, defined as the OTUs that are present in all of the samples in the 'Fast' treatment (as specified in the mapping file). Write the list of core OTUs to a text file.
 compute_core_microbiome.py -i otu_table.biom -o otu_table_core_fast --mapping_fp map.txt --valid_states "Treatment:Fast"

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --max_fraction_for_core=MAX_FRACTION_FOR_CORE
                        the max fractions of samples that an OTU must be
                        observed in to be considered part of the core as a
                        number in the range [0,1] [default: 1.0]
  --min_fraction_for_core=MIN_FRACTION_FOR_CORE
                        the min fractions of samples that an OTU must be
                        observed in to be considered part of the core as a
                        number in the range [0,1] [default: 0.5]
  --num_fraction_for_core_steps=NUM_FRACTION_FOR_CORE_STEPS
                        the number of evenly sizes steps to take between
                        min_fraction_for_core and max_fraction_for_core
                        [default: 11]
  --otu_md=OTU_MD       the otu metadata category to write to the output file
                        [defualt: taxonomy]
  --mapping_fp=MAPPING_FP
                        mapping file path (for use with --valid_states)
                        [default: none]
  --valid_states=VALID_STATES
                        description of sample ids to retain (for use with
                        --mapping_fp) [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        the input otu table in BIOM format [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory to store output data [REQUIRED]
Usage: compute_taxonomy_ratios.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Compute the log ratio abundance of specified taxonomic groups. This method is based on the microbial dysbiosis index described in Gevers et al. 2014: http://www.ncbi.nlm.nih.gov/pubmed/24629344

Example usage: 
Print help message and exit
 compute_taxonomy_ratios.py -h

Example: Compute the microbial dysbiosis (MD) index
 compute_taxonomy_ratios.py -i table.biom.gz -e md -o md.txt

Example: Compute the microbial dysbiosis (MD) index and add it to an existing mapping file
 compute_taxonomy_ratios.py -i table.biom.gz -e md -o map_w_md.txt -m map.txt

Example: Compute the log of the abundance of p__Firmicutes plus p__Fusobacteria divided by the abundance of p__Bacteroidetes and write the results to custom_index.txt.
 compute_taxonomy_ratios.py -i table.biom.gz --increased p__Firmicutes,p__Fusobacteria --decreased p__Bacteroidetes -o custom_index.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -i INPUT, --input=INPUT
                        The input BIOM table [REQUIRED if not passing -s]
  -o OUTPUT, --output=OUTPUT
                        Path to where the output will be written; this will be
                        a new sample metadata mapping file [REQUIRED if not
                        passing -s]
  --increased=INCREASED
                        Comma-separated list of taxa whose abundances are
                        included in the numerator of the ratio [REQUIRED if
                        not passing -s or -e]
  --decreased=DECREASED
                        Comma-separated list of taxa whose abundances are
                        included in the denominator of the ratio [REQUIRED if
                        not passing -s or -e]
  -e INDEX, --index=INDEX
                        Apply an existing index. Options are: md [REQUIRED if
                        not passing -s or --increased and --decreased]
  -n NAME, --name=NAME  Column name for the index in the output file [default:
                        'index', or value passed as -e if provided]
  -m MAPPING_FILE, --mapping_file=MAPPING_FILE
                        A mapping file containing data that should be included
                        in the output file [default: no additional mapping
                        file data is included in output]
  -k KEY, --key=KEY     Metadata key to use for computing index [default:
                        taxonomy]
  -s, --show_indices    List known indices and exit [default: False]
Usage: conditional_uncovered_probability.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script calculates the conditional uncovered probability for each sample
in an OTU table. It uses the methods introduced in Lladser, Gouet, and Reeder,
"Extrapolation of Urn Models via Poissonization: Accurate Measurements of the
Microbial Unknown" PLoS 2011.

Specifically, it computes a point estimate and a confidence interval using two
different methods. Thus it can happen that the PE is actually outside of the
CI.

We only provide the ability to generate 95% (alpha=0.95) CIs. The CIs are ULCL
CIs; they provide an upper and lower bound, where the lower bound is
conservative. The CIs are constructed using an upper-to-lower bound ratio of
10.

The CI method requires precomputed constants that depend on the lookahead. We
only provide constants for r=3..25,30,40,50.



Example usage: 
Print help message and exit
 conditional_uncovered_probability.py -h

Default case: To calculate the cond. uncovered probability with the default values, you can use the following command
 conditional_uncovered_probability.py -i otu_table.biom -o cup.txt

Change lookahead: To change the accuracy of the prediction change the lookahead value. Larger values of r lead to more precise predictions, but might be unfeasable for small samples. For deeply sequenced samples, try increasing r to 50
 conditional_uncovered_probability.py -i otu_table.biom -o cup_r50.txt -r 50

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -i INPUT_PATH, --input_path=INPUT_PATH
                        Input OTU table filepath. [default: none]
  -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        Output filepath to store the predictions. [default:
                        none]
  -r LOOK_AHEAD, --look_ahead=LOOK_AHEAD
                        Number of unobserved, new colors necessary for
                        prediction. [default: 25]
  -m METRICS, --metrics=METRICS
                        CUP metric(s) to use. A comma-separated list should be
                        provided when multiple metrics are specified.
                        [default: lladser_pe,lladser_ci]
  -s, --show_metrics    Show the available CUP metrics and exit.
Usage: consensus_tree.py [options] {-i/--input_dir INPUT_DIR -o/--output_fname OUTPUT_FNAME}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 consensus_tree.py -h

basic usage: given a directory of trees 'jackknifed_trees', compute the majority consensus and save as a newick formatted text file: 
 consensus_tree.py -i jackknifed_trees -o consensus_tree.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s, --strict          Use only nodes occurring >50% of the time [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        input folder containing trees [REQUIRED]
    -o OUTPUT_FNAME, --output_fname=OUTPUT_FNAME
                        the output consensus tree filepath [REQUIRED]
Usage: convert_fastaqual_fastq.py [options] {-f/--fasta_file_path FASTA_FILE_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

From a FASTA file and a matching QUAL file, generates a FASTQ file. A minimal FASTQ file omits the redundant sequence label on the quality scores; the quality scores for a sequence are assumed to follow immediately after the sequence with which they are associated. The output FASTQ file will be generated in the specified output directory with the same name as the input FASTA file, suffixed with '.fastq'. A FASTQ file will be split into FASTA and QUAL files, and generated in the designated output directory.

Example usage: 
Print help message and exit
 convert_fastaqual_fastq.py -h

Example: Using the input files seqs.fna and seqs.qual, generate seqs.fastq in the fastq_files directory
 convert_fastaqual_fastq.py -f seqs.fna -q seqs.qual -o fastq_files/

Example: Using input seqs.fastq generate fasta and qual files in fastaqual directory
 convert_fastaqual_fastq.py -c fastq_to_fastaqual -f seqs.fastq -o fastaqual

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -q QUAL_FILE_PATH, --qual_file_path=QUAL_FILE_PATH
                        Required input QUAL file if converting to FASTQ.
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Output directory. Will be created if does not exist.
                        [default: .]
  -c CONVERSION_TYPE, --conversion_type=CONVERSION_TYPE
                        type of conversion: fastaqual_to_fastq or
                        fastq_to_fastaqual [default: fastaqual_to_fastq]
  -a ASCII_INCREMENT, --ascii_increment=ASCII_INCREMENT
                        The number to add (subtract if coverting from FASTQ)
                        to the quality score to get the ASCII character (or
                        numeric quality score). [default: 33]
  -F, --full_fasta_headers
                        Include full FASTA headers in output file(s) (as
                        opposed to merely the sequence label). [default:
                        False]
  -b, --full_fastq      Include identifiers on quality lines in the FASTQ file
                        (those beginning with a "+"). Irrelevant when
                        converting from FASTQ. [default=False]
  -m, --multiple_output_files
                        Create multiple FASTQ files, one for each sample, or
                        create multiple matching FASTA/QUAL for each sample.
                        [default=False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f FASTA_FILE_PATH, --fasta_file_path=FASTA_FILE_PATH
                        Input FASTA or FASTQ file. [REQUIRED]

This script has been moved to the FastUnifrac repository. For more information, see http://github.com/qiime/FastUnifrac
Usage: core_diversity_analyses.py [options] {-i/--input_biom_fp INPUT_BIOM_FP -o/--output_dir OUTPUT_DIR -m/--mapping_fp MAPPING_FP -e/--sampling_depth SAMPLING_DEPTH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script plugs several QIIME diversity analyses together to form a basic workflow beginning with a BIOM table, mapping file, and optional phylogenetic tree.

The included scripts are those run by the workflow scripts alpha_rarefaction.py, beta_diversity_through_plots.py, summarize_taxa_through_plots.py, plus the (non-workflow) scripts make_distance_boxplots.py, compare_alpha_diversity.py, and group_significance.py. To update parameters to the workflow scripts, you should pass the same parameters file that you would pass if calling the workflow script directly.

Additionally, a table summary is generated by running the 'biom summarize-table' command (part of the biom-format package). To update parameters to this command, your parameters file should use 'biom-summarize-table' (without quotes) as the script name. See http://qiime.org/documentation/qiime_parameters_files.html for more details.


Example usage: 
Print help message and exit
 core_diversity_analyses.py -h

Run diversity analyses at 20 sequences/sample, with categorical analyses focusing on the SampleType and day categories. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 core_diversity_analyses.py -i $PWD/otu_table.biom -o $PWD/core_output -m $PWD/map.txt -c SampleType,day -t $PWD/rep_set.tre -e 20

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. For more information, see www.qi
                        ime.org/documentation/qiime_parameters_files.html [if
                        omitted, default values will be used]
  -a, --parallel        Run in parallel where available. Specify number of
                        jobs to start with -O or in the parameters file.
                        [default: False]
  --nonphylogenetic_diversity
                        Apply non-phylogenetic alpha (chao1 and observed_otus)
                        and beta (bray_curtis) diversity calculations. This is
                        useful if, for example, you are working with non-
                        amplicon BIOM tables, or if a reliable tree is not
                        available (e.g., if you're  working with ITS
                        amplicons) [default: False]
  --suppress_taxa_summary
                        Suppress generation of taxa summary plots. [default:
                        False]
  --suppress_beta_diversity
                        Suppress beta diversity analyses. [default: False]
  --suppress_alpha_diversity
                        Suppress alpha diversity analyses. [default: False]
  --suppress_group_significance
                        Suppress OTU/category significance analysis. [default:
                        False]
  -t TREE_FP, --tree_fp=TREE_FP
                        Path to the tree file if one should be used. Required
                        unless --nonphylogenetic_diversity is passed.
                        [default: no tree will be used]
  -c CATEGORIES, --categories=CATEGORIES
                        The metadata category or categories to compare (i.e.,
                        column headers in the mapping file) for categorical
                        analyses. These should be passed  as a comma-separated
                        list. [default: none; do not perform categorical
                        analyses]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging or recovering from failed runs. [default:
                        False]
  --recover_from_failure
                        Don't fail if output directory exists, but attempt to
                        recover from the failed run. [default: False]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start. NOTE: you must also pass -a
                        to run in parallel, this defines the number of jobs to
                        be started if and only if -a is passed [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_BIOM_FP, --input_biom_fp=INPUT_BIOM_FP
                        the input biom file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath [REQUIRED]
    -e SAMPLING_DEPTH, --sampling_depth=SAMPLING_DEPTH
                        Sequencing depth to use for even sub-sampling and
                        maximum rarefaction depth. You should review the
                        output of the 'biom summarize-table' command to decide
                        on this value. [REQUIRED]
Usage: count_seqs.py [options] {-i/--input_fps INPUT_FPS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 count_seqs.py -h

Count the sequences in a fasta file and write results to stdout.
 count_seqs.py -i in.fasta

Count the sequences in a fasta file and a fastq file and write results to file. Note that fastq files can only be processed if they end with .fastq -- all other files are assumed to be fasta.
 count_seqs.py -i in1.fasta,in2.fastq -o seq_counts.txt

Count the sequences all .fasta files in current directory and write results to stdout. Note that -i option must be quoted.
 count_seqs.py -i "*.fasta"

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath [default: write to stdout]
  --suppress_errors     Suppress warnings about missing files [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FPS, --input_fps=INPUT_FPS
                        the input filepaths (comma-separated) [REQUIRED]
Usage: demultiplex_fasta.py [options] {-m/--map MAP_FNAME -f/--fasta FASTA_FNAMES}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Using barcodes and/or data from fasta labels provided in a mapping file, will demultiplex sequences from an input fasta file.  Barcodes will be removed from the sequences in the output fasta file by default.  If a quality scores file is supplied, the quality score file will be truncated to match the output fasta file.  The default barcode type are 12 base pair Golay codes.  Alternative barcodes allowed are 8 base pair Hamming codes, variable_length, or generic barcodes of a specified length.  Generic barcodes utilize mismatch counts for correction.  One can also use an added demultiplex field (-j option) to specify data in the fasta labels that can be used alone or in conjunction with barcode sequences for demultiplexing.  All barcode correction is disabled when variable length barcodes are used.

Example usage: 
Print help message and exit
 demultiplex_fasta.py -h

Standard Example: Using a single 454 run, which contains a single FASTA, QUAL, and mapping file while using default parameters and outputting the data into the Directory "demultiplexed_output"
 demultiplex_fasta.py -m Mapping_File_golay.txt -f 1.TCA.454Reads.fna -q 1.TCA.454Reads.qual -o demultiplexed_output/

For the case where there are multiple FASTA and QUAL files, the user can run the following command as long as there are not duplicate barcodes listed in the mapping file
 demultiplex_fasta.py -m Mapping_File_golay.txt -f 1.TCA.454Reads.fna,2.TCA.454Reads.fna -q 1.TCA.454Reads.qual,2.TCA.454Reads.qual -o demultiplexed_output_comma_separated/

Duplicate Barcode Example: An example of this situation would be a study with 1200 samples. You wish to have 400 samples per run, so you split the analysis into three runs with and reuse barcodes (you only have 600). After initial analysis you determine a small subset is underrepresented (<500 sequences per samples) and you boost the number of sequences per sample for this subset by running a fourth run. Since the same sample IDs are in more than one run, it is likely that some sequences will be assigned the same unique identifier by demultiplex_fasta.py when it is run separately on the four different runs, each with their own barcode file. This will cause a problem in file concatenation of the four different runs into a single large file. To avoid this, you can use the '-n' parameter which defines a start index for demultiplex_fasta.py fasta label enumeration. From experience, most 454 runs (when combining both files for a single plate) will have 350,000 to 650,000 sequences. Thus, if Run 1 for demultiplex_fasta.py uses '-n 1000000', Run 2 uses '-n 2000000', etc., then you are guaranteed to have unique identifiers after concatenating the results of multiple 454 runs. With newer technologies you will just need to make sure that your start index spacing is greater than the potential number of sequences.

To run demultiplex_fasta.py, you will need two or more (depending on the number of times the barcodes were reused) separate mapping files (one for each Run, for example one Run1 and another one for Run2), then you can run demultiplex_fasta.py using the FASTA and mapping file for Run1 and FASTA and mapping file for Run2. Once you have independently run demultiplex_fasta on each file, followed by quality filtering, you can concatenate (cat) the sequence files generated. You can also concatenate the mapping files, since the barcodes are not necessary for downstream analyses, unless the same sample ids are found in multiple mapping files.

Run demultiplex_fasta.py on Run 1
 demultiplex_fasta.py -m Mapping_File1.txt -f 1.TCA.454Reads.fna -q 1.TCA.454Reads.qual -o demultiplexed_output_Run1/ -n 1000000

Run demultiplex_fasta on Run 2
 demultiplex_fasta.py -m Mapping_File2.txt -f 2.TCA.454Reads.fna -q 2.TCA.454Reads.qual -o demultiplexed_output_Run2/ -n 2000000

Barcode Decoding Example: The standard barcode types supported by demultiplex_fasta.py are golay (Length: 12 NTs) and hamming (Length: 8 NTs). For situations where the barcodes are of a different length than golay and hamming, the user can define a generic barcode type "-b" as an integer, where the integer is the length of the barcode used in the study.

For the case where the generic 8 base pair barcodes were used, you can use the following command
 demultiplex_fasta.py -m Mapping_File_8bp_barcodes.txt -f 1.TCA.454Reads.fna -q 1.TCA.454Reads.qual -o demultiplexed_output_8bp_barcodes/ -b 8

To use the run prefix at the beginning of the fasta label for demultiplexing, there has to be a field in the mapping file labeled "run_prefix", and can be used by the following command
 demultiplex_fasta.py -m Mapping_File_run_prefix.txt -f 1.TCA.454Reads.fna -q 1.TCA.454Reads.qual -o demultiplexed_output_run_prefix/ -j run_prefix

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -q QUAL_FNAMES, --qual=QUAL_FNAMES
                        file paths of qual files, comma-delimited [default:
                        none]
  -B, --keep_barcode    do not remove barcode from sequences
  -b BARCODE_TYPE, --barcode_type=BARCODE_TYPE
                        barcode type, hamming_8, golay_12, variable_length
                        (will disable any barcode correction if
                        variable_length set), or a number representing the
                        length of the barcode, such as -b 4. The max barcode
                        errors (-e) should be lowered for short barcodes.
                        [default: golay_12]
  -o DIR_PREFIX, --dir_prefix=DIR_PREFIX
                        directory prefix for output files [default: .]
  -e MAX_BC_ERRORS, --max_barcode_errors=MAX_BC_ERRORS
                        maximum number of errors in barcode.  If using generic
                        barcodes every 0.5 specified counts as a primer
                        mismatch. [default: 1.5]
  -n START_INDEX, --start_numbering_at=START_INDEX
                        seq id to use for the first sequence [default: 1]
  --retain_unassigned_reads
                        retain sequences which can not be demultiplexed in a
                        seperate output sequence file [default: False]
  -c, --disable_bc_correction
                        Disable attempts to find nearest corrected barcode.
                        Can improve performance. [default: False]
  -F, --save_barcode_frequencies
                        Save frequences of barcodes as they appear in the
                        given sequences.  Sorts in order of largest to
                        smallest.  Will do nothing if barcode type is 0 or
                        variable_length.  [default: False]
  -j ADDED_DEMULTIPLEX_FIELD, --added_demultiplex_field=ADDED_DEMULTIPLEX_FIELD
                        Use -j to add a field to use in the mapping file as an
                        additional demultiplexing option to the barcode.  All
                        combinations of barcodes and the values in these
                        fields must be unique. The fields must contain values
                        that can be parsed from the fasta labels such as
                        "plate=R_2008_12_09".  In this case, "plate" would be
                        the column header and "R_2008_12_09" would be the
                        field data (minus quotes) in the mapping file.  To use
                        the run prefix from the fasta label, such as
                        ">FLP3FBN01ELBSX", where "FLP3FBN01" is generated from
                        the run ID, use "-j run_prefix" and set the run prefix
                        to be used as the data under the column headerr
                        "run_prefix".  [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAP_FNAME, --map=MAP_FNAME
                        name of mapping file. NOTE: Must contain a header line
                        indicating SampleID in the first column and
                        BarcodeSequence in the second, LinkerPrimerSequence in
                        the third. [REQUIRED]
    -f FASTA_FNAMES, --fasta=FASTA_FNAMES
                        names of fasta files, comma-delimited [REQUIRED]
Usage: denoise_wrapper.py [options] {-i/--input_file SFF_FPS -f/--fasta_file FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script will denoise a flowgram file in .sff.txt format, which is the output of sffinfo.

Example usage: 
Print help message and exit
 denoise_wrapper.py -h

Example: Denoise flowgrams in file 454Reads.sff.txt, discard flowgrams not in seqs.fna, and extract primer from map.txt
 denoise_wrapper.py -i 454Reads.sff.txt -f seqs.fna -m map.txt

Multi-core Example: Denoise flowgrams in file 454Reads.sff.txt using 2 cores on your machine in parallel
 denoise_wrapper.py -n 2 -i 454Reads.sff.txt -f seqs.fna -m map.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to output directory [default: denoised_seqs/]
  -n NUM_CPUS, --num_cpus=NUM_CPUS
                        number of CPUs [default: 1]
  --force_overwrite     Overwrite files in output directory [default: False]
  -m MAP_FNAME, --map_fname=MAP_FNAME
                        name of mapping file, Has to contain field
                        LinkerPrimerSequence. [REQUIRED unless --primer
                        specified]
  -p PRIMER, --primer=PRIMER
                        primer sequence [REQUIRED unless --map_fname
                        specified]
  --titanium            Select Titanium defaults for denoiser, otherwise use
                        FLX defaults [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i SFF_FPS, --input_file=SFF_FPS
                        path to flowgram files (.sff.txt), comma separated
                        [REQUIRED]
    -f FASTA_FP, --fasta_file=FASTA_FP
                        path to fasta file from split_libraries.py [REQUIRED]
Usage: denoiser.py [options] {-i/--input_files SFF_FPS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The denoiser removes sequencing noise characteristic to pyrosequencing by flowgram clustering. For a detailed explanation of the underlying algorithm see (Reeder and Knight, Nature Methods 7(9), 2010).

Example usage: 
Print help message and exit
 denoiser.py -h

Run denoiser on flowgrams in 454Reads.sff.txt with read-to-barcode mapping in seqs.fna,
put results into Outdir, log progress in Outdir/denoiser.log
 denoiser.py -i 454Reads.sff.txt -f seqs.fna -v -o Outdir

Multiple sff.txt files: Run denoiser on two flowgram files in 454Reads_1.sff.txt and 454Reads_2.sff.txt
with read-to-barcode mapping in seqs.fna, put results into Outdir,
log progress in Outdir/denoiser.log
 denoiser.py -i 454Reads_1.sff.txt,454Reads_2.sff.txt -f seqs.fna -v -o Outdir

Denoise multiple library separately: Run denoiser on flowgrams in 454Reads.sff.txt with read-to-barcode mapping in seqs.fna,
split input files into libraries and process each library separately,
put results into Outdir, log progress in Outdir/denoiser.log
 denoiser.py -S -i 454Reads.sff.txt -f seqs.fna -v -o Outdir

Resuming a failed run: Resume a previous denoiser run from breakpoint stored in Outdir_from_failed_run/checkpoints/checkpoint100.pickle.
The checkpoint option requires the -p or --preprocess option, which usually can be set to the output dir of the failed run.
All other arguments must be identical to the failed run.
 denoiser.py -i 454Reads.sff.txt -f seqs.fna -v -o Outdir_resumed -p Outdir_from_failed_run --checkpoint Outdir_from_failed_run/checkpoints/checkpoint100.pickle

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -f FASTA_FP, --fasta_fp=FASTA_FP
                        path to fasta input file. Reads not in the fasta file
                        are filtered out before denoising. File format is as
                        produced by split_libraries.py [default: none]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to output directory [default: random dir in ./]
  -c, --cluster         Use cluster/multiple CPUs for flowgram alignments
                        [default: False]
  -p PREPROCESS_FP, --preprocess_fp=PREPROCESS_FP
                        Do not do preprocessing (phase I),instead use already
                        preprocessed data in PREPROCESS_FP
  --checkpoint_fp=CHECKPOINT_FP
                        Resume denoising from checkpoint. Be careful when
                        changing parameters for a resumed run. Requires -p
                        option.  [default: none]
  -s, --squeeze         Use run-length encoding for prefix filtering in phase
                        I [default: False]
  -S, --split           Split input into per library sets and denoise
                        separately [default: False]
  --force               Force overwrite of existing directory [default: False]
  --primer=PRIMER       primer sequence [default: CATGCTGCCTCCCGTAGGAGT]
  -n NUM_CPUS, --num_cpus=NUM_CPUS
                        number of cpus, requires -c [default: 1]
  -m MAX_NUM_ITER, --max_num_iterations=MAX_NUM_ITER
                        maximal number of iterations in phase II. None means
                        unlimited iterations [default: none]
  -b BAIL, --bail_out=BAIL
                        stop clustering in phase II with clusters smaller or
                        equal than BAILde [default: 1]
  --percent_id=PERCENT_ID
                        sequence similarity clustering threshold, expressed as
                        a fraction between 0 and 1 [default: 0.97]
  --low_cut_off=LOW_CUTOFF
                        low clustering threshold for phase II [default: 3.75]
  --high_cut_off=HIGH_CUTOFF
                        high clustering threshold for phase III [default: 4.5]
  --low_memory          Use slower, low memory method [default: False]
  -e ERROR_PROFILE, --error_profile=ERROR_PROFILE
                        path to error profile [default= <qiime-install-path>/q
                        iime/support_files/denoiser/Data/FLX_error_profile.dat
                        ]
  --titanium            shortcut for -e <qiime-install-path>/qiime/support_fil
                        es/denoiser/Data//Titanium_error_profile.dat
                        --low_cut_off=4 --high_cut_off=5 . Warning: overwrites
                        all previous cut-off values [DEFAULT: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i SFF_FPS, --input_files=SFF_FPS
                        path to flowgram files (.sff.txt), comma separated
                        [REQUIRED]
Usage: denoiser_preprocess.py [options] {-i/--input_files SFF_FPS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The script denoiser_preprocess.py runs the first clustering phase
which groups reads based on common prefixes.

Example usage: 
Print help message and exit
 denoiser_preprocess.py -h

Run program on flowgrams in 454Reads.sff. Remove reads which are not in split_lib_filtered_seqs.fasta.
Remove primer CATGCTGCCTCCCGTAGGAGT from reads before running phase I
 denoiser_preprocess.py -i Fasting_Example.sff.txt -f seqs.fna -p CATGCTGCCTCCCGTAGGAGT

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -f FASTA_FP, --fasta_file=FASTA_FP
                        path to fasta input file [default: none]
  -s, --squeeze         Use run-length encoding for prefix filtering [default:
                        False]
  -l LOG_FP, --log_file=LOG_FP
                        path to log file [default: preprocess.log]
  -p PRIMER, --primer=PRIMER
                        primer sequence used for the amplification [default:
                        CATGCTGCCTCCCGTAGGAGT]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to output directory [default: /tmp/]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i SFF_FPS, --input_files=SFF_FPS
                        path to flowgram files (.sff.txt), comma separated
                        [REQUIRED]
Usage: denoiser_worker.py [options] {-f/--file_path FILE_PATH -p/--port PORT -s/--server_address SERVER}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The workers are automatically started by the denoiser.py script.
You usually never need to use this script yourself.

A worker waits for data and does flowgram alignments once it gets it.

Example usage: 
Print help message and exit
 denoiser_worker.py -h

Start worker and connect to server listening on port 12345 on the same machine (localhost)
 denoiser_worker.py -f seqs.fna -f denoiser_out/worker99 -p 12345 -s localhost

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -e ERROR_PROFILE, --error_profile=ERROR_PROFILE
                        Path to error profile [DEFAULT: /home/vsts/conda
                        /conda-bld/qiime_1630456709534/_test_env_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeho
                        ld_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placeh/lib/python2.7/site-packages/qiim
                        e/support_files/denoiser/Data/FLX_error_profile.dat]
  -c COUNTER, --counter=COUNTER
                        Round counter to start this worker with  [default: 0]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f FILE_PATH, --file_path=FILE_PATH
                        path used as prefix for worker data files[REQUIRED]
    -p PORT, --port=PORT
                        Server port [REQUIRED]
    -s SERVER, --server_address=SERVER
                        Server address[REQUIRED]
Usage: detrend.py [options] {-i/--input_fp INPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Ordination plots (e.g. principal coordinates analysis) of samples that lay along a naturally occurring gradient (e.g. depth, time, pH) often exhibit a curved shape known as the "arch" or "horseshoe" effect. This can cause samples near the endpoints of the gradient to appear closer to one another than would be expected. This script will attempt to remove any (compounded) quadratic curvature in a set of 2D coordinates. If requested, it will also report an evaluation of the association of the transformed coordinates with a known gradient.

Example usage: 
Print help message and exit
 detrend.py -h

Examples: The simplest usage takes as input only a table of principal coordinates
 detrend.py -i $PWD/pcoa.txt -o detrending

One may also include a metadata file with a known real-valued gradient as one of the columns. In this case, the output folder will include a text file providing a summary of how well the analysis fit with the hypothesis that the primary variation is due to the gradient (in this case, "DEPTH")
 detrend.py -i $PWD/pcoa.txt -m map.txt -c DEPTH -o detrending

Note that if you provide a real-valued known gradient the script will prerotate the first two axes of the PCoA coords in order to achieve optimal alignment with that gradient. This can be disabled with "-r"
 detrend.py -i $PWD/pcoa.txt -m map.txt -c DEPTH -o detrending -r

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Path to output directory [default: .]
  -m MAP_FP, --map_fp=MAP_FP
                        Path to metadata file [default: None]
  -c GRADIENT_VARIABLE, --gradient_variable=GRADIENT_VARIABLE
                        Column header for gradient variable in metadata table
                        [default: None]
  -r, --suppress_prerotate
                        Suppress pre-rotation of the coordinates for optimal
                        detrending; not pre-rotating assumes that the
                        curvature is symmetrical across the vertical axis
                        [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        Path to read PCoA/PCA/ordination table [REQUIRED]
Usage: differential_abundance.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

OTU differential abundance testing is commonly used to identify OTUs that
differ between two mapping file sample categories (i.e. Palm and Tongue body
sites).  We would recommend having at least 5 samples in each category.  These 
methods can be used in comparison to group_significance.py on a rarefied matrix, 
and we would always recommend comparing the results of these approaches to the
rarefied/group_significance.py approaches.  We would also recommend treating the
differentially abundant OTUs identified by these (metagenomeSeq zero-inflated Gaussian,
or ZIG, and DESeq2 negative binomial Wald test) techniques with caution, as they assume a
distribution and are therefore parametric tests.  Parametric tests can do poorly if
the assumptions about the data are not met.  These tests are also newer techniques
that are less well tested compared to rarefying with a group_significance.py test.  

The input is a raw (not normalized, not rarefied) matrix having uneven column sums.
With these techniques, we would still recommend removing low depth samples (e.g. below
1000 sequences per sample), and low abundance/rare OTUs from the data set.  The DESeq2 method
should NOT be used if the fit line on the dispersion plot (one of the diagnostic plots
output by the -d, or --DESeq2_diagnostic_plots option) does not look smooth, there are big
gaps in the point spacings, and the fitted line does not look appropriate for the data.

DESeq2 is stronger at very small/smaller data sets, but the run-time beyond 100 
total samples becomes very long.  MetagenomeSeq's fitZIG is a better algorithm for larger
library sizes and over 50 samples per category (e.g. 50 Palm samples), the more the better. 
In simulation, these techniques have higher sensitivity, but sometimes higher false positive
rate compared to the non-parametric tests (e.g. Wilcoxon rank sum) in group_significance.py, 
especially with very uneven library sizes (starting at 2-3 fold difference).  In practice 
and with real data, we do not observe much of a difference between these results and 
the tests in group_significance.py.  

For more on these techniques please see McMurdie, P. and Holmes, S. 'Waste not want not 
why rarefying microbiome data is inadmissible.' PLoS Comp. Bio. 2014.  For more on 
metagenomeSeq and fitZIG, please read Paulson, JN, et al. 'Differential abundance analysis
for microbial marker-gene surveys.'  Nature Methods 2013.  For DESeq2/DESeq please read
Love, MI et al. 'Moderated estimation of fold change and dispersion for RNA-Seq data 
with DESeq2,' Genome Biology 2014.  Anders S, Huber W. 'Differential expression analysis 
for sequence count data.' Genome Biology 2010.  Additionally, you can also read the
vignettes for each of the techniques on the Bioconductor/R websites.  Also, if you use
these methods, please CITE the proper sources above (metagenomeSeq and DESeq2) as well as QIIME.

Example usage: 
Print help message and exit
 differential_abundance.py -h

Single File OTU Differential Abundance Testing with metagenomeSeq_fitZIG: Apply metagenomeSeq_fitZIG differential OTU abundance testing to a raw (NOT normalized) BIOM table to test for differences in OTU abundance between samples in the Treatment:Control and Treatment:Fast groups.
 differential_abundance.py -i otu_table.biom -o diff_otus.txt -m map.txt -a metagenomeSeq_fitZIG -c Treatment -x Control -y Fast

Single File OTU Differential Abundance Testing with DESeq2_nbinom: Apply DESeq2_nbinom differential OTU abundance testing to a raw (NOT normalized) BIOM table to test for differences in OTU abundance between samples in the Treatment:Control and Treatment:Fast groups, including output of plots.
 differential_abundance.py -i otu_table.biom -o diff_otus.txt -m map.txt -a DESeq2_nbinom -c Treatment -x Control -y Fast -d

Multiple File OTU Differential Abundance Testing with metagenomeSeq_fitZIG: Apply metagenomeSeq_fitZIG differential OTU abundance testing to a folder of raw (NOT normalized) BIOM tables to test for differences in OTU abundance between samples in the Treatment:Control and Treatment:Fast groups.
 differential_abundance.py -i otu_tables/ -o diff_otus/ -m map.txt -a metagenomeSeq_fitZIG -c Treatment -x Control -y Fast

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -i INPUT_PATH, --input_path=INPUT_PATH
                        path to the input BIOM file (e.g., the output from OTU
                        picking) or directory containing input BIOM files for
                        batch processing [REQUIRED if not passing -l]
  -o OUT_PATH, --out_path=OUT_PATH
                        output filename for single file operation, or output
                        directory for batch processing [REQUIRED if not
                        passing -l]
  -a ALGORITHM, --algorithm=ALGORITHM
                        differential abundance algorithm to apply to input
                        BIOM table(s) [default: metagenomeSeq_fitZIG]
                        Available options are: metagenomeSeq_fitZIG,
                        DESeq2_nbinom
  -m MAPPING_FILE_PATH, --mapping_file_path=MAPPING_FILE_PATH
                        path to mapping file [REQUIRED if not passing -l]
  -c MAPPING_FILE_CATEGORY, --mapping_file_category=MAPPING_FILE_CATEGORY
                        mapping file category [REQUIRED if not passing -l]
  -x MAPPING_FILE_SUBCATEGORY_1, --mapping_file_subcategory_1=MAPPING_FILE_SUBCATEGORY_1
                        mapping file subcategory [REQUIRED if not passing -l]
  -y MAPPING_FILE_SUBCATEGORY_2, --mapping_file_subcategory_2=MAPPING_FILE_SUBCATEGORY_2
                        mapping file subcategory [REQUIRED if not passing -l]
  -l, --list_algorithms
                        show available differential abundance algorithms and
                        exit [default: False]
  -d, --DESeq2_diagnostic_plots
                        show a MA plot - y axis: log2 fold change, x axis:
                        average size factor normalized OTU value. Also show a
                        Dispersion Estimate plot - visualize the fitted
                        dispersion vs. mean relationship [default: False]
Usage: dissimilarity_mtx_stats.py [options] {-i/--input_dir INPUT_DIR -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script reads in all (dis)similarity matrices from an input directory
(input_dir), then calculates and writes the mean, median, standdard deviation
(stdev) to an output folder.

The input_dir must contain only (dis)similarity matrices, and only those you
wish to perform statistical analyses on.


Example usage: 
Print help message and exit
 dissimilarity_mtx_stats.py -h

Example: This examples takes the "dists/" directory as input and returns the results in the "dist_stats/" directory.
 dissimilarity_mtx_stats.py -i dists/ -o dist_stats/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        Path to input directory [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Path to store result files [REQUIRED]
Usage: distance_matrix_from_mapping.py [options] {-i/--input_path INPUT_PATH -c/--column COLUMN}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The input for this script is a mapping file and the name of a column, it has to be numeric, from which a distance matrix will be created. The output of this script is a distance matrix containing a dissimilarity value for each pairwise comparison.

As this is a univariate procedure only one metric is supported: d = c-b.

Example usage: 
Print help message and exit
 distance_matrix_from_mapping.py -h

Pairwise dissimilarity: To calculate the distance matrix (using euclidean distance) on a column of the mapping file, where the results are output to DOB.txt, use the following command
 distance_matrix_from_mapping.py -i Fasting_Map.txt -c DOB

Pairwise dissimilarity using the Vincenty formula for distance between two Latitude/Longitude points: To calculate the distance matrix (using Vincenty formula) on a column of the mapping file, where the results are output to lat_long.txt, use the following command
 distance_matrix_from_mapping.py -i lat_long.txt -c Latitute,Longitude -o lat_long_dtx_matrix.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        Output path to store the distance matrix.
                        [default=map_distance_matrix.txt]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        Mapping filepath. [REQUIRED]
    -c COLUMN, --column=COLUMN
                        string containing the name of the column in the
                        mapping file, e.g. 'DOB'. If you pass two colums
                        separated by a comma (e.g. 'Latitude,Longitud') the
                        script will calculate the Vincenty formula (WGS-84)
                        for distance between two Latitude/Longitude points.
                        [REQUIRED]
Usage: estimate_observation_richness.py [options] {-i/--otu_table_fp OTU_TABLE_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script provides estimates of the observation (e.g., OTU) richness (i.e.
number of observations) given a sampling depth (i.e. number of
individuals/sequences per sample). Estimators are provided for both
interpolation/rarefaction and extrapolation.

Interpolation/rarefaction applies when the richness is estimated for a
*smaller* number of individuals than the original number of individuals in that
sample. We refer to this original sampling depth as the "reference sampling
depth" or "reference sample size".

Extrapolation applies when the richness is estimated for a *larger* number of
individuals than the reference sample size.

This script currently only provides a single unified estimation model for
interpolation and extrapolation. This model is the individual-based multinomial
model, which uses Chao1 to estimate the full richness of the sample. Please
refer to Colwell et al. (2012) for more details; equations 4, 5, 9, 10, 15a,
and 15b are used in this script.

For each interpolation/extrapolation point, the estimate, its unconditional
standard error, and confidence interval are reported. The script currently only
outputs this information to a table, which can be easily viewed in a program
such as Excel. Other output formats, such as plots, may be added in the future.

If an estimate is reported as "N/A", not enough information was present to
compute an estimate. This can occur when extrapolating if a sample does not
contain any singletons or doubletons, or if there is exactly one singleton and
no doubletons. A singleton is defined as an observation with exactly one
individual/sequence in the sample. A doubleton is defined as an observation
with exactly two individuals/sequences in the sample.

IMPORTANT: If you use the results of this script in any published works, please
be sure to cite the Colwell et al. (2012) paper, as well as QIIME (see
http://qiime.org for details).

In addition to Colwell et al. (2012), the following resources were extremely
useful while implementing and testing these estimators, so it is appropriate to
also acknowledge them here:

- Hsieh et al. (2013)
- Shen et al. (2003)
- Colwell (2013)

References:

Chao, A., N. J. Gotelli, T. C. Hsieh, E. L. Sander, K. H. Ma, R. K. Colwell, and A. M. Ellison 2013. Rarefaction and extrapolation with Hill numbers: a unified framework for sampling and estimation in biodiversity studies, Ecological Monographs (under revision).

Colwell, R. K. 2013. EstimateS: Statistical estimation of species richness and shared species from samples. Version 9. User's Guide and application published at: http://purl.oclc.org/estimates.

Colwell, R. K., A. Chao, N. J. Gotelli, S. Y. Lin, C. X. Mao, R. L. Chazdon, and J. T. Longino. 2012. Models and estimators linking individual-based and sample-based rarefaction, extrapolation and comparison of assemblages. Journal of Plant Ecology 5:3-21.

Hsieh, T. C., K. H. Ma, and A. Chao. 2013. iNEXT online: interpolation and extrapolation (Version 1.0) [Software]. Available from http://chao.stat.nthu.edu.tw/inext/.

Shen T-J, Chao A, Lin C- F. Predicting the number of new species in further taxonomic sampling. Ecology 2003;84:798-804.


Example usage: 
Print help message and exit
 estimate_observation_richness.py -h

Interpolation and extrapolation of richness: Estimate the richness of each sample in the input BIOM table using the default sampling depth range, which includes interpolation and extrapolation.
 estimate_observation_richness.py -i otu_table.biom -o estimates_out

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m MIN, --min=MIN     the number of individuals (e.g. sequences) per sample
                        to start performing estimations at [default: 1]
  -x MAX, --max=MAX     the number of individuals (e.g. sequences) per sample
                        to stop performing estimations at. By default, the
                        base sample size will be used, which is defined in
                        Chao et al. (2013) as "double the smallest reference
                        sample size or the maximum reference sample size,
                        whichever is larger [default: base sample size]
  -n NUM_STEPS, --num_steps=NUM_STEPS
                        the number of steps to make between -m/--min and
                        -x/--max. Increasing this number will result in
                        smoother curves, but will also increase the amount of
                        time needed to run the script. Note that the reference
                        sample size for each sample will be included if it
                        does not fall within the min/max/num_steps range
                        [default: 10]
  -c CONFIDENCE_LEVEL, --confidence_level=CONFIDENCE_LEVEL
                        the confidence level of the unconditional confidence
                        interval for each estimate. Must be a value between 0
                        and 1 (exclusive). For example, a 95% unconditional
                        confidence interval would be 0.95 [default: 0.95]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        path to the input BIOM table (e.g., the output from
                        make_otu_table.py). IMPORTANT: This table should
                        contain observation *counts* (integers), NOT relative
                        abundances (fractions) [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
Usage: exclude_seqs_by_blast.py [options] {-i/--querydb QUERYDB -d/--subjectdb SUBJECTDB -o/--outputdir OUTPUTDIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



This code is designed to allow users of the QIIME workflow to conveniently exclude unwanted sequences from their data. This is mostly useful for excluding human sequences from runs to comply with Internal Review Board (IRB) requirements, but may also have other uses (e.g. perhaps excluding a major bacterial contaminant). Sequences from a run are searched against a user-specified subject database, where BLAST hits are screened by e-value and the percentage of the query that aligns to the sequence.

For human screening THINK CAREFULLY about the data set that you screen against. Are you excluding human non-coding sequences? What about mitochondrial sequences? This point is CRITICAL because submitting human sequences that are not IRB-approved is BAD.

(e.g. you would NOT want to just screen against just the coding sequences of the human genome as found in the KEGG .nuc files, for example)

One valid approach is to screen all putative 16S rRNA sequences against greengenes to ensure they are bacterial rather than human.

WARNING: You cannot use this script if there are spaces in the path to the database of fasta files because formatdb cannot handle these paths (this is a limitation of NCBI's tools and we have no control over it).


Example usage: 
Print help message and exit
 exclude_seqs_by_blast.py -h

Examples: The following is a simple example, where the user can take a given FASTA file (i.e. resulting FASTA file from pick_rep_set.py) and blast those sequences against a reference FASTA file containing the set of sequences which are considered contaminated
 exclude_seqs_by_blast.py -i repr_set_seqs.fasta -d ref_seq_set.fna -o exclude_seqs/

Alternatively, if the user would like to change the percent of aligned sequence coverage ("-p") or the maximum E-value ("-e"), they can use the following command
 exclude_seqs_by_blast.py -i repr_set_seqs.fasta -d ref_seq_set.fna -o exclude_seqs/ -p 0.95 -e 1e-10

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -e E_VALUE, --e_value=E_VALUE
                        The e-value cutoff for blast queries [default: 1e-10].
  -p PERCENT_ALIGNED, --percent_aligned=PERCENT_ALIGNED
                        The % alignment cutoff for blast queries, expressed as
                        a fraction between 0 and 1 [default: 0.97].
  --no_clean            If set, don't delete files generated by formatdb after
                        running [default: False].
  --blastmatroot=BLASTMATROOT
                        Path to a folder containing blast matrices [default:
                        none].
  --working_dir=WORKING_DIR
                        Working dir for BLAST [default: /tmp/].
  -m MAX_HITS, --max_hits=MAX_HITS
                        Max hits parameter for BLAST. CAUTION: Because
                        filtering on alignment percentage occurs after BLAST,
                        a max hits value of 1 in combination with an alignment
                        percent filter could miss valid contaminants.
                        [default: 100]
  -w WORDSIZE, --word_size=WORDSIZE
                        Word size to use for BLAST search [default: 28]
  -n, --no_format_db    If this flag is specified, format_db will not be
                        called on the subject database (formatdb will be set
                        to False).  This is  useful if you have already
                        formatted the database and a) it took a very long time
                        or b) you want to run the script in parallel on the
                        pre-formatted database [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i QUERYDB, --querydb=QUERYDB
                        The path to a FASTA file containing query sequences
                        [REQUIRED]
    -d SUBJECTDB, --subjectdb=SUBJECTDB
                        The path to a FASTA file to BLAST against [REQUIRED]
    -o OUTPUTDIR, --outputdir=OUTPUTDIR
                        The output directory [REQUIRED]
Usage: extract_barcodes.py [options] {-f/--fastq1 FASTQ1}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

A variety of data formats are possible, depending upon how one utilized sequencing primers, designed primer constructs (e.g., partial barcodes on each end of the read), or processed the data (e.g., barcodes were put into the sequence labels rather than the reads). See various input examples below.

Example usage: 
Print help message and exit
 extract_barcodes.py -h

Parse barcodes of 12 base pairs from the beginning of a single read. Will create an output fastq file of the barcodes and an output file of the reads supplied with the barcodes removed.: 
 extract_barcodes.py -f inseqs.fastq -c barcode_single_end --bc1_len 12 -o processed_seqs

Parse barcodes of 12 base pairs from the beginning of a single read, reverse complement the barcodes before writing. Will create an output fastq file of the barcodes and an output file of the reads supplied with the barcodes removed: 
 extract_barcodes.py -f inseqs.fastq -c barcode_single_end --bc1_len 12 -o processed_seqs --rev_comp_bc1

Parse barcodes of 6 base pairs from the beginning of paired reads. Will create an output fastq file of the barcodes and an output file of each of the reads supplied with the barcodes removed. The order of the barcodes written is determined by the order of the files passed (-f is written first, followed by -r): 
 extract_barcodes.py -f inseqs_R1.fastq -r inseqs_R2.fastq -c barcode_paired_end --bc1_len 6 --bc2_len 6 -o processed_seqs

Parse barcodes of 6 base pairs from the beginning of paired reads, attempt to orient reads based upon detection of forward and reverse primers in the mapping file. Will create an output fastq file of the barcodes and an output file of each of the reads supplied with the barcodes removed. The order of the barcodes written is determined by the order of the files passed (-f is written first, followed by -r): 
 extract_barcodes.py -f inseqs_R1.fastq -r inseqs_R2.fastq -c barcode_paired_end --map_fp mapping_data.txt --attempt_read_reorientation --bc1_len 6 --bc2_len 6 -o processed_seqs

Parse barcodes of 6 base pairs from the beginning, 8 base pairs at the end of a stitched read. Will create an output fastq file of the barcodes and an output fastq file of the stitched read supplied with the barcodes removed. The barcode at the beginning of the stitched read is written first, followed by the barcode at the end, unless reversed by the --switch_bc_order option is used: 
 extract_barcodes.py -f inseqs_R1.fastq -c barcode_paired_stitched --bc1_len 6 --bc2_len 8 -o processed_seqs

Parse barcodes of 12 base pairs from labels of the input fastq file. Example label (note that the desired character preceding the barcode is '#'): @MCIC-SOLEXA_0051_FC:1:1:14637:1026#CGATGTGATTTC/1 This will create an output fastq file of the barcodes (no other sequence are written). A second file with barcodes in the label can be passed with -r, and if this is done, the combined barcodes from -f and -r will be written together: 
 extract_barcodes.py -f inseqs_R1.fastq -c barcode_in_label --char_delineator '#' --bc1_len 12 -o processed_seqs

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -r FASTQ2, --fastq2=FASTQ2
                        input fastq filepath. This file is considered read 2.
                        [default: none]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory prefix for output files [default: .]
  -c INPUT_TYPE, --input_type=INPUT_TYPE
                        Specify the input type. barcode_single_end: Input is a
                        single fastq file, that starts with the barcode
                        sequence. barcode_paired_end: Input is a pair of fastq
                        files (--fastq1 and --fastq2) that each begin with a
                        barcode sequence. The barcode for fastq1 will be
                        written first, followed by the barcode from fastq2.
                        barcode_paired_stitched: Input is a single fastq file
                        that has barcodes at the beginning and end. The
                        barcode from the beginning of the read will be written
                        first followed by the barcode from the end of the
                        read, unless the order is switched with
                        --switch_bc_order. barcode_in_label: Input is a one
                        (--fastq1) or two (--fastq2) fastq files with the
                        barcode written in the labels. [default:
                        barcode_single_end]
  -l BC1_LEN, --bc1_len=BC1_LEN
                        Specify the length, in base pairs, of barcode 1. This
                        applies to the --fastq1 file and all options specified
                        by --input_type [default: 6]
  -L BC2_LEN, --bc2_len=BC2_LEN
                        Specify the length, in base pairs, of barcode 2. This
                        applies to the --fastq2 file and options
                        "barcode_paired_end", "barcode_paired_stitched", and
                        "barcode_in_label" for the --input_type [default: 6]
  --rev_comp_bc1        Reverse complement barcode 1 before writing [default:
                        False]
  --rev_comp_bc2        Reverse complement barcode 2 before writing [default:
                        False]
  -s CHAR_DELINEATOR, --char_delineator=CHAR_DELINEATOR
                        Character in fastq label that should immediately
                        precede the barcode sequence. The length of the
                        barcode is specified by the --bc1_len (and optionally
                        --bc2_len if paired end files are used) parameter.
                        [default: :]
  --switch_bc_order     Reverse barcode order written when using the -c
                        barcode_paired_stitched option. [default: False]
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        Filepath of mapping file. NOTE: Must contain a header
                        line indicating SampleID in the first column and
                        BarcodeSequence in the second, LinkerPrimerSequence in
                        the third and a ReversePrimer column before the final
                        Description column. Needed for
                        --attempt_read_orientation option. [default: none]
  -a, --attempt_read_reorientation
                        Will attempt to search for the forward and reverse
                        primer in the read and adjust the sequence orientation
                        to match the orientation of the forward primer. An
                        exact match for the  forward and reverse complemented
                        versions of the primers are tested for, and sequences
                        are reverse complemented, if necessary, before
                        writing. Sequences without an exact match are written
                        to a separate output fastq file, labeled as
                        _no_primer_match.fastq. [default: False]
  -d, --disable_header_match
                        Enable this option to suppress header matching between
                        input fastq files.[default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f FASTQ1, --fastq1=FASTQ1
                        input fastq filepath. This file is considered read 1.
                        [REQUIRED]
Usage: extract_reads_from_interleaved_file.py [options] {-i/--input_fp INPUT_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script takes an interleaved file, like the ones produced by JGI, and outputs a forward and reverse fastq file with the corresponding reads in each file. 

Example usage: 
Print help message and exit
 extract_reads_from_interleaved_file.py -h

Extract reads from an interleaved file: 
 extract_reads_from_interleaved_file.py -i $PWD/reads_to_extract.fastq -o $PWD/extracted_reads

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --forward_read_identifier=FORWARD_READ_IDENTIFIER
                        This is the string identifying the forward reads.
                        [default: 1:N:0].
  --reverse_read_identifier=REVERSE_READ_IDENTIFIER
                        This is the string identifying the reverse reads.
                        [default: 2:N:0].

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        Path to input forward reads in FASTQ format.
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Directory to store result files [REQUIRED]
Usage: extract_seqs_by_sample_id.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_fasta_fp OUTPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script creates a fasta file which will contain only sequences that ARE associated with a set of sample IDs, OR all sequences that are NOT associated with a set of sample IDs (-n)

Example usage: 
Print help message and exit
 extract_seqs_by_sample_id.py -h

Examples: Create the file outseqs.fasta (-o), which will be a subset of inseqs.fasta (-i) containing only the sequences THAT ARE associated with sample ids S2, S3, S4 (-s). As always, sample IDs are case-sensitive
 extract_seqs_by_sample_id.py -i inseqs.fasta -o outseqs_by_sample.fasta -s S2,S3,S4

Create the file outseqs.fasta (-o), which will be a subset of inseqs.fasta (-i) containing only the sequences THAT ARE NOT (-n) associated with sample ids S2, S3, S4 (-s). As always, sample IDs are case-sensitive
 extract_seqs_by_sample_id.py -i inseqs.fasta -o outseqs_by_sample_negated.fasta -s S2,S3,S4 -n

Create the file outseqs.fasta (-o), which will be a subset of inseqs.fasta (-i) containing only the sequences THAT ARE associated with sample ids whose "Treatment" value is "Fast" in the mapping file
 extract_seqs_by_sample_id.py -i inseqs.fasta -o outseqs_by_mapping_field.fasta -m map.txt -s "Treatment:Fast"

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n, --negate          negate the sample ID list (i.e., output sample ids not
                        passed via -s) [default: False]
  -s SAMPLE_IDS, --sample_ids=SAMPLE_IDS
                        comma-separated sample_ids to include in output fasta
                        file (or exclude if --negate), or string describing
                        mapping file states defining sample ids (mapping_fp
                        must be provided for the latter)
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
    -o OUTPUT_FASTA_FP, --output_fasta_fp=OUTPUT_FASTA_FP
                        the output fasta file [REQUIRED]
Usage: filter_alignment.py [options] {-i/--input_fasta_file INPUT_FASTA_FILE}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script should be applied to generate a useful tree when aligning against a template alignment (e.g., with PyNAST). This script will remove positions which are gaps in every sequence (common for PyNAST, as typical sequences cover only 200-400 bases, and they are being aligned against the full 16S gene). Additionally, the user can supply a lanemask file, that defines which positions should included when building the tree, and which should be ignored. Typically, this will differentiate between non-conserved positions, which are uninformative for tree building, and conserved positions which are informative for tree building. FILTERING ALIGNMENTS WHICH WERE BUILT WITH PYNAST AGAINST THE GREENGENES CORE SET ALIGNMENT SHOULD BE CONSIDERED AN ESSENTIAL STEP.

Example usage: 
Print help message and exit
 filter_alignment.py -h

Example 1: As a simple example of this script, the user can use the following command, which consists of an input FASTA file (i.e. resulting file from align_seqs.py) and the output directory "filtered_alignment/"
 filter_alignment.py -i seqs_rep_set_aligned.fasta -o filtered_alignment/

Example 2: Apply the same filtering as above, but additionally remove sequences whose distance from the majority consensus sequence is more than 3 (can be changed by passing --threshold) standard deviations above the mean
 filter_alignment.py -i seqs_rep_set_aligned.fasta -o filtered_alignment/ --remove_outliers

Example 3: Alternatively, if the user would like to use a different gap fraction threshold ("-g"), they can use the following command
 filter_alignment.py -i seqs_rep_set_aligned.fasta -o filtered_alignment/ -g 0.95

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [default: .]
  -m LANE_MASK_FP, --lane_mask_fp=LANE_MASK_FP
                        path to lane mask file [default: 16S alignment lane
                        mask (Lane, D.J. 1991)]
  -s, --suppress_lane_mask_filter
                        suppress lane mask filtering [default: False]
  -g ALLOWED_GAP_FRAC, --allowed_gap_frac=ALLOWED_GAP_FRAC
                        gap filter threshold, filters positions which are gaps
                        in > allowed_gap_frac of the sequences [default:
                        0.999999]
  -r, --remove_outliers
                        remove seqs very dissimilar to the alignment consensus
                        (see --threshold).  [default: False]
  -t THRESHOLD, --threshold=THRESHOLD
                        with -r, remove seqs whose dissimilarity to the
                        consensus sequence is approximately > x standard
                        deviations above the mean of the sequences [default:
                        3.0]
  -e ENTROPY_THRESHOLD, --entropy_threshold=ENTROPY_THRESHOLD
                        Percent threshold for removing base positions with the
                        highest entropy, expressed as a fraction between 0 and
                        1.  For example, if 0.10 were specified, the top 10%
                        most entropic base positions would be filtered.  If
                        this value is used, any lane mask supplied will be
                        ignored.  Entropy filtering occurs after gap
                        filtering. [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FILE, --input_fasta_file=INPUT_FASTA_FILE
                        the input fasta file containing the alignment
                        [REQUIRED]
Usage: filter_distance_matrix.py [options] {-i/--input_distance_matrix INPUT_DISTANCE_MATRIX -o/--output_distance_matrix OUTPUT_DISTANCE_MATRIX}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Remove samples from a distance matrix based on a mapping file or an otu table or a list of sample ids.

Example usage: 
Print help message and exit
 filter_distance_matrix.py -h

Filter samples ids listed in sample_id_list.txt from dm.txt
 filter_distance_matrix.py -i dm.txt -o dm_out_sample_list.txt --sample_id_fp sample_id_list.txt

Filter samples ids in otu_table.biom from dm.txt
 filter_distance_matrix.py -i dm.txt -o dm_out_otu_table.txt -t otu_table.biom

Filter samples ids where DOB is 20061218 in Fasting_Map.txt. (Run "filter_samples_from_otu_table.py -h" for additional information on how metadata filtering can be specified.)
 filter_distance_matrix.py -i dm.txt -o dm_out_mapping_file.txt -m Fasting_Map.txt -s "DOB:20061218"

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --sample_id_fp=SAMPLE_ID_FP
                        A list of sample identifiers (or tab-delimited lines
                        with a sample identifier in the first field) which
                        should be retained
  -t OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        the otu table filepath
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to the mapping file
  -s VALID_STATES, --valid_states=VALID_STATES
                        string containing valid states, e.g. 'STUDY_NAME:DOB'
  --negate              discard specified samples (instead of keeping them)
                        [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DISTANCE_MATRIX, --input_distance_matrix=INPUT_DISTANCE_MATRIX
                        the input distance matrix [REQUIRED]
    -o OUTPUT_DISTANCE_MATRIX, --output_distance_matrix=OUTPUT_DISTANCE_MATRIX
                        path to store the output distance matrix [REQUIRED]
Usage: filter_fasta.py [options] {-f/--input_fasta_fp INPUT_FASTA_FP -o/--output_fasta_fp OUTPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 filter_fasta.py -h

OTU map-based filtering: Keep all sequences that show up in an OTU map.
 filter_fasta.py -f inseqs.fasta -o otu_map_filtered_seqs.fasta -m otu_map.txt

Chimeric sequence filtering: Discard all sequences that show up in chimera checking output. NOTE: It is very important to pass -n here as this tells the script to negate the request, or discard all sequences that are listed via -s. This is necessary to remove the identified chimeras from inseqs.fasta.
 filter_fasta.py -f inseqs.fasta -o non_chimeric_seqs.fasta -s chimeric_seqs.txt -n

Sequence list filtering: Keep all sequences from as fasta file that are listed in a text file.
 filter_fasta.py -f inseqs.fasta -o list_filtered_seqs.fasta -s seqs_to_keep.txt

biom-based filtering: Keep all sequences that are listed as observations in a biom file.
 filter_fasta.py -f inseqs.fastq -o biom_filtered_seqs.fastq -b otu_table.biom

fastq filtering: Keep all sequences from a fastq file that are listed in a text file (note: file name must end with .fastq to support fastq filtering).
 filter_fasta.py -f inseqs.fastq -o list_filtered_seqs.fastq -s seqs_to_keep.txt

sample id list filtering: Keep all sequences from a fasta file where the sample id portion of the sequence identifier is listed in a text file (sequence identifiers in fasta file must be in post-split libraries format: sampleID_seqID).
 filter_fasta.py -f sl_inseqs.fasta -o sample_id_list_filtered_seqs.fasta --sample_id_fp map.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m OTU_MAP, --otu_map=OTU_MAP
                        An OTU map where sequences ids are those which should
                        be retained.
  -s SEQ_ID_FP, --seq_id_fp=SEQ_ID_FP
                        A list of sequence identifiers (or tab-delimited lines
                        with a seq identifier in the first field) which should
                        be retained.
  -b BIOM_FP, --biom_fp=BIOM_FP
                        A biom file where otu identifiers should be retained.
  -a SUBJECT_FASTA_FP, --subject_fasta_fp=SUBJECT_FASTA_FP
                        A fasta file where the seq ids should be retained.
  -p SEQ_ID_PREFIX, --seq_id_prefix=SEQ_ID_PREFIX
                        Keep seqs where seq_id starts with this prefix.
  --sample_id_fp=SAMPLE_ID_FP
                        Keep seqs where seq_id starts with a sample id listed
                        in this file. Must be newline delimited and may not
                        contain a header.
  -n, --negate          Discard passed seq ids rather than keep passed seq
                        ids. [default: False]
  --mapping_fp=MAPPING_FP
                        Mapping file path (for use with --valid_states).
                        [default: none]
  --valid_states=VALID_STATES
                        Description of sample ids to retain (for use with
                        --mapping_fp). [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
    -o OUTPUT_FASTA_FP, --output_fasta_fp=OUTPUT_FASTA_FP
                        the output fasta filepath [REQUIRED]
Usage: filter_otus_by_sample.py [options] {-i/--otu_map_fp OTU_MAP_FP -f/--input_fasta_fp INPUT_FASTA_FP -s/--samples_to_extract SAMPLES_TO_EXTRACT}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This filter allows for the removal of sequences and OTUs containing user-specified Sample IDs, for instance, the removal of negative control samples. This script identifies OTUs containing the specified Sample IDs and removes its corresponding sequence from the sequence collection.

Example usage: 
Print help message and exit
 filter_otus_by_sample.py -h

Example: The following command can be used, where all options are passed (using the resulting OTU file from pick_otus.py, FASTA file from split_libraries.py and removal of sample 'PC.636') with the resulting data being written to the output directory "filtered_otus/"
 filter_otus_by_sample.py -i seqs_otus.txt -f seqs.fna -s PC.636 -o filtered_otus/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_MAP_FP, --otu_map_fp=OTU_MAP_FP
                        path to the input OTU map (i.e., the output from
                        pick_otus.py) [REQUIRED]
    -f INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
    -s SAMPLES_TO_EXTRACT, --samples_to_extract=SAMPLES_TO_EXTRACT
                        This is a list of sample ids, which should be removed
                        from the OTU file [REQUIRED]
Usage: filter_otus_from_otu_table.py [options] {-i/--input_fp INPUT_FP -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 filter_otus_from_otu_table.py -h

Singleton filtering: Discard all OTUs that are observed fewer than 2 times (i.e., singletons)
 filter_otus_from_otu_table.py -i otu_table.biom -o otu_table_no_singletons.biom -n 2

Abundance filtering: Discard all OTUs that are observed greater than 100 times (e.g., if you want to look at low abundance OTUs only)
 filter_otus_from_otu_table.py -i otu_table.biom -o otu_table_low_abundance.biom -x 100

Chimera filtering: Discard all OTUs listed in chimeric_otus.txt (e.g., to remove chimeric OTUs from an OTU table)
 filter_otus_from_otu_table.py -i otu_table.biom -o otu_table_non_chimeric.biom -e chimeric_otus.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --negate_ids_to_exclude
                        keep OTUs in otu_ids_to_exclude_fp rather than discard
                        them [default:False]
  -n MIN_COUNT, --min_count=MIN_COUNT
                        the minimum total observation count of an otu for that
                        otu to be retained [default: 0]
  --min_count_fraction=MIN_COUNT_FRACTION
                        fraction of the total observation (sequence) count to
                        apply as the minimum total observation count of an otu
                        for that otu to be retained. this is a fraction, not
                        percent, so if you want to filter to 1%, you specify
                        0.01. [default: 0]
  -x MAX_COUNT, --max_count=MAX_COUNT
                        the maximum total observation count of an otu for that
                        otu to be retained [default: infinity]
  -s MIN_SAMPLES, --min_samples=MIN_SAMPLES
                        the minimum number of samples an OTU must be observed
                        in for that otu to be retained [default: 0]
  -y MAX_SAMPLES, --max_samples=MAX_SAMPLES
                        the maximum number of samples an OTU must be observed
                        in for that otu to be retained [default: infinity]
  -e OTU_IDS_TO_EXCLUDE_FP, --otu_ids_to_exclude_fp=OTU_IDS_TO_EXCLUDE_FP
                        file containing list of OTU ids to exclude: can be a
                        text file with one id per line, a text file where id
                        is the first value in a tab-separated line, or can be
                        a fasta file (extension must be .fna or .fasta)
                        [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        the input otu table filepath in biom format [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath in biom format [REQUIRED]
Usage: filter_samples_from_otu_table.py [options] {-i/--input_fp INPUT_FP -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 filter_samples_from_otu_table.py -h

Abundance filtering (low coverage): Filter samples with fewer than 150 observations from the otu table.
 filter_samples_from_otu_table.py -i otu_table.biom -o otu_table_no_low_coverage_samples.biom -n 150

Abundance filtering (high coverage): Filter samples with greater than 149 observations from the otu table.
 filter_samples_from_otu_table.py -i otu_table.biom -o otu_table_no_high_coverage_samples.biom -x 149

Metadata-based filtering (positive): Filter samples from the table, keeping samples where the value for 'Treatment' in the mapping file is 'Control'
 filter_samples_from_otu_table.py -i otu_table.biom -o otu_table_control_only.biom -m map.txt -s 'Treatment:Control'

Metadata-based filtering (negative): Filter samples from the table, keeping samples where the value for 'Treatment' in the mapping file is not 'Control'
 filter_samples_from_otu_table.py -i otu_table.biom -o otu_table_not_control.biom -m map.txt -s 'Treatment:*,!Control'

ID-based filtering: Keep samples where the id is listed in ids.txt
 filter_samples_from_otu_table.py -i otu_table.biom -o filtered_otu_table.biom --sample_id_fp ids.txt

ID-based filtering (negation): Discard samples where the id is listed in ids.txt
 filter_samples_from_otu_table.py -i otu_table.biom -o filtered_otu_table.biom --sample_id_fp ids.txt --negate_sample_id_fp

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to the map file [default: none]
  --output_mapping_fp=OUTPUT_MAPPING_FP
                        path to write filtered mapping file [default: filtered
                        mapping file is not written]
  --sample_id_fp=SAMPLE_ID_FP
                        Path to file listing sample ids to keep. Valid formats
                        for the file are: 1) any white space, newline, or tab
                        delimited list of samples, 2) a mapping file with
                        samples in the first column [default: none]
  -s VALID_STATES, --valid_states=VALID_STATES
                        string describing valid states (e.g.
                        'Treatment:Fasting') [default: none]
  -n MIN_COUNT, --min_count=MIN_COUNT
                        the minimum total observation count in a sample for
                        that sample to be retained [default: 0]
  -x MAX_COUNT, --max_count=MAX_COUNT
                        the maximum total observation count in a sample for
                        that sample to be retained [default: infinity]
  --negate_sample_id_fp
                        discard samples specified in --sample_id_fp instead of
                        keeping them [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        the input otu table filepath in biom format [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath in biom format [REQUIRED]
Usage: filter_taxa_from_otu_table.py [options] {-i/--input_otu_table_fp INPUT_OTU_TABLE_FP -o/--output_otu_table_fp OUTPUT_OTU_TABLE_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This scripts filters an OTU table based on taxonomic metadata. It can be applied for positive filtering (i.e., keeping only certain taxa), negative filtering (i.e., discarding only certain taxa), or both at the same time.

Example usage: 
Print help message and exit
 filter_taxa_from_otu_table.py -h

Filter otu_table.biom to include only OTUs identified as __Bacteroidetes or p__Firmicutes.
 filter_taxa_from_otu_table.py -i otu_table.biom -o otu_table_bac_firm_only.biom -p p__Bacteroidetes,p__Firmicutes

Filter otu_table.biom to exclude OTUs identified as p__Bacteroidetes or p__Firmicutes.
 filter_taxa_from_otu_table.py -i otu_table.biom -o otu_table_non_bac_firm.biom -n p__Bacteroidetes,p__Firmicutes

Filter otu_table.biom to include OTUs identified as p__Firmicutes but not c__Clostridia.
 filter_taxa_from_otu_table.py -i otu_table.biom -o otu_table_all_firm_but_not_clos.biom -p p__Firmicutes -n c__Clostridia

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p POSITIVE_TAXA, --positive_taxa=POSITIVE_TAXA
                        comma-separated list of taxa to retain [default: None;
                        retain all taxa]
  -n NEGATIVE_TAXA, --negative_taxa=NEGATIVE_TAXA
                        comma-separated list of taxa to discard [default:
                        None; retain all taxa]
  --metadata_field=METADATA_FIELD
                        observation metadata identifier to filter based on
                        [default: taxonomy]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_OTU_TABLE_FP, --input_otu_table_fp=INPUT_OTU_TABLE_FP
                        the input otu table filepath [REQUIRED]
    -o OUTPUT_OTU_TABLE_FP, --output_otu_table_fp=OUTPUT_OTU_TABLE_FP
                        the output otu table filepath [REQUIRED]
Usage: filter_tree.py [options] {-i/--input_tree_filepath INPUT_TREE_FP -o/--output_tree_filepath OUTPUT_TREE_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script takes a tree and a list of OTU IDs (in one of several supported formats) and outputs a subtree retaining only the tips on the tree which are found in the inputted list of OTUs (or not found, if the --negate option is provided).

Example usage: 
Print help message and exit
 filter_tree.py -h

Prune a tree to include only the tips in tips_to_keep.txt: 
 filter_tree.py -i rep_seqs.tre -t tips_to_keep.txt -o pruned.tre

Prune a tree to remove the tips in tips_to_remove.txt. Note that the -n/--negate option must be passed for this functionality: 
 filter_tree.py -i rep_seqs.tre -t tips_to_keep.txt -o negated.tre -n

Prune a tree to include only the tips found in the fasta file provided: 
 filter_tree.py -i rep_seqs.tre -f fast_f.fna -o pruned_fast.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n, --negate          if negate is True will remove input tips/seqs, if
                        negate is False, will retain input tips/seqs [default:
                        False]
  -t TIPS_FP, --tips_fp=TIPS_FP
                        A list of tips (one tip per line) or sequence
                        identifiers   (tab-delimited lines with a seq
                        identifier in the first field)   which should be
                        retained   [default: none]
  -f FASTA_FP, --fasta_fp=FASTA_FP
                        A fasta file where the seq ids should be retained
                        [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_TREE_FP, --input_tree_filepath=INPUT_TREE_FP
                        input tree filepath [REQUIRED]
    -o OUTPUT_TREE_FP, --output_tree_filepath=OUTPUT_TREE_FP
                        output tree filepath [REQUIRED]
Usage: fix_arb_fasta.py [options] {-f/--input_fasta_fp INPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script fixes ARB FASTA formatting by repairing incorrect line break chararcters, stripping spaces and replacing "." with "-" characters.

Example usage: 
Print help message and exit
 fix_arb_fasta.py -h

Example: Fix the input ARB FASTA format file arb.fasta and print the result to stdout
 fix_arb_fasta.py -f arb.fasta

Example saving to an output file: Fix the input ARB FASTA format file arb.fasta and print the result to fixed.fasta
 fix_arb_fasta.py -f arb.fasta -o fixed.fasta

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        path where output will be written [default: print to
                        screen]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
Usage: group_significance.py [options] {-i/--otu_table_fp OTU_TABLE_FP -m/--mapping_fp MAPPING_FP -c/--category CATEGORY -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script is used to compare OTU frequencies in sample groups and to ascertain
whether or not there are statistically significant differences between the OTU
abundance in the different sample groups.

The script will compare each OTU based
on the passed sample groupings to see if it is differentially represented. The
sample groupings are determined by the -c option. The script will group together
samples which have the same value in the mapping file under the header passed
with the -c option. Any samples that do not contain a value under the given
header will not be included in the comparison.
At a basic level, the script is constructing a OTUxSample
(rowXcolumn) contingency table, and testing whether or not each OTU is
differentially represented in cerstain groups of columns (determined by the
metadata category passed).

There are several important considerations with this script.

Errors and behind-the-scenes processing:

- This script will ignore samples that are found in the mapping file but do not
  contain information for the passed category. This would cause the mapping
  file to fail validate_mapping_file.py, but will not cause a failure here.
- This script will silenty ignore situations where the set of samples in the
  mapping file is a superset of the samples in the biom file. If the reverse is
  true, the script will error unless --biom_samples_are_superset is passed.
- This script will round P-values greater than 1 (after correcting for multiple
  comparisons) to 1.0.
- If your results file contains nans for p values its because one or more of
  the assumptions the selected test makes about the data was not met by the
  given OTU. The inverse of this statement is not guaranteed; just because the
  test worked on the data doesn't mean all its assumptions are met, just that
  enough assumptions are met so it doesn't fail (see below).

Filtering your OTU table prior to this script is important:

- Filtering out OTUs which are found in a low percentage of samples is a good
  idea before using this script. The old otu_category_significance script
  removed OTUs that were not found in at least 25 percent of samples. This
  prevents 0 variance errors and spurious significance for really low abundance
  OTUs and focuses the hypothesis discovery process on the abundant OTUs which
  are likely playing a larger role.

Test assumptions:

- This script tests that some basic assumptions of the given statistical test
  are met by the passed data. These 'assumption tests' are *necessary* not
  *sufficient* to ensure that the given statistical test you are applying is
  appropriate for the data. For instance, the script will error if you use the
  Mann-Whitney-U test and one of your group sizes is smaller than 20. It is
  likely that assumptions about the distribution of the data, the distribution
  of the variance, etc. are not robustly met. IT IS YOUR REPSONSIBILTY TO CHECK
  THAT YOU ARE USING AN APPROPRIATE TEST. For more information on assumptions
  made by the tests, please view the following resources:

  - Biometry by Sokal and Rolhf
  - Nonparamteric Statistical Methods by Hollander and Wolfe
  - Documentation in R and Scipy packages
  - Handbook of Biological Statistics by McDonald (available at
    http://udel.edu/~mcdonald/statintro.html)

The assumptions we check for:

- Kruskal-Wallis: No assumptions are checked for Kruskal-Wallis.
- G-test: We check that all the values in the table are non-negative and we
  check that each sample grouping contains at least one non 0 value. If either
  condition is not met we return G-stat, pval = (nan, nan) but don't error.
- Mann-Whitney-U: The number of data points in the combined samples is >= 20. If
  there are fewer than 21 data points the script will error. Although R gives
  exact values for less than 50 data points,  its definition of 'exact' is
  unclear since a conditional permutation calculation with ~ 10^14 calculations
  would be required. The normal approximation is suggested for more than 16 data
  points by Mann and Whitney 1947, and more than 20 in the Scipy documentation.
  The bootstrapped version of the test does not require >20 data points. If all
  the data ranks are tied for one of the groups, the function will return U,nan.
- ANOVA: No assumptions are checked for ANOVA. However, if the within group
  variance is 0, we return nan,nan.
- T-test: No assumptions are checked for the T-test. If no variance groups are
  detected None or nan will be returned.

The assumptions we do not check for are:

- Kruskal-Wallis: The scipy documentation indicates that each group must have
  at least 5 samples to make the Chi Squared approximation for the distribution
  of the H statistic be appropriate. R has no such requirement, and we do not
  implement the requirement here. The KW test does assume that the distributions
  from which the samples come are the same (although they may be non-normal)
  except for their location parameter, and we do not check this.
- G-test: we check that the data are counts rather than relative abundance.
- Mann-Whitney-U: Equality of variance between groups. Sample 1 is IID, Sample 2
  is IID. Sample 1 and Sample 2 are mutually independent.
- ANOVA: ANOVA assumes equality of variance between groups (homoscedasticity),
  normality of the residuals, and independence of the individual observations in
  the samples. None of these conditions are checked by this script.
- T-test: the t-test assumes that the samples come from populations which are
  normally distributed. that the variances of the sampled groups are ~ equal and
  that the individual observations are independent. None of these conditions are
  checked by this script.

Null and alternate hypothesis:

- G-test: The null hypothesis for the g_test (aka goodness of fit,
  log-likelihood ratio test) is that the frequency of any given OTU is equal
  across all sample groups. The alternate hypothesis is that the frequency of
  the OTU is not the same across all sample groups.
- Kruskal-Wallis: The null hypothesis is that the location paramater of the
  groups of abundances for a given OTU is the same. The alternate hypothesis is
  that at least one of the location parameters is different.
- ANOVA: the null hypothesis is that the means of the observations in the groups
  are the same, the alternate is that at least one is not.
- Mann-Whitney-U: the null hypothesis is that the distributions of the groups
  are equal, such that there is a 50 percent chance that a value from group1 is
  greater than a value from group2. The alternate is that the distributions are
  not the same.
- T-test: the null hypothesis is that the means of the two groups are the same
  versus the alternate that they are unequal.

The available tests are:

- ANOVA: one way analysis of variance. This test compares the within-group
  variance to the between-group variance in order to assess whether or not the
  sample groups have even frequencies of a given OTU. It generalizes the t-test
  to more than two groups. This is a parametric test whose assumptions are
  likely violated by data found in most gene surveys.

- kruskal_wallis: nonparametric ANOVA. This test is functionally an expansion of
  ANOVA to cases where the sample means are unequal and the distribution is not
  normal. The assumption that the distribution from which each group (within a
  single OTU) came is the same remains. This is a nonparametric test.

- g_test: goodness of fit or log-likelihood ratio test. This test compares the
  ratio of the OTU frequencies in the sample groups to an 'extrinsic hypothesis'
  about what their distribution should be. The extrinsic hypothesis coded in this
  script is that all sample groups have equal OTU frequencies. The test compares
  the ratio of the observed OTU frequencies in the sample groups to the expected
  frequencies based on the extrinsic hypothesis. This is a parametric test.

- parametric_t_test: Student's t-test. This test compares the frequencies of an
  OTU in one sample group versus another sample group to see what the probability
  of drawing the samples given that each sample had an equal proportion of the OTU
  in it. This is a parametric test whose assumptions are likely violated by data
  found in most gene surveys.

- nonparametric_t_test: nonparametric t-test is calculated using Monte Carlo
  simulation. This test performs in the same way as the parametric t-test, but
  computes the probability based on a boot-strap procedure where the sample
  group values are permuted. The fraction of the time that a t-statistic
  greater than or equal to the observed t-statistic is found is the basis of
  the nonparametric p-value. This is a nonparametric test.

- mann_whitney_u: aka Wilcoxon rank sum test is a nonparametric test where the
  null hypothesis is that the populations from which the two samples come have
  equal means. It is basically an extension of the t-test. This is a nonparametric
  test.

- bootstrap_mann_whitney_u: the bootstrapped version of the mann_whitney_u test.
  Identical behavior to the nonparametric_t_test. This is a nonparametric test.



Example usage: 
Print help message and exit
 group_significance.py -h

Find which OTUs have the highest probablilty of being differently represented depending on the sample category 'diet' using a G test: 
 group_significance.py -i otu_table.biom -m map_overlapping.txt -c diet -s g_test -o gtest_ocs.txt

Find which OTUs are differentially represented in two sample groups 'before_after' using a T-test: 
 group_significance.py -i otu_table.biom -m map_overlapping.txt -c before_after -s parametric_t_test -o tt_ocs.txt

Find which OTUs are differentially represented in the sample groups formed by 'diet' based on nonparamteric ANOVA, aka, Kruskal Wallis test. In addition, prevent the script from erroring because the biom table samples are a superset of the mapping file samples, and print the non-overlapping samples: 
 group_significance.py -i otu_table.biom -m map.txt -c diet -s kruskal_wallis -o kw_ocs.txt --biom_samples_are_superset --print_non_overlap

Find which OTUs are differentially represented in the sample groups formed by 'before_after' based on bootstrapped T-testing with 100 permutations: 
 group_significance.py -i otu_table.biom -m map_overlapping.txt -c before_after -s nonparametric_t_test --permutations 100 -o btt_ocs.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s TEST, --test=TEST  Test to use. Choices are: nonparametric_t_test,
                        bootstrap_mann_whitney_u, ANOVA, kruskal_wallis,
                        g_test, parametric_t_test, mann_whitney_u [default:
                        kruskal_wallis]
  --metadata_key=METADATA_KEY
                        Key to extract metadata from biom table. default:
                        taxonomy]
  --permutations=PERMUTATIONS
                        Number of permutations to use for bootstrapped
                        tests.[default: 1000]
  --biom_samples_are_superset
                        If this flag is passed you will be able to use a biom
                        table that contains all the samples listed in the
                        mapping file as well as additional samples not listed
                        in the mapping file. Only their intersecting samples
                        will be used for calculations.
  --print_non_overlap   If this flag is passed the script will display the
                        samples that do not overlap between the mapping file
                        and the biom file.

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        path to biom format table [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to category mapping file [REQUIRED]
    -c CATEGORY, --category=CATEGORY
                        name of the category over which to run the analysis
                        [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        path to the output file [REQUIRED]
Usage: identify_chimeric_seqs.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

A FASTA file of sequences, can be screened to remove chimeras (sequences generated due to the PCR amplification of multiple templates or parent sequences). QIIME currently includes a taxonomy-assignment-based approach, blast_fragments, for identifying sequences as chimeric and the ChimeraSlayer algorithm.

1. Blast_fragments approach:

The reference sequences (-r) and id-to-taxonomy map (-t) provided are the same format as those provided to assign_taxonomy.py. The reference sequences are in fasta format, and the id-to-taxonomy map contains tab-separated lines where the first field is a sequence identifier, and the second field is the taxonomy separated by semi-colons (e.g., Archaea;Euryarchaeota;Methanobacteriales;Methanobacterium). The reference collection should be derived from a chimera-checked database (such as the full greengenes database), and filtered to contain only sequences at, for example, a maximum of 97% sequence identity.

2. ChimeraSlayer:

ChimeraSlayer uses BLAST to identify potential chimera parents and computes the optimal branching alignment of the query against two parents.
We suggest to use the pynast aligned representative sequences as input.

3. usearch61:

usearch61 performs both de novo (abundance based) chimera and reference based detection.  Unlike the other two chimera checking software, unclustered sequences should be used as input rather than a representative sequence set, as these sequences need to be clustered to get abundance data.  The results can be taken as the union or intersection of all input sequences not flagged as chimeras.  For details, see: http://drive5.com/usearch/usearch_docs.html


Example usage: 
Print help message and exit
 identify_chimeric_seqs.py -h

blast_fragments example: For each sequence provided as input, the blast_fragments method splits the input sequence into n roughly-equal-sized, non-overlapping fragments, and assigns taxonomy to each fragment against a reference database. The BlastTaxonAssigner (implemented in assign_taxonomy.py) is used for this. The taxonomies of the fragments are compared with one another (at a default depth of 4), and if contradictory assignments are returned the sequence is identified as chimeric. For example, if an input sequence was split into 3 fragments, and the following taxon assignments were returned:

==========  ==========================================================
fragment1:  Archaea;Euryarchaeota;Methanobacteriales;Methanobacterium
fragment2:  Archaea;Euryarchaeota;Halobacteriales;uncultured
fragment3:  Archaea;Euryarchaeota;Methanobacteriales;Methanobacterium
==========  ==========================================================

The sequence would be considered chimeric at a depth of 3 (Methanobacteriales vs. Halobacteriales), but non-chimeric at a depth of 2 (all Euryarchaeota).

blast_fragments begins with the assumption that a sequence is non-chimeric, and looks for evidence to the contrary. This is important when, for example, no taxonomy assignment can be made because no blast result is returned. If a sequence is split into three fragments, and only one returns a blast hit, that sequence would be considered non-chimeric. This is because there is no evidence (i.e., contradictory blast assignments) for the sequence being chimeric. This script can be run by the following command, where the resulting data is written to the directory "identify_chimeras/" and using default parameters (e.g. chimera detection method ("-m blast_fragments"), number of fragments ("-n 3"), taxonomy depth ("-d 4") and maximum E-value ("-e 1e-30"))
 identify_chimeric_seqs.py -i repr_set_seqs.fasta -t taxonomy_assignment.txt -r ref_seq_set.fna -m blast_fragments -o chimeric_seqs_blast.txt

ChimeraSlayer Example: Identify chimeric sequences using the ChimeraSlayer algorithm against a user provided reference data base. The input sequences need to be provided in aligned (Py)Nast format. The reference data base needs to be provided as aligned FASTA (-a). Note that the reference database needs to be the same that was used to build the alignment of the input sequences!
 identify_chimeric_seqs.py -m ChimeraSlayer -i repr_set_seqs_aligned.fasta -a ref_seq_set_aligned.fasta -o chimeric_seqs_cs.txt

usearch61 Example: Identify chimeric sequences using the usearch61 algorithm against a user provided reference data base.  The input sequences should be the demultiplexed (not clustered rep set!) sequences, such as those output from split_libraries.py. The input sequences need to be provided as unaligned fasta in the same orientation as the query sequences.
 identify_chimeric_seqs.py -m usearch61 -i seqs.fna -r ref_sequences.fasta -o usearch61_chimera_checking/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t ID_TO_TAXONOMY_FP, --id_to_taxonomy_fp=ID_TO_TAXONOMY_FP
                        Path to tab-delimited file mapping sequences to
                        assigned taxonomy. Each assigned taxonomy is provided
                        as a comma-separated list. [default: none; REQUIRED
                        when method is blast_fragments]
  -r REFERENCE_SEQS_FP, --reference_seqs_fp=REFERENCE_SEQS_FP
                        Path to reference sequences (used to build a blast db
                        when method blast_fragments or reference database for
                        usearch61). [default: none; REQUIRED when method
                        blast_fragments if no blast_db is provided, suppress
                        requirement for usearch61 with
                        --suppress_usearch61_ref;]
  -a ALIGNED_REFERENCE_SEQS_FP, --aligned_reference_seqs_fp=ALIGNED_REFERENCE_SEQS_FP
                        Path to (Py)Nast aligned reference sequences. REQUIRED
                        when method ChimeraSlayer [default: none]
  -b BLAST_DB, --blast_db=BLAST_DB
                        Database to blast against. Must provide either
                        --blast_db or --reference_seqs_fp when method is
                        blast_fragments [default: none]
  -m CHIMERA_DETECTION_METHOD, --chimera_detection_method=CHIMERA_DETECTION_METHOD
                        Chimera detection method. Choices: blast_fragments or
                        ChimeraSlayer or usearch61. [default:ChimeraSlayer]
  -n NUM_FRAGMENTS, --num_fragments=NUM_FRAGMENTS
                        Number of fragments to split sequences into (i.e.,
                        number of expected breakpoints + 1) [default: 3]
  -d TAXONOMY_DEPTH, --taxonomy_depth=TAXONOMY_DEPTH
                        Number of taxonomic divisions to consider when
                        comparing taxonomy assignments [default: 4]
  -e MAX_E_VALUE, --max_e_value=MAX_E_VALUE
                        Max e-value to assign taxonomy [default: 1e-30]
  -R MIN_DIV_RATIO, --min_div_ratio=MIN_DIV_RATIO
                        min divergence ratio (passed to ChimeraSlayer). If set
                        to None uses ChimeraSlayer default value.  [default:
                        none]
  -k, --keep_intermediates
                        Keep intermediate files, useful for debugging
                        [default: False]
  --suppress_usearch61_intermediates
                        Use to suppress retention of usearch intermediate
                        files/logs.[default: False]
  --suppress_usearch61_ref
                        Use to suppress reference based chimera detection with
                        usearch61 [default: False]
  --suppress_usearch61_denovo
                        Use to suppress de novo based chimera detection with
                        usearch61 [default: False]
  --split_by_sampleid   Enable to split sequences by initial SampleID,
                        requires that fasta be in demultiplexed format, e.g.,
                        >Sample.1_0, >Sample.2_1, >Sample.1_2, with the
                        initial string before first underscore matching
                        SampleIDs. If not in this format, could cause
                        unexpected errors. [default: False]
  --non_chimeras_retention=NON_CHIMERAS_RETENTION
                        usearch61 only - selects subsets of sequences detected
                        as non-chimeras to retain after de novo and reference
                        based chimera detection.  Options are intersection or
                        union.  union will retain sequences that are flagged
                        as non-chimeric from either filter, while intersection
                        will retain only those sequences that are flagged as
                        non-chimeras from both detection methods. [default:
                        union]
  --usearch61_minh=USEARCH61_MINH
                        Minimum score (h). Increasing this value tends to
                        reduce the number of false positives and decrease
                        sensitivity.[default: 0.28]
  --usearch61_xn=USEARCH61_XN
                        Weight of 'no' vote. Increasing this value tends to
                        the number of false positives (and also sensitivity).
                        Must be > 1.[default: 8.0]
  --usearch61_dn=USEARCH61_DN
                        Pseudo-count prior for 'no' votes. (n). Increasing
                        this value tends to the number of false positives (and
                        also sensitivity). Must be > 0.[default: 1.4]
  --usearch61_mindiffs=USEARCH61_MINDIFFS
                        Minimum number of diffs in a segment. Increasing this
                        value tends to reduce the number of false positives
                        while reducing sensitivity to very low-divergence
                        chimeras. Must be > 0.[default: 3]
  --usearch61_mindiv=USEARCH61_MINDIV
                        Minimum divergence, i.e. 100% - identity between the
                        query and closest reference database sequence.
                        Expressed as a percentage, so the default is 0.8,
                        which allows chimeras that are up to 99.2% similar to
                        a reference sequence. This value is chosen to improve
                        sensitivity to very low-divergence chimeras.  Must be
                        > 0.[default: 0.8]
  --usearch61_abundance_skew=USEARCH61_ABUNDANCE_SKEW
                        Abundance skew setting for de novo chimera detection
                        with usearch61. Must be > 0. [default: 2.0]
  --percent_id_usearch61=PERCENT_ID_USEARCH61
                        Percent identity threshold for clustering with
                        usearch61, expressed as a fraction between 0 and 1.
                        [default: 0.97]
  --minlen=MINLEN       Minimum length of sequence allowed for usearch61
                        [default: 64]
  --word_length=WORD_LENGTH
                        word length value for usearch61. [default: 8]
  --max_accepts=MAX_ACCEPTS
                        max_accepts value to usearch61. [default: 1]
  --max_rejects=MAX_REJECTS
                        max_rejects value for usearch61.  [default: 8]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        Path to store output, output filepath in the case of
                        blast_fragments and ChimeraSlayer, or directory in
                        case of usearch61  [default: derived from
                        input_seqs_fp]
  --threads=THREADS     Specify number of threads per core to be used for
                        usearch61 commands that utilize multithreading. By
                        default, will calculate the number of cores to utilize
                        so a single thread will be used per CPU. Specify a
                        fractional number, e.g. 1.0 for 1 thread per core, or
                        0.5 for a single thread on a two core CPU. Only
                        applies to usearch61. [default: one_per_cpu]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
Usage: identify_missing_files.py [options] {-e/--expected_out_fp EXPECTED_OUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script checks for the existence of expected files in parallel runs, and is useful for checking the status of a parallel run or for finding out what poller.py is waiting on in a possibly failed run.

Example usage: 
Print help message and exit
 identify_missing_files.py -h

Example: Check for the existence of files listed in expected_out_files.txt from a PyNAST alignment run, and print a warning for any that are missing.
 identify_missing_files.py -e ALIGN_BQ7_/expected_out_files.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -e EXPECTED_OUT_FP, --expected_out_fp=EXPECTED_OUT_FP
                        the list of expected output files [REQUIRED]
Usage: identify_paired_differences.py [options] {-m/--mapping_fp MAPPING_FP -o/--output_dir OUTPUT_DIR -t/--state_category STATE_CATEGORY -x/--state_values STATE_VALUES -c/--individual_id_category INDIVIDUAL_ID_CATEGORY}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script provides a framework for paired-difference testing (i.e., analysis of data generated under a pre/post experimental design). In a pre/post experimental design, individuals are sampled before and after some 'treatment'. This code plots differences in values in the sample metadata (i.e., the mapping file) or observation counts in a BIOM table, and runs a (Bonferroni-corrected) one sample t-test on each sample metadata category or BIOM observation to determine if the mean of each distribution of pre/post differences differs from zero. If 'None' appears for the t score and p-values, this often means that the distribution of differences contained no variance, so the t-test could not be run. This can happen, for example, if the value passed for --valid_states is so restrictive that only a single sample is retained for analysis.

Example usage: 
Print help message and exit
 identify_paired_differences.py -h

Generate plots and stats for one category from the mapping file where the y-axis should be consistent across plots and the lines in the plots should be light blue.: 
 identify_paired_differences.py -m map.txt --metadata_categories 'Streptococcus Abundance' --state_category TreatmentState --state_values Pre,Post --individual_id_category PersonalID -o taxa_results --ymin 0 --ymax 60 --line_color '#eeefff'

Generate plots and stats for three categories from the mapping file.: 
 identify_paired_differences.py -m map.txt --metadata_categories 'Streptococcus Abundance,Phylogenetic Diversity,Observed OTUs' --state_category TreatmentState --state_values Pre,Post --individual_id_category PersonalID -o taxa_and_alpha_results

Generate plots for all observations in a biom file: 
 identify_paired_differences.py -m map.txt -b otu_table.biom --state_category TreatmentState --state_values Pre,Post --individual_id_category PersonalID -o otu_results

Generate plots for all observations in a biom file, but only including samples from individuals whose 'TreatmentResponse' was 'Improved' (as defined in the mapping file).: 
 identify_paired_differences.py -m map.txt -b otu_table.biom --state_category TreatmentState --state_values Pre,Post --individual_id_category PersonalID -o otu_results_improved_only --valid_states TreatmentResponse:Improved

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --ymin=YMIN           set the minimum y-value across plots [default:
                        determined on a per-plot basis]
  --ymax=YMAX           set the maximum y-value across plots [default:
                        determined on a per-plot basis]
  --metadata_categories=METADATA_CATEGORIES
                        ordered list of the mapping file column names to test
                        for paired differences (usually something like
                        "StreptococcusAbundance,Phylogenetic Diversity")
                        [default: none]
  --observation_ids=OBSERVATION_IDS
                        ordered list of the observation ids to test for paired
                        differences if a biom table is provided (usually
                        something like "otu1,otu2") [default: compute paired
                        differences for all observation ids]
  -b BIOM_TABLE_FP, --biom_table_fp=BIOM_TABLE_FP
                        path to biom table to use for computing paired
                        differences [default: none]
  -s VALID_STATES, --valid_states=VALID_STATES
                        string describing samples that should be included
                        based on their metadata (e.g.
                        'TreatmentResponse:Improved') [default: all samples
                        are included in analysis]
  --line_color=LINE_COLOR
                        color of lines in plots, useful if generating multiple
                        plots in different runs of this script to overlay on
                        top of one another. these can be specified as
                        matplotlib color names, or as html hex strings
                        [default: black]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the input metadata map filepath [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory where output files should be saved
                        [REQUIRED]
    -t STATE_CATEGORY, --state_category=STATE_CATEGORY
                        the mapping file column name to plot change over
                        (usually has values like "pre-treatment" and "post-
                        treatment") [REQUIRED]
    -x STATE_VALUES, --state_values=STATE_VALUES
                        ordered list of state values to test change over
                        (defines direction of graphs, generally something like
                        "pre-treatment,post-treatment"). currently limited to
                        two states. [REQUIRED]
    -c INDIVIDUAL_ID_CATEGORY, --individual_id_category=INDIVIDUAL_ID_CATEGORY
                        the mapping file column name containing each
                        individual's identifier (usually something like
                        "personal_identifier") [REQUIRED]
Usage: inflate_denoiser_output.py [options] {-c/--centroid_fps CENTROID_FPS -s/--singleton_fps SINGLETON_FPS -f/--fasta_fps FASTA_FPS -d/--denoiser_map_fps DENOISER_MAP_FPS -o/--output_fasta_fp OUTPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Inflate denoiser results so they can be passed directly to pick_otus.py, parallel_pick_otus_uclust_ref.py, or pick_de_novo_otus.py. Note that the results of this script have not be abundance sorted, so they must be before being passed to the OTU picker. The uclust OTU pickers incorporate this abundance presorting by default.

The inflation process writes each centroid sequence n times, where n is the number of reads that cluster to that centroid, and writes each singleton once. Flowgram identifiers are mapped back to post-split_libraries identifiers in this process (i.e., identifiers in fasta fps).


Example usage: 
Print help message and exit
 inflate_denoiser_output.py -h

Inflate the results of a single denoiser run.
 inflate_denoiser_output.py -c centroids.fasta -s singletons.fasta -f seqs.fna -d denoiser_mapping.txt -o inflated_seqs.fna

Inflate the results of multiple denoiser runs to a single inflated_seqs.fna file.
 inflate_denoiser_output.py -c centroids1.fasta,centroids2.fasta -s singletons1.fasta,singletons2.fasta -f seqs1.fna,seqs2.fna -d denoiser_mapping1.txt,denoiser_mapping2.txt -o inflated_seqs_combined.fna

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -c CENTROID_FPS, --centroid_fps=CENTROID_FPS
                        the centroid fasta filepaths [REQUIRED]
    -s SINGLETON_FPS, --singleton_fps=SINGLETON_FPS
                        the singleton fasta filepaths [REQUIRED]
    -f FASTA_FPS, --fasta_fps=FASTA_FPS
                        the input (to denoiser) fasta filepaths [REQUIRED]
    -d DENOISER_MAP_FPS, --denoiser_map_fps=DENOISER_MAP_FPS
                        the denoiser map filepaths [REQUIRED]
    -o OUTPUT_FASTA_FP, --output_fasta_fp=OUTPUT_FASTA_FP
                        the output fasta filepath [REQUIRED]
Usage: jackknifed_beta_diversity.py [options] {-i/--otu_table_fp OTU_TABLE_FP -o/--output_dir OUTPUT_DIR -e/--seqs_per_sample SEQS_PER_SAMPLE -m/--mapping_fp MAPPING_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

To directly measure the robustness of individual UPGMA clusters and clusters in PCoA plots, one can perform jackknifing (repeatedly resampling a subset of the available data from each sample).

Example usage: 
Print help message and exit
 jackknifed_beta_diversity.py -h

Example: These steps are performed by the following command: Compute beta diversity distance matrix from otu table (and tree, if applicable); build rarefied OTU tables by evenly sampling to the specified depth (-e); build UPGMA tree from full distance matrix; compute distance matrics for rarefied OTU tables; build UPGMA trees from rarefied OTU table distance matrices; build a consensus tree from the rarefied UPGMA trees; compare rarefied OTU table distance matrix UPGMA trees to either (full or consensus) tree for jackknife support of tree nodes; perform principal coordinates analysis on distance matrices generated from rarefied OTU tables; generate Emperor PCoA plots with jackknifed support.
 jackknifed_beta_diversity.py -i otu_table.biom -o bdiv_jk100 -e 100 -m Fasting_Map.txt -t rep_set.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t TREE_FP, --tree_fp=TREE_FP
                        path to the tree file [default: none; REQUIRED for
                        phylogenetic measures]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters . [if omitted, default values will
                        be used]
  --master_tree=MASTER_TREE
                        method for computing master trees in jackknife
                        analysis. "consensus": consensus of trees from
                        jackknifed otu tables.  "full": tree generated from
                        input (unsubsambled) otu table.  [default: consensus]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]
  -a, --parallel        Run in parallel where available [default: False]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start. NOTE: you must also pass -a
                        to run in parallel, this defines the number of jobs to
                        be started if and only if -a is passed [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        the input OTU table in biom format [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
    -e SEQS_PER_SAMPLE, --seqs_per_sample=SEQS_PER_SAMPLE
                        number of sequences to include in each jackknifed
                        subset [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to the mapping file [REQUIRED]
Usage: join_paired_ends.py [options] {-f/--forward_reads_fp FORWARD_READS_FP -r/--reverse_reads_fp REVERSE_READS_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script takes forward and reverse Illumina reads and joins them using the method chosen. Will optionally create an updated index reads file containing index reads for the surviving joined paired end reads. If the option to write an updated index file is chosen, be sure that the order and header format of the index reads is the same as the order and header format of reads in the files that will be joined (this is the default for reads generated on the Illumina instruments).

Currently, there are two methods that can be selected by the user to join paired-end data:

1. fastq-join - Erik Aronesty, 2011. ea-utils : "Command-line tools for processing biological sequencing data" (http://code.google.com/p/ea-utils)

2. SeqPrep - (https://github.com/jstjohn/SeqPrep)


Example usage: 
Print help message and exit
 join_paired_ends.py -h

Join paired-ends with 'fastq-join': This is the default method to join paired-end Illumina data
 join_paired_ends.py -f $PWD/forward_reads.fastq -r $PWD/reverse_reads.fastq -o $PWD/fastq-join_joined

Join paired-ends with 'SeqPrep': Produces similar output to the 'fastq-join' but returns data in gzipped format.
 join_paired_ends.py -m SeqPrep -f $PWD/forward_reads.fastq -r $PWD/reverse_reads.fastq -o $PWD/SeqPrep_joined

Update the index / barcode reads file to match the surviving joined pairs.: This is required if you will be using split_libraries_fastq.py.
 join_paired_ends.py -f $PWD/forward_reads.fastq -r $PWD/reverse_reads.fastq -b $PWD/barcodes.fastq -o $PWD/fastq-join_joined

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m PE_JOIN_METHOD, --pe_join_method=PE_JOIN_METHOD
                        Method to use for joining paired-ends. Valid choices
                        are: fastq-join, SeqPrep [default: fastq-join]
  -b INDEX_READS_FP, --index_reads_fp=INDEX_READS_FP
                        Path to the barcode / index reads in FASTQ format.
                        Will be filtered based on surviving joined pairs.
  -j MIN_OVERLAP, --min_overlap=MIN_OVERLAP
                        Applies to both fastq-join and SeqPrep methods.
                        Minimum allowed overlap in base-pairs required to join
                        pairs. If not set, progam defaults will be used. Must
                        be an integer. [default: none]
  -p PERC_MAX_DIFF, --perc_max_diff=PERC_MAX_DIFF
                        Only applies to fastq-join method, otherwise ignored.
                        Maximum allowed % differences within region of
                        overlap. If not set, progam defaults will be used.
                        Must be an integer between 1-100 [default: none]
  -y MAX_ASCII_SCORE, --max_ascii_score=MAX_ASCII_SCORE
                        Only applies to SeqPrep method, otherwise ignored.
                        Maximum quality score / ascii code allowed to appear
                        within joined pairs output. For more information,
                        please see: http://en.wikipedia.org/wiki/FASTQ_format.
                        [default: J]
  -n MIN_FRAC_MATCH, --min_frac_match=MIN_FRAC_MATCH
                        Only applies to SeqPrep method, otherwise ignored.
                        Minimum allowed fraction of matching bases required to
                        join reads. Must be a float between 0-1. If not set,
                        progam defaults will be used. [default: none]
  -g MAX_GOOD_MISMATCH, --max_good_mismatch=MAX_GOOD_MISMATCH
                        Only applies to SeqPrep method, otherwise ignored.
                        Maximum mis-matched high quality bases allowed to join
                        reads. Must be a float between 0-1. If not set, progam
                        defaults will be used. [default: none]
  -6 PHRED_64, --phred_64=PHRED_64
                        Only applies to SeqPrep method, otherwise ignored. Set
                        if input reads are in phred+64 format. Output will
                        always be phred+33. [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f FORWARD_READS_FP, --forward_reads_fp=FORWARD_READS_FP
                        Path to input forward reads in FASTQ format.
                        [REQUIRED]
    -r REVERSE_READS_FP, --reverse_reads_fp=REVERSE_READS_FP
                        Path to input reverse reads in FASTQ format.
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Directory to store result files [REQUIRED]
Usage: load_remote_mapping_file.py [options] {-k/--spreadsheet_key SPREADSHEET_KEY -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script exports, downloads, and saves a mapping file that is stored
remotely. Currently, the only type of remote mapping file that is supported is
a Google Spreadsheet, though other methods of remote storage may be supported
in the future.

For more information and examples pertaining to this script and remote mapping
files in general, please refer to the accompanying tutorial, which can be found
at http://qiime.org/tutorials/remote_mapping_files.html.


Example usage: 
Print help message and exit
 load_remote_mapping_file.py -h

Load mapping file from Google Spreadsheet: The following command exports and downloads a QIIME metadata mapping file from a Google Spreadsheet, using the data found in the first worksheet of the spreadsheet.
 load_remote_mapping_file.py -k 0AnzomiBiZW0ddDVrdENlNG5lTWpBTm5kNjRGbjVpQmc -o example1_map.txt

Load specific worksheet: The following command exports from a worksheet named 'Fasting_Map'.
 load_remote_mapping_file.py -k 0AnzomiBiZW0ddDVrdENlNG5lTWpBTm5kNjRGbjVpQmc -w Fasting_Map -o example2_map.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -w WORKSHEET_NAME, --worksheet_name=WORKSHEET_NAME
                        the name of the worksheet in the Google Spreadsheet
                        that contains the mapping file. If the worksheet name
                        contains spaces, please include quotes around the
                        name. [default: the first worksheet in the Google
                        Spreadsheet will be used]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -k SPREADSHEET_KEY, --spreadsheet_key=SPREADSHEET_KEY
                        the spreadsheet key that will be used to identify the
                        Google Spreadsheet to load. This is the part of the
                        Google Spreadsheet URL that comes after 'key='. You
                        may instead provide the entire URL and the key will be
                        extracted from it. If you provide the entire URL, you
                        may need to enclose it in single quotes [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath [REQUIRED]
Usage: make_2d_plots.py [options] {-i/--coord_fname COORD_FNAME -m/--map_fname MAP_FNAME}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script generates 2D PCoA plots using the principal coordinates file generated by performing beta diversity measures of an OTU table.

Example usage: 
Print help message and exit
 make_2d_plots.py -h

Default Example: If you just want to use the default output, you can supply the principal coordinates file (i.e., resulting file from principal_coordinates.py), where the default coloring will be based on the SampleID as follows
 make_2d_plots.py -i unweighted_unifrac_pc.txt -m Fasting_Map.txt

Output Directory Usage: If you want to give an specific output directory (e.g. "2d_plots"), use the following code.
 make_2d_plots.py -i unweighted_unifrac_pc.txt -m Fasting_Map.txt -o 2d_plots/

Mapping File Usage: Additionally, the user can supply their mapping file ('-m') and a specific category to color by ('-b') or any combination of categories. When using the -b option, the user can specify the coloring for multiple mapping labels, where each mapping label is separated by a comma, for example: -b 'mapping_column1,mapping_column2'. The user can also combine mapping labels and color by the combined label that is created by inserting an '&&' between the input columns, for example: -b 'mapping_column1&&mapping_column2'.If the user wants to color by specific mapping labels, they can use the following code
 make_2d_plots.py -i unweighted_unifrac_pc.txt -m Fasting_Map.txt -b 'Treatment'

Scree plot Usage: A scree plot can tell you how many axes are likely to be important and help determine how many 'real' underlying gradients there might be in your data as well as their relative 'strength'. If you want to generate a scree plot, use the following code.
 make_2d_plots.py -i unweighted_unifrac_pc.txt -m Fasting_Map.txt --scree

Color by all categories: If the user would like to color all categories in their metadata mapping file, they should not pass -b. Color by all is the default behavior.
 make_2d_plots.py -i unweighted_unifrac_pc.txt -m Fasting_Map.txt

Prefs File: The user can supply a prefs file to color by, as follows
 make_2d_plots.py -i unweighted_unifrac_pc.txt -m Fasting_Map.txt -p prefs.txt

Jackknifed Principal Coordinates (w/ confidence intervals): If you have created jackknifed PCoA files, you can pass the folder containing those files, instead of a single file.  The user can also specify the opacity of the ellipses around each point '--ellipsoid_opacity', which is a value from 0-1. Currently there are two metrics '--ellipsoid_method' that can be used for generating the ellipsoids, which are 'IQR' and 'sdev'. The user can specify all of these options as follows
 make_2d_plots.py -i pcoa/ -m Fasting_Map.txt -b 'Treatment&&DOB' --ellipsoid_opacity=0.5 --ellipsoid_method=IQR

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -b COLORBY, --colorby=COLORBY
                        Comma-separated list categories metadata categories
                        (column headers) to color by in the plots. The
                        categories must match the name of a column header in
                        the mapping file exactly. Multiple categories can be
                        list by comma separating them without spaces. The user
                        can also combine columns in the mapping file by
                        separating the categories by "&&" without spaces.
                        [default=color by all]
  -p PREFS_PATH, --prefs_path=PREFS_PATH
                        Input user-generated preferences filepath. NOTE: This
                        is a file with a dictionary containing preferences for
                        the analysis. [default: none]
  -k BACKGROUND_COLOR, --background_color=BACKGROUND_COLOR
                        Background color to use in the plots. [default: white]
  --ellipsoid_opacity=ELLIPSOID_OPACITY
                        Used only when plotting ellipsoids for jackknifed beta
                        diversity (i.e. using a directory of coord files
                        instead of a single coord file). The valid range is
                        between 0-1. 0 produces completely transparent
                        (invisible) ellipsoids and 1 produces completely
                        opaque ellipsoids. [default=0.33]
  --ellipsoid_method=ELLIPSOID_METHOD
                        Used only when plotting ellipsoids for jackknifed beta
                        diversity (i.e. using a directory of coord files
                        instead of a single coord file). Valid values are
                        "IQR" and "sdev". [default=IQR]
  --master_pcoa=MASTER_PCOA
                        Used only when plotting ellipsoids for jackknifed beta
                        diversity  (i.e. using a directory of coord files
                        instead of a single coord file). These coordinates
                        will be the center of each ellipisoid. [default: none;
                        arbitrarily chosen PC matrix will define the center
                        point]
  --scree               Generate the scree plot [default: False]
  --pct_variation_below_one
                        Allow the percent variation explained by the axes to
                        be below one. The default behaivor is to multiply by
                        100 all values if PC1 is < 1.0 [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i COORD_FNAME, --coord_fname=COORD_FNAME
                        Input principal coordinates filepath (i.e., resulting
                        file from principal_coordinates.py).  Alternatively, a
                        directory containing multiple principal coordinates
                        files for jackknifed PCoA results. [REQUIRED]
    -m MAP_FNAME, --map_fname=MAP_FNAME
                        Input metadata mapping filepath [REQUIRED]
Usage: make_bipartite_network.py [options] {-i/--biom_fp BIOM_FP -m/--map_fp MAP_FP -o/--output_dir OUTPUT_DIR -k/--observation_md_header_key OBSERVATION_MD_HEADER_KEY --md_fields MD_FIELDS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script was created to ease the process of making bipartite networks
that have appeared in high profile publications including
10.1073/pnas.1217767110 and 10.1126/science.1198719. The script will take a
biom table and a mapping file and produce an edge table which connects each
sample in the biom table to the observations found in that sample. It is
bipartite because there are two distinct node classes -- OTU and Sample. The
'OTU' node class does not have to be an operational taxonomic unit, it can be a
KEGG category or metabolite etc.  -- anything that is an observation. The edges
are weighted by the abundance of the observation in the sample to which it is
connected. The output files of this script are intended to be loaded into
Cytoscape. The EdgeTable should be uploaded first, and then the NodeAttrTable
file can be uploaded as node attributes to control coloring, sizing, and
shaping as the user desires. The overall idea behind this script is to make
bipartite network creation easier.  To that end, the color, size, and shape
options are used to provide fields in the NetworkViz tab of Cytoscape so that
nodes can be appropriately presented.  Those options are passed via comma
separated strings (as in the example below).  The most common visualization
strategy is to color sample nodes by a metadata category like timepoint or pH,
color OTU nodes by one of their taxonomic levels, and to scale OTU node size by
abundance. This script makes this process easy (as well as a myriad of other
visualiation strategies). Once the tables are created by this script they must
be opened in cytoscape. This process is described in detail in the QIIME
bipartite network tutorial available at:
http://qiime.org/tutorials/making_cytoscape_networks.html All color, size, and
shape options in this script default to 'NodeType'. OTU nodes have NodeType:
otu, sample nodes have NodeType: sample. Thus, if you ran this script with
defaults, you would only be able to change the shape, size, and color of the
nodes depending on whether or not they were observations or samples. You would
not be able to distinguish between two observations based on color, shape, or
size. The script is flexible in that it allows you to pass any number of fields
for the --{s,o}{shape,size,color}. This will allow you to distinguish between
OTU and sample nodes in a huge number of different ways. The usage examples
below show some of the common use cases and what options you would pass to
emulate them. There are a couple of important considerations for using this
script:

Note that the --md_fields option has a different meaning depending on the type
of metadata in the biom table. Regardless of type, the md_fields will be the
headers in the OTUNodeTable.txt. If the metadata is a dict or default dict, the
md_fields will be used as keys to extract data from the biom file metadata. If
the metadata is a list or a string, then the md_fields will be have no
intrinsic relation to the columns they head. For example if
md_fields=['k','p','c'] and the metadata contained in a given OTU was
'k__Bacteria;p__Actinobacter;c__Actino' the resulting OTUNodeTable would have
k__Bacteria in the 'k' column, p__Actinobacter in the 'p' column, and c__Actino
in the 'c' column. If one passed md_fields=['1.0','XYZ','Five'] then the
OTUNodeTable would have columns headed by ['1.0','XYZ','Five'], but the
metadata values in those columns would be the same (e.g. '1.0' column entry
would be k__Bacteria etc.) If the number of elements in the metadata for a
given OTU is not equal to the number of headers provided the script will adjust
the OTU metadata. In the case where the metadata is too short, it will add
'Other' into the OTU metadata until the required length is reached.  In the
case where the metadata is too long it will simply remove extra entries. This
means you can end up with many observations which have the value of 'Other' if
you have short taxonomic strings/lists for your observations.

The available fields for both sample and otu nodes are:
[NodeType, Abundance]

For observation nodes the additional fields available are:
any fields you passed for the md_fields

For sample nodes the additional fields available are
any fields found in the mapping file headers

If multiple fields are passed for a given option, they will be concatenated in the output with a '_' character.


Example usage: 
Print help message and exit
 make_bipartite_network.py -h

Create an EdgeTable and NodeAttrTable that allow you to color sample nodes with one of their metadata categories (Treatment for our example), observation nodes (in this case OTUs) by their taxonomic level (class for our example), control observation node size by their abundance, and control node shape by whether its an observation or sample.
 make_bipartite_network.py -i otu_table.biom -m mapping_file.txt -k taxonomy --md_fields 'k,p,c,o,f' -o bipartite_network/ --scolors 'Treatment' --ocolors 'c' --osize 'Abundance'

Create an EdgeTable and NodeAttrTable that allow you to color sample nodes by a combination of their time point and diet, color observation nodes by their abundance and family, and node shape by whether the node is an observation or sample. Note that the names in the --md_fields are irrelevant as long as the field passed for --ocolors is available. The length is important however, since there are 5 levels in our OTU table. If fewer fewer than 5 fields were passed for --md_fields we would get an error.
 make_bipartite_network.py -i otu_table.biom -m mapping_file.txt -k taxonomy --md_fields 'a1,a2,a3,a4,f' -o bipartite_network_combo_colors/ --scolors 'TimePt,Diet' --ocolors 'f,Abundance'

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --scolors=SCOLORS     commas separated string specifying fields of interest
                        for sample node coloring [default: NodeType].
  --ocolors=OCOLORS     commas separated string specifying fields of interest
                        for observation node coloring [default: NodeType].
  --sshapes=SSHAPES     commas separated string specifying fields of interest
                        for sample node shape [default: NodeType].
  --oshapes=OSHAPES     commas separated string specifying fields of interest
                        for observation node shape [default: NodeType].
  --ssizes=SSIZES       commas separated string specifying fields of interest
                        for sample node size [default: NodeType].
  --osizes=OSIZES       commas separated string specifying fields of interest
                        for observation node size [default: NodeType].

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i BIOM_FP, --biom_fp=BIOM_FP
                        the input file path for biom table. [REQUIRED]
    -m MAP_FP, --map_fp=MAP_FP
                        the input file path for mapping file. [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory to be created for storing the results.
                        [REQUIRED]
    -k OBSERVATION_MD_HEADER_KEY, --observation_md_header_key=OBSERVATION_MD_HEADER_KEY
                        Key to retrieve metadata (usually taxonomy) from the
                        biom file. [REQUIRED]
    --md_fields=MD_FIELDS
                        metadata fields that will be the headers of the
                        OTUNodeTable. If the biom table has metadata
                        dictionaries, md_fields will be the keys extracted
                        from the biom table metadata. Passed like
                        "kingdom,phylum,class". [REQUIRED]
Usage: make_bootstrapped_tree.py [options] {-m/--master_tree MASTER_TREE -s/--support SUPPORT -o/--output_file OUTPUT_FILE}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script takes a tree and bootstrap support file and creates a pdf, colored by bootstrap support.

Example usage: 
Print help message and exit
 make_bootstrapped_tree.py -h

Example: In this example, the user supplies a tree file and a text file containing the jackknife support information, which results in a pdf file
 make_bootstrapped_tree.py -m master_tree.tre -s jackknife_support.txt -o jackknife_samples.pdf

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MASTER_TREE, --master_tree=MASTER_TREE
                        This is the path to the master tree [REQUIRED]
    -s SUPPORT, --support=SUPPORT
                        This is the path to the bootstrap support file
                        [REQUIRED]
    -o OUTPUT_FILE, --output_file=OUTPUT_FILE
                        This is the filename where the output should be
                        written. [REQUIRED]
Usage: make_distance_boxplots.py [options] {-m/--mapping_fp MAPPING_FP -o/--output_dir OUTPUT_DIR -d/--distance_matrix_fp DISTANCE_MATRIX_FP -f/--fields FIELDS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script creates boxplots that allow for the comparison between different
categories found within the mapping file. The boxplots that are created compare
distances within all samples of a field value, as well as between different
field values. Individual within and between distances are also plotted.

The script also performs two-sample t-tests for all pairs of boxplots to help
determine which boxplots (distributions) are significantly different.

Tip: the script tries its best to fit everything into the plot, but there are
cases where plot elements may get cut off (e.g. if axis labels are extremely
long), or things may appear squashed, cluttered, or too small (e.g. if
there are many boxplots in one plot). Increasing the width and/or height of the
plot (using --width and --height) usually fixes these problems.

For more information and examples pertaining to this script, please refer to
the accompanying tutorial, which can be found at
http://qiime.org/tutorials/creating_distance_comparison_plots.html.


Example usage: 
Print help message and exit
 make_distance_boxplots.py -h

Compare distances between Fast and Control samples: This example will generate an image with boxplots for all within and all between distances for the field Treatment, and will also include plots for individual within (e.g. Control vs. Control, Fast vs. Fast) and individual between (e.g. Control vs. Fast). The generated plot PDF and signifiance testing results will be written to the output directory 'out1'.
 make_distance_boxplots.py -d unweighted_unifrac_dm.txt -m Fasting_Map.txt -f "Treatment" -o out1

Only plot individual field value distances: This example will generate a PNG of all individual field value distances (within and between) for the Treatment field.
 make_distance_boxplots.py -d unweighted_unifrac_dm.txt -m Fasting_Map.txt -f "Treatment" -o out2 -g png --suppress_all_within --suppress_all_between

Save raw data: This example will generate an SVG image of the boxplots and also output the plotting data to a tab-delimited file.
 make_distance_boxplots.py -d unweighted_unifrac_dm.txt -m Fasting_Map.txt -f "Treatment" -o out3 -g svg --save_raw_data

Suppress significance tests: This example will only generate a plot and skip the significance testing step. This can be useful if you are operating on a large dataset and are not interested in performing the statistical tests (or at least not initially).
 make_distance_boxplots.py -d unweighted_unifrac_dm.txt -m Fasting_Map.txt -f "Treatment" -o out4 --suppress_significance_tests

Sort boxplots: To sort the boxplots by increasing median, supply the --sort option.
 make_distance_boxplots.py -d unweighted_unifrac_dm.txt -m Fasting_Map.txt -f "Treatment" -o out5 --sort median

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -g IMAGETYPE, --imagetype=IMAGETYPE
                        type of image to produce (i.e. png, svg, pdf)
                        [default: pdf]
  --save_raw_data       store raw data used to create boxplots in tab-
                        delimited files [default: False]
  --suppress_all_within
                        suppress plotting of "all within" boxplot [default:
                        False]
  --suppress_all_between
                        suppress plotting of "all between" boxplot [default:
                        False]
  --suppress_individual_within
                        suppress plotting of individual "within" boxplot(s)
                        [default: False]
  --suppress_individual_between
                        suppress plotting of individual "between" boxplot(s)
                        [default: False]
  --suppress_significance_tests
                        suppress performing signifance tests between each pair
                        of boxplots [default: False]
  -n NUM_PERMUTATIONS, --num_permutations=NUM_PERMUTATIONS
                        the number of Monte Carlo permutations to perform when
                        calculating the nonparametric p-value in the
                        significance tests. Must be an integer greater than or
                        equal to zero. If zero, the nonparametric p-value will
                        not be calculated and will instead be reported as
                        "N/A". This option has no effect if
                        --suppress_significance_tests is supplied [default: 0]
  -t TAIL_TYPE, --tail_type=TAIL_TYPE
                        the type of tail test to compute when calculating the
                        p-values in the significance tests. "high" specifies a
                        one-tailed test for values greater than the observed t
                        statistic, while "low" specifies a one-tailed test for
                        values less than the observed t statistic. "two-sided"
                        specifies a two-tailed test for values greater in
                        magnitude than the observed t statistic. This option
                        has no effect if --suppress_significance_tests is
                        supplied. Valid choices: low or high or two-sided
                        [default: two-sided]
  --y_min=Y_MIN         the minimum y-axis value in the resulting plot. If
                        "auto", it is automatically calculated [default: 0]
  --y_max=Y_MAX         the maximum y-axis value in the resulting plot. If
                        "auto", it is automatically calculated [default: 1]
  --width=WIDTH         width of the output image in inches. If not provided,
                        a "best guess" width will be used [default: auto]
  --height=HEIGHT       height of the output image in inches [default: 6]
  --transparent         make output images transparent (useful for overlaying
                        an image on top of a colored background) [default:
                        False]
  --whisker_length=WHISKER_LENGTH
                        length of the whiskers as a function of the IQR. For
                        example, if 1.5, the whiskers extend to 1.5 * IQR.
                        Anything outside of that range is seen as an outlier
                        [default: 1.5]
  --box_width=BOX_WIDTH
                        width of each box in plot units [default: 0.5]
  --box_color=BOX_COLOR
                        the color of the boxes. Can be any valid matplotlib
                        color string, such as "black", "magenta", "blue", etc.
                        See
                        http://matplotlib.sourceforge.net/api/colors_api.html
                        for more examples of valid color strings that may be
                        used. Will be ignored if
                        --color_individual_within_by_field is supplied
                        [default: same as plot background, which is white
                        unless --transparent is enabled]
  --color_individual_within_by_field=COLOR_INDIVIDUAL_WITHIN_BY_FIELD
                        field in the the mapping file to color the individual
                        "within" boxes by. A legend will be provided to match
                        boxplot colors to field states. A one-to-one mapping
                        must exist between the field to be colored and the
                        field to color by, otherwise the coloring will be
                        ambiguous. If this option is supplied, --box_color
                        will be ignored. If --suppress_individual_within is
                        supplied, this option will be ignored [default: none]
  --sort=SORT           If "median", sort boxplots by increasing median. If
                        "alphabetical", sort boxplots alphabetically by their
                        labels. If this option is not supplied (the default),
                        boxplots will be grouped logically as follows: all
                        within, all between, individual within, and individual
                        between [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
    -d DISTANCE_MATRIX_FP, --distance_matrix_fp=DISTANCE_MATRIX_FP
                        input distance matrix filepath (i.e. the result of
                        beta_diversity.py). WARNING: Only symmetric, hollow
                        distance matrices may be used as input. Asymmetric
                        distance matrices, such as those obtained by the
                        UniFrac Gain metric (i.e. beta_diversity.py -m
                        unifrac_g), should not be used as input [REQUIRED]
    -f FIELDS, --fields=FIELDS
                        comma-separated list of fields to compare, where the
                        list of fields should be in quotes (e.g.
                        "Field1,Field2,Field3") [REQUIRED]
Usage: make_distance_comparison_plots.py [options] {-m/--mapping_fp MAPPING_FP -o/--output_dir OUTPUT_DIR -d/--distance_matrix_fp DISTANCE_MATRIX_FP -f/--field FIELD -c/--comparison_groups COMPARISON_GROUPS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script creates plots (bar charts, scatter plots, or box plots) that
allow for the comparison between samples grouped at different field states
of a mapping file field.

This script can work with any field in the mapping file, and it can compare
any number of field states to all other field states within that field.
This script may be especially useful for fields that represent a time series,
because a plot can be generated showing the distances between samples at
certain timepoints against all other timepoints.

For example, a time field might contain the values 1, 2, 3, 4, and 5, which
label samples that are from day 1, day 2, day 3, and so on. This time field
can be specified when the script is run, as well as the timepoint(s) to
compare to every other timepoint. For example, two comparison groups
might be timepoints 1 and 2. The resulting plot would contain timepoints for
days 3, 4, and 5 along the x-axis, and at each of those timepoints, the
distances between day 1 and that timepoint would be plotted, as well as the
distances between day 2 and the timepoint.

The script also performs two-sample t-tests for all pairs of distributions to
help determine which distributions are significantly different from each other.

Tip: the script tries its best to fit everything into the plot, but there are
cases where plot elements may get cut off (e.g. if axis labels are extremely
long), or things may appear squashed, cluttered, or too small (e.g. if
there are many boxplots in one plot). Increasing the width and/or height of the
plot (using --width and --height) usually fixes these problems.

For more information and examples pertaining to this script, please refer to
the accompanying tutorial, which can be found at
http://qiime.org/tutorials/creating_distance_comparison_plots.html.


Example usage: 
Print help message and exit
 make_distance_comparison_plots.py -h

Compare distances between Native and Input samples for each timepoint in the Time field: This example will generate a PDF containing a bar chart with the distances between Native samples and every other timepoint, as well as the distances between Input samples and every other timepoint. The output image will be put in the 'out1' directory. For more details about this example input data, please refer to the accompanying tutorial.
 make_distance_comparison_plots.py -d forearm_only_unweighted_unifrac_dm.txt -m costello_timeseries_map.txt -f TIME_SINCE_TRANSPLANT -c "Native,Input" -o out1

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t PLOT_TYPE, --plot_type=PLOT_TYPE
                        type of plot to produce ("bar" is bar chart, "scatter"
                        is scatter plot, and "box" is box plot) [default: bar]
  -g IMAGETYPE, --imagetype=IMAGETYPE
                        type of image to produce (i.e. png, svg, pdf)
                        [default: pdf]
  --save_raw_data       store raw data used to create plot in a tab-delimited
                        file [default: False]
  --suppress_significance_tests
                        suppress performing signifance tests between each pair
                        of distributions [default: False]
  -n NUM_PERMUTATIONS, --num_permutations=NUM_PERMUTATIONS
                        the number of Monte Carlo permutations to perform when
                        calculating the nonparametric p-value in the
                        significance tests. Must be an integer greater than or
                        equal to zero. If zero, the nonparametric p-value will
                        not be calculated and will instead be reported as
                        "N/A". This option has no effect if
                        --suppress_significance_tests is supplied [default: 0]
  --tail_type=TAIL_TYPE
                        the type of tail test to compute when calculating the
                        p-values in the significance tests. "high" specifies a
                        one-tailed test for values greater than the observed t
                        statistic, while "low" specifies a one-tailed test for
                        values less than the observed t statistic. "two-sided"
                        specifies a two-tailed test for values greater in
                        magnitude than the observed t statistic. This option
                        has no effect if --suppress_significance_tests is
                        supplied. Valid choices: low or high or two-sided
                        [default: two-sided]
  --width=WIDTH         width of the output image in inches [default: 12]
  --height=HEIGHT       height of the output image in inches [default: 6]
  --x_tick_labels_orientation=X_TICK_LABELS_ORIENTATION
                        type of orientation for x-axis tick labels [default:
                        vertical]
  -a LABEL_TYPE, --label_type=LABEL_TYPE
                        Label type ("numeric" or "categorical"). If the label
                        type is defined as numeric, the x-axis will be scaled
                        accordingly. Otherwise the x-values will treated
                        categorically and will be evenly spaced [default:
                        categorical].
  --y_min=Y_MIN         the minimum y-axis value in the resulting plot. If
                        "auto", it is automatically calculated [default: 0]
  --y_max=Y_MAX         the maximum y-axis value in the resulting plot. If
                        "auto", it is automatically calculated [default: 1]
  --transparent         make output images transparent (useful for overlaying
                        an image on top of a colored background ) [default:
                        False]
  --whisker_length=WHISKER_LENGTH
                        if --plot_type is "box", determines the length of the
                        whiskers as a function of the IQR. For example, if
                        1.5, the whiskers extend to 1.5 * IQR. Anything
                        outside of that range is seen as an outlier. If
                        --plot_type is not "box", this option is ignored
                        [default: 1.5]
  --error_bar_type=ERROR_BAR_TYPE
                        if --plot_type is "bar", determines the type of error
                        bars to use. "stdv" is standard deviation and "sem" is
                        the standard error of the mean. If --plot_type is not
                        "bar", this option is ignored [default: stdv]
  --distribution_width=DISTRIBUTION_WIDTH
                        width (in plot units) of each individual distribution
                        (e.g. each bar if the plot type is a bar chart, or the
                        width of each box if the plot type is a boxplot)
                        [default: auto]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
    -d DISTANCE_MATRIX_FP, --distance_matrix_fp=DISTANCE_MATRIX_FP
                        input distance matrix filepath (i.e. the result of
                        beta_diversity.py). WARNING: Only symmetric, hollow
                        distance matrices may be used as input. Asymmetric
                        distance matrices, such as those obtained by the
                        UniFrac Gain metric (i.e. beta_diversity.py -m
                        unifrac_g), should not be used as input [REQUIRED]
    -f FIELD, --field=FIELD
                        field in the mapping file to make comparisons on
                        [REQUIRED]
    -c COMPARISON_GROUPS, --comparison_groups=COMPARISON_GROUPS
                        comma-separated list of field states to compare to
                        every other field state, where the list of field
                        states should be in quotes (e.g.
                        "FieldState1,FieldState2,FieldState3") [REQUIRED]
Usage: make_fastq.py [options] {-f/--input_fasta_fp INPUT_FASTA_FP -q/--qual QUAL_FPS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The ERA currently requires a separate FASTQ file for each library, split by library id. This code takes the output from split_libraries.py and the corresponding QUAL files and produces ERA-compatible FASTQ files.

Example usage: 
Print help message and exit
 make_fastq.py -h

Example: Take input FASTA file input_fasta_filepath and QUAL file input_qual_filepath: make separate file for each library (with the -s option: assumes that the FASTA file is the output of split_libraries.py or similar script)
 make_fastq.py -f $PWD/seqs.fna -q $PWD/Fasting_Example.qual -s

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o RESULT_FP, --result_fp=RESULT_FP
                        Path to store results [default:
                        <input_sequences_filename>.fastq]
  -s, --split           make separate file for each library [default:False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
    -q QUAL_FPS, --qual=QUAL_FPS
                        names of QUAL files, comma-delimited [REQUIRED]
Usage: make_library_id_lists.py [options] {-i/--input_fasta IN_FASTA}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Makes a list of the ids corresponding to each library represented in the input fasta file. Assumes that the libraries are the output of split_libraries.py and that they contain the 454 read id for each sequence as is standard in the split_libraries.py output. Produces a separate file for each library.

Example usage: 
Print help message and exit
 make_library_id_lists.py -h

Example: Create a list containing library ids for a fasta file (seqs.fna)
 make_library_id_lists.py -i seqs.fna -o results/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s SCREENED_REP_SEQS, --screened_rep_seqs=SCREENED_REP_SEQS
                        The path to a FASTA file containing screened
                        representative seqs[DEFAULT: none]
  -u OTUS, --otus=OTUS  The path to an OTU file mapping OTUs onto rep
                        seqs[DEFAULT: none]
  -o OUTDIR, --outdir=OUTDIR
                         The base directory to save results (one file per
                        library).
  -f FIELD, --field=FIELD
                        Index of space-delimited field to read id from
                        [DEFAULT: 1]
  --debug               Show debug output.

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i IN_FASTA, --input_fasta=IN_FASTA
                        The path to a FASTA file containing input sequences
                        [REQUIRED]
Usage: make_otu_heatmap.py [options] {-i/--otu_table_fp OTU_TABLE_FP -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script visualizes an OTU table as a heatmap where each row corresponds to an OTU and each column corresponds to a sample. The higher the relative abundance of an OTU in a sample, the more intense the color at the corresponsing position in the heatmap. By default, the OTUs (rows) will be clustered by UPGMA hierarchical clustering, and the samples (columns) will be presented in the order in which they appear in the OTU table. Alternatively, the user may supply a tree to sort the OTUs (rows) or samples (columns), or both. The user may also pass in a mapping file for sorting samples. If the user passes in a mapping file and a metadata category, samples (columns) will be grouped by category value and subsequently clustered within each group.

Example usage: 
Print help message and exit
 make_otu_heatmap.py -h

Generate a heatmap as a PDF using all default values
 make_otu_heatmap.py -i otu_table.biom -o heatmap.pdf

Generate a heatmap as a PNG
 make_otu_heatmap.py -i otu_table.biom -o heatmap.png -g png

Sort the heatmap columns (samples) by the order of samples in the mapping file
 make_otu_heatmap.py -i otu_table.biom -o heatmap_sorted_samples.pdf -m mapping_file.txt

Sort the heatmap columns (samples) by the order of samples in the mapping file, and sort the heatmap rows by the order of tips in the tree
 make_otu_heatmap.py -i otu_table.biom -o heatmap_sorted.pdf -m mapping_file.txt -t rep_set.tre

Group the heatmap columns (samples) by metadata category (e.g., Treatment), then cluster within each group
 make_otu_heatmap.py -i otu_table.biom -o heatmap_grouped_by_Treatment.pdf -m mapping_file.txt -c Treatment

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t OTU_TREE, --otu_tree=OTU_TREE
                        Tree file to be used for sorting OTUs in the heatmap
  -m MAP_FNAME, --map_fname=MAP_FNAME
                        Metadata mapping file to be used for sorting Samples
                        in the heatmap.
  -c CATEGORY, --category=CATEGORY
                        Metadata category for sorting samples. Samples will be
                        clustered within each category level using euclidean
                        UPGMA.
  -s SAMPLE_TREE, --sample_tree=SAMPLE_TREE
                        Tree file to be used for sorting samples (e.g, output
                        from upgma_cluster.py). If both this and the sample
                        mapping file are provided, the mapping file is
                        ignored.
  -g IMAGETYPE, --imagetype=IMAGETYPE
                        type of image to produce (i.e. png, svg, pdf)
                        [default: pdf]
  --no_log_transform    Data will not be log-transformed. Without this option,
                        all zeros will be set to a small value (default is 1/2
                        the smallest non-zero entry). Data will be translated
                        to be non-negative after log transform, and
                        num_otu_hits will be set to 0.
  --suppress_row_clustering
                        No UPGMA clustering of OTUs (rows) is performed. If
                        --otu_tree is provided, this flag is ignored.
  --suppress_column_clustering
                        No UPGMA clustering of Samples (columns) is performed.
                        If --map_fname is provided, this flag is ignored.
  --absolute_abundance  Do not normalize samples to sum to 1 [default: False]
  --color_scheme=COLOR_SCHEME
                        color scheme for figure. see http://matplotlib.org/exa
                        mples/color/colormaps_reference.html for choices
                        [default: YlGn]
  --width=WIDTH         width of the figure in inches [default: 5]
  --height=HEIGHT       height of the figure in inches [default: 5]
  --dpi=DPI             resolution of the figure in dots per inch [default:
                        value of savefig.dpi in matplotlibrc file]
  --obs_md_category=OBS_MD_CATEGORY
                        observation metadata category to plot [default:
                        taxonomy]
  --obs_md_level=OBS_MD_LEVEL
                        the level of observation metadata to plot for
                        hierarchical metadata [default: lowest level]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        path to the input OTU table (i.e., the output from
                        make_otu_table.py) [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath [REQUIRED]
make_otu_heatmap_html.py is no longer supported in QIIME. You should instead use make_otu_heatmap.py.
Usage: make_otu_network.py [options] {-i/--input_fp INPUT_FP -m/--map_fname MAP_FNAME -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script generates the otu network files to be passed into cytoscape and statistics for those networks. It uses the OTU fileand the user metadata mapping file.

Network-based analysis is used to display and analyze how OTUs are partitioned between samples. This is a powerful way to display visually large and highly complex datasets in such a way that similarities and differences between samples are emphasized. The visual output of this analysis is a clustering of samples according to their shared OTUs - samples that share more OTUs cluster closer together. The degree to which samples cluster is based on the number of OTUs shared between samples (when OTUs are found in more than one sample) and this is weighted according to the number of sequences within an OTU. In the network diagram, there are two kinds of "nodes" represented, OTU-nodes and sample-nodes. These are shown with symbols such as filled circles and filled squares. If an OTU is found within a sample, the two nodes are connected with a line (an "edge"). (OTUs found only in one sample are given a second, distinct OTU-node shape.) The nodes and edges can then be colored to emphasize certain aspects of the data. For instance, in the initial application of this analysis in a microbial ecology study, the gut bacteria of a variety of mammals was surveyed, and the network diagrams were colored according to the diets of the animals, which highlighted the clustering of hosts by diet category (herbivores, carnivores, omnivores). In a meta-analysis of bacterial surveys across habitat types, the networks were colored in such a way that the phylogenetic classification of the OTUs was highlighted: this revealed the dominance of shared Firmicutes in vertebrate gut samples versus a much higher diversity of phyla represented amongst the OTUs shared by environmental samples.

Not just pretty pictures: the connections within the network are analyzed statistically to provide support for the clustering patterns displayed in the network. A G-test for independence is used to test whether sample-nodes within categories (such as diet group for the animal example used above) are more connected within than a group than expected by chance. Each pair of samples is classified according to whether its members shared at least one OTU, and whether they share a category. Pairs are then tested for independence in these categories (this asks whether pairs that share a category also are equally likely to share an OTU). This statistical test can also provide support for an apparent lack of clustering when it appears that a parameter is not contributing to the clustering.

This OTU-based approach to comparisons between samples provides a counterpoint to the tree-based PCoA graphs derived from the UniFrac analyses. In most studies, the two approaches reveal the same patterns. They can reveal different aspects of the data, however. The network analysis can provide phylogenetic information in a visual manner, whereas PCoA-UniFrac clustering can reveal subclusters that may be obscured in the network. The PCs can be pulled out individually and regressed against other metadata; the network analysis can provide a visual display of shared versus unique OTUs. Thus, together these tools can be used to draw attention to disparate aspects of a dataset, as desired by the author.

In more technical language: OTUs and samples are designated as two types of nodes in a bipartite network in which OTU-nodes are connected via edges to sample-nodes in which their sequences are found. Edge weights are defined as the number of sequences in an OTU. To cluster the OTUs and samples in the network, a stochastic spring-embedded algorithm is used, where nodes act like physical objects that repel each other, and connections act a springs with a spring constant and a resting length: the nodes are organized in a way that minimized forces in the network. These algorithms are implemented in Cytoscape (Shannon et al., 2003).

Example usage: 
Print help message and exit
 make_otu_network.py -h

Example: Create network cytoscape and statistic files in a user-specified output directory. This example uses an OTU table (-i) and the metadata mapping file (-m), and the results are written to the "otu_network/" folder.
 make_otu_network.py -i otu_table.biom -m Fasting_Map.txt -o otu_network

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -b COLORBY, --colorby=COLORBY
                        This is the categories to color by in the plots from
                        the user-generated mapping file. The categories must
                        match the name of a column header in the mapping file
                        exactly and multiple categories can be list by comma
                        separating them without spaces. The user can also
                        combine columns in the mapping file by separating the
                        categories by "&&" without spaces [default=none]
  -p PREFS_PATH, --prefs_path=PREFS_PATH
                        This is the user-generated preferences file. NOTE:
                        This is a file with a dictionary containing
                        preferences for the analysis [default: none]
  -k BACKGROUND_COLOR, --background_color=BACKGROUND_COLOR
                        This is the background color to use in the plots.
                        [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        name of otu table file in biom format [REQUIRED]
    -m MAP_FNAME, --map_fname=MAP_FNAME
                        name of input map file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        output directory for all analyses [REQUIRED]
Usage: make_otu_table.py [options] {-i/--otu_map_fp OTU_MAP_FP -o/--output_biom_fp OUTPUT_BIOM_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The script make_otu_table.py tabulates the number of times an OTU is found in each sample, and adds the taxonomic predictions for each OTU in the last column if a taxonomy file is supplied.

Example usage: 
Print help message and exit
 make_otu_table.py -h

Make OTU table: Make an OTU table from an OTU map (i.e., result from pick_otus.py) and a taxonomy assignment file (i.e., result from assign_taxonomy.py). Write the output file to otu_table.biom.
 make_otu_table.py -i otu_map.txt -t tax_assignments.txt -o otu_table.biom

Make OTU table, excluding OTU ids listed in a fasta file: Make an OTU table, excluding the sequences listed in pynast_failures.fna. Note that the file pass as -e must end with either '.fasta' or '.fna'.
 make_otu_table.py -i otu_map.txt -o otu_table_no_pynast_failures.biom -e pynast_failures.fna

Make OTU table, excluding a list of OTU ids: Make an OTU table, excluding the sequences listed in chimeric_seqs.txt
 make_otu_table.py -i otu_map.txt -o otu_table_non_chimeric.biom -e chimeric_seqs.txt

Make OTU table, passing a mapping file with sample metadata: Make an OTU table from an OTU map (i.e., result from pick_otus.py). Write the output file to otu_table.biom.
 make_otu_table.py -i otu_map.txt -t tax_assignments.txt -o otu_table.biom -m mapping_file.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t TAXONOMY_FNAME, --taxonomy=TAXONOMY_FNAME
                        Path to taxonomy assignment, containing the
                        assignments of taxons to sequences (i.e., resulting
                        txt file from assign_taxonomy.py) [default: none]
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath
  -e EXCLUDE_OTUS_FP, --exclude_otus_fp=EXCLUDE_OTUS_FP
                        path to a file listing OTU identifiers that should not
                        be included in the OTU table (e.g., the output of
                        identify_chimeric_seqs.py) or a fasta file where seq
                        ids should be excluded (e.g., failures fasta file from
                        align_seqs.py)

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_MAP_FP, --otu_map_fp=OTU_MAP_FP
                        path to the input OTU map (i.e., the output from
                        pick_otus.py) [REQUIRED]
    -o OUTPUT_BIOM_FP, --output_biom_fp=OUTPUT_BIOM_FP
                        the output otu table in biom format (recommended
                        extension: .biom) [REQUIRED]
Usage: make_per_library_sff.py [options] {-i/--input_sff INPUT_SFF -l/--libdir LIBDIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script generates per-library sff files using a directory of text files, one per library, which list read ID's to be included.

The ID list files should contain one read ID per line. If a line contains multiple words (separated by whitespace), then only the first word is used. A '>' character is stripped from the beginning of the line, if present. Blank lines in the file are skipped.


Example usage: 
Print help message and exit
 make_per_library_sff.py -h

Example: Make per-library sff files using input.sff and a directory of libs where each file in the directory contains the id lists for each library
 make_per_library_sff.py -i input.sff -l libs

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p SFFFILE_PATH, --sfffile_path=SFFFILE_PATH
                        Path to sfffile binary [default: use sfffile in $PATH]
  --use_sfftools        Use external sfffile program instead of equivalent
                        Python routines.
  --debug               Print debugging output to stdout [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_SFF, --input_sff=INPUT_SFF
                        Input sff file (separate multiple files w/ comma)
                        [REQUIRED]
    -l LIBDIR, --libdir=LIBDIR
                        Directory containing ID list text files, one per
                        library [REQUIRED]
Usage: make_phylogeny.py [options] {-i/--input_fp INPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Many downstream analyses require that the phylogenetic tree relating the OTUs in a study be present. The script make_phylogeny.py produces this tree from a multiple sequence alignment. Trees are constructed with a set of sequences representative of the OTUs, by default using FastTree (Price, Dehal, & Arkin, 2009).

Example usage: 
Print help message and exit
 make_phylogeny.py -h

Examples: A simple example of make_phylogeny.py is shown by the following command, where we use the default tree building method (fasttree) and write the file to the current working directory without a log file
 make_phylogeny.py -i $PWD/aligned.fasta -o $PWD/rep_phylo.tre

Alternatively, if the user would prefer using another tree building method (i.e. clearcut (Sheneman, Evans, & Foster, 2006)), then they could use the following command
 make_phylogeny.py -i $PWD/aligned.fasta -t clearcut

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t TREE_METHOD, --tree_method=TREE_METHOD
                        Method for tree building. Valid choices are: clustalw,
                        raxml_v730, muscle, fasttree, clearcut [default:
                        fasttree]
  -o RESULT_FP, --result_fp=RESULT_FP
                        Path to store result file [default:
                        <input_sequences_filename>.tre]
  -l LOG_FP, --log_fp=LOG_FP
                        Path to store log file [default: No log file created.]
  -r ROOT_METHOD, --root_method=ROOT_METHOD
                        method for choosing root of phylo tree  Valid choices
                        are: midpoint, tree_method_default [default:
                        tree_method_default]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        Path to read input fasta alignment, only first word in
                        defline will be considered [REQUIRED]
Usage: make_prefs_file.py [options] {-m/--map_fname MAP_FNAME -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script generates a preferences (prefs) file, which can be passed to make_2d_plots.py. The prefs file allows for defining the monte_carlo distance, gradient coloring of continuous values in the 2D plots, the ball size scale for all the samples and the color of the arrow and the line of the arrow for the procrustes analysis. Currently there is only one color gradient: red to blue.

Example usage: 
Print help message and exit
 make_prefs_file.py -h

Examples: To make a prefs file, the user is required to pass in a user-generated mapping file using "-m" and an output filepath, using "-o". When using the defaults, the script will use ALL categories from the mapping file, set the background to black and the monte_carlo distances to 10.
 make_prefs_file.py -m mapping.txt -o prefs_out.txt

If the user would like to use specified categories ('SampleID,Individual') or combinations of categories ('SampleID&&Individual'), they will need to use the -b option, where each category is comma delimited, as follows
 make_prefs_file.py -m mapping.txt -b "SampleID,Treatment,SampleID&&Treatment" -o prefs_out_1.txt

If the user would like to change the background color for their plots, they can pass the '-k' option, where the colors: black and white can be used for 3D plots and many additional colors can be used for the 2D plots, such as cyan, pink, yellow, etc.:
 make_prefs_file.py -m mapping.txt -k white -o prefs_out_white.txt

If the user would like to change the monte_carlo distances, they can pass the '-d' option as follows:
 make_prefs_file.py -m mapping.txt -d 15 -o prefs_out_d15.txt

If the user would like to add a list of taxons they can pass the '-i' option, which is the resulting taxa file from summarize_taxa.py, as follows:
 make_prefs_file.py -m mapping.txt -i taxa_level_3.txt -o prefs_out_taxa_l3.txt

If the user would like to add the ball size scale they can pass the '-s' option as follows:
 make_prefs_file.py -m mapping.txt -s 3 -o prefs_out_s3.txt

If the user would like to add the head and line color for the arrows in the procrustes analysis plot they can pass the '-a' and '-l' options as follows:
 make_prefs_file.py -m mapping.txt -a black -l blue -o prefs_out_procrustes.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -b MAPPING_HEADERS_TO_USE, --mapping_headers_to_use=MAPPING_HEADERS_TO_USE
                        mapping fields to use in prefs file [default: ALL]
  -k BACKGROUND_COLOR, --background_color=BACKGROUND_COLOR
                        This is the backgroundcolor to  use in the plots.
                        [default: black]
  -d MONTE_CARLO_DIST, --monte_carlo_dists=MONTE_CARLO_DIST
                        monte carlo distanceto use for each sample header
                        [default: 10]
  -i INPUT_TAXA_FILE, --input_taxa_file=INPUT_TAXA_FILE
                        summarized taxa file with samplecounts by taxonomy
                        (resulting file from summarize_taxa.py)
  -s BALL_SCALE, --ball_scale=BALL_SCALE
                        scale factor for the size of each ball in the plots
                        [default: 1.0]
  -l ARROW_LINE_COLOR, --arrow_line_color=ARROW_LINE_COLOR
                        arrow line color forprocrustes analysis. [default:
                        white]
  -a ARROW_HEAD_COLOR, --arrow_head_color=ARROW_HEAD_COLOR
                        arrow head color forprocrustes analysis. [default:
                        red]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAP_FNAME, --map_fname=MAP_FNAME
                        This is the metadata mapping file [default=none]
                        [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath [REQUIRED]
Usage: make_qiime_py_file.py [options] {-o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This is a script which will add headers and footers to new python files and make them executable.

Example usage: 
Print help message and exit
 make_qiime_py_file.py -h

Example usage: Create a new script
 make_qiime_py_file.py -s -a "Greg Caporaso" -e gregcaporaso@gmail.com -o my_script.py

Create a new test file
 make_qiime_py_file.py -t -a "Greg Caporaso" -e gregcaporaso@gmail.com -o my_test.py

Create a basic file (e.g., for library code)
 make_qiime_py_file.py -a "Greg Caporaso" -e gregcaporaso@gmail.com -o my_lib.py

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s, --script          Pass if creating a script to include option parsing
                        framework [default:False].
  -t, --test            Pass if creating a unit test file to include relevant
                        information [default:False].
  -a AUTHOR_NAME, --author_name=AUTHOR_NAME
                        The script author's (probably you) name to be included
                        the header variables. This will typically need to be
                        enclosed in quotes to handle spaces.
                        [default:AUTHOR_NAME]
  -e AUTHOR_EMAIL, --author_email=AUTHOR_EMAIL
                        The script author's (probably you) e-mail address to
                        be included the header variables.
                        [default:AUTHOR_EMAIL]
  -c COPYRIGHT, --copyright=COPYRIGHT
                        The copyright information to be included in the header
                        variables. [default:Copyright 2014, The QIIME Project]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath [REQUIRED]
Usage: make_rarefaction_plots.py [options] {-i/--input_dir INPUT_DIR -m/--map_fname MAP_FNAME}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Once the batch alpha diversity files have been collated, you may want to compare the diversity using plots. Using the results from collate_alpha.py, you can plot the samples and or by category in the mapping file using this script.

This script creates an html file of rarefaction plots based on the supplied collated alpha-diversity files in a folder or a comma-separated list of files, by passing the "-i" option.  Be aware that this script produces many images for the interactive html pages, so you may choose to not create these pages. The user may also supply optional arguments like an image type (-g), and a resolution (-d).

Example usage: 
Print help message and exit
 make_rarefaction_plots.py -h

Default Example: For generated rarefaction plots using the default parameters, including the mapping file and one rarefaction file, you can use the following command
 make_rarefaction_plots.py -i alpha_div_collated/ -m Fasting_Map.txt

Specify Image Type and Resolution: Optionally, you can change the resolution ('-d') and the type of image created ('-i'), by using the following command
 make_rarefaction_plots.py -i alpha_div_collated/ -m Fasting_Map.txt -d 180 -g pdf

Use Prefs File: You can also supply a preferences file '-p', as follows
 make_rarefaction_plots.py -i alpha_div_collated/ -m Fasting_Map.txt -d 180 -p prefs.txt

Set Background Color: Alternatively, you can set the plot background '-k'
 make_rarefaction_plots.py -i alpha_div_collated/ -m Fasting_Map.txt -k black

Generate raw data without interactive webpages: The user can choose to not create an interactive webpage ('-w' option). This is for the case, where the user just wants the average plots and the raw average data.
 make_rarefaction_plots.py -i alpha_div_collated/ -m Fasting_Map.txt -w

Generate average tables and per-sample plots: Pass --generate_average_tables and --generate_per_sample_plots to generate average tables of results as tab-separated text and per-sample plots for each of the metadata categories
 make_rarefaction_plots.py -i alpha_div_collated/ -m Fasting_Map.txt --generate_average_tables --generate_per_sample_plots

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -b COLORBY, --colorby=COLORBY
                        Comma-separated list categories metadata categories
                        (column headers) to color by in the plots. The
                        categories must match the name of a column header in
                        the mapping file exactly. Multiple categories can be
                        list by comma separating them without spaces. The user
                        can also combine columns in the mapping file by
                        separating the categories by "&&" without spaces.
                        [default=color by all]
  -p PREFS_PATH, --prefs_path=PREFS_PATH
                        Input user-generated preferences filepath. NOTE: This
                        is a file with a dictionary containing preferences for
                        the analysis. [default: none]
  -k BACKGROUND_COLOR, --background_color=BACKGROUND_COLOR
                        Background color to use in the plots[default: white]
  -g IMAGETYPE, --imagetype=IMAGETYPE
                        Type of image to produce (i.e. png, svg, pdf).
                        WARNING: Some formats may not properly open in your
                        browser! [default: png]
  -d RESOLUTION, --resolution=RESOLUTION
                        Resolution of the plot. [default: 75]
  -y YMAX, --ymax=YMAX  Maximum y-value to be used for the plots. Allows for
                        directly comparable rarefaction plots between analyses
                        [default: none]
  -w, --webpage         DEPRECATED: Suppress HTML output. [default: True]
  -s, --suppress_html_output
                        Suppress HTML output. [default: False]
  -e STD_TYPE, --std_type=STD_TYPE
                        Calculation to perform for generating error bars.
                        Options are standard deviation (stddev) or standard
                        error (stderr). [default: stddev]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory
  --output_type=OUTPUT_TYPE
                        Write the HTML output as one file, images embedded, or
                        several. Options are "file_creation" and "memory".
                        [default: file_creation]
  --generate_per_sample_plots
                        generate per sample plots for each of the metadata
                        categories. This will allow you to show/hide samples
                        from the plots but will require a longer processing
                        time. In general, this option is useful only for small
                        datasets. [default: False]
  --generate_average_tables
                        generate average tables of results. A summary of the
                        metrics and alpha diversity measurements. [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        Input directory containing results from
                        collate_alpha.py. [REQUIRED]
    -m MAP_FNAME, --map_fname=MAP_FNAME
                        Input metadata mapping filepath. [REQUIRED]
Usage: make_tep.py [options] {-i/--otu_table_fp OTU_TABLE_FP -m/--mapping_fp MAPPING_FP -t/--tree_fp TREE_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script makes a TopiaryExplorer project file (.tep) and a jnlp file with the data location preloaded.

WARNING: The jnlp file relies on an absolute path, if you move the .tep file, the generated jnlp will no longer work. However, you can still open the .tep file from your normal TopiaryExplorer install.

Example usage: 
Print help message and exit
 make_tep.py -h

Example: Create .tep file and .jnlp file
 make_tep.py -i otu_table.biom -m Fasting_Map.txt -t rep_set.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory
  -p PREFS_FILE_FP, --prefs_file_fp=PREFS_FILE_FP
                        path to prefs file
  -w, --web_flag        web codebase jnlp flag [default: False]
  -u URL, --url=URL     url path for the tep file. Note: when passing this
                        flag, it will overwrite the supplied OTU table,
                        Mapping and Tree files.

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        path to the input OTU table (i.e., the output from
                        make_otu_table.py) [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath [REQUIRED]
    -t TREE_FP, --tree_fp=TREE_FP
                        path to tree [REQUIRED]
Usage: map_reads_to_reference.py [options] {-i/--input_seqs_filepath INPUT_SEQS_FILEPATH -r/--refseqs_fp REFSEQS_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

 

Example usage: 
Print help message and exit
 map_reads_to_reference.py -h

Run assignment with usearch using default parameters
 map_reads_to_reference.py -i query_nt.fasta -r refseqs_pr.fasta

Run nucleotide versus protein BLAT using default parameters
 map_reads_to_reference.py -i query_nt.fasta -r refseqs_pr.fasta -m blat

Run nucleotide versus protein BLAT using scricter e-value threshold
 map_reads_to_reference.py -i query_nt.fasta -r refseqs_pr.fasta -o blat_mapped_strict/ -e 1e-70  -m blat

Run nucleotide versus nucleotide BLAT with default parameters
 map_reads_to_reference.py -i query_nt.fasta -r refseqs_nt.fasta -m blat-nt

Run assignment with bwa-short using default parameters. bwa-short is intended to be used for reads up to 200bp. WARNING: reference sequences must be dereplicated! No matches will be found to reference sequences which show up multiple times (even if their sequence identifiers are different)!
 map_reads_to_reference.py -i query_nt.fasta -r refseqs_nt.fasta -m bwa-short

Run assignment with bwa-sw using default parameters.  WARNING: reference sequences must be dereplicated! No matches will be found to reference sequences which show up multiple times (even if their sequence identifiers are different)!
 map_reads_to_reference.py -i query_nt.fasta -r refseqs_nt.fasta -m bwa-sw

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m ASSIGNMENT_METHOD, --assignment_method=ASSIGNMENT_METHOD
                        Method for picking OTUs.  Valid choices are: bwa-
                        short, usearch, bwa-sw, blat, blat-nt. [default:
                        usearch]
  -t OBSERVATION_METADATA_FP, --observation_metadata_fp=OBSERVATION_METADATA_FP
                        Path to observation metadata (e.g., taxonomy, EC, etc)
                        [default: none]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Path to store result file [default:
                        ./<METHOD>_mapped/]
  -e EVALUE, --evalue=EVALUE
                        Max e-value to consider a match [default: 1e-10]
  -s MIN_PERCENT_ID, --min_percent_id=MIN_PERCENT_ID
                        Min percent id to consider a match, expressed as a
                        fraction between 0 and 1 [default: 0.75]
  --genetic_code=GENETIC_CODE
                        ID of genetic code to use for DNA translations (please
                        see http://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintg
                        c.cgi) Only valid with -m blat. [default: 11]
  --max_diff=MAX_DIFF   maxDiff to consider a match (applicable for -m bwa-
                        short) -- see the aln section of "man bwa" for details
                        [default (defined by bwa): 0.04]
  --queryalnfract=QUERYALNFRACT
                        Min percent of the query seq that must match to
                        consider a match, expressed as a fraction between 0
                        and 1 (usearch only) [default: 0.35]
  --targetalnfract=TARGETALNFRACT
                        Min percent of the target/reference seq that must
                        match to consider a match, expressed as a fraction
                        between 0 and 1 (usearch only) [default: 0.0]
  --max_accepts=MAX_ACCEPTS
                        max_accepts value (usearch only) [default: 1]
  --max_rejects=MAX_REJECTS
                        max_rejects value to (usearch only) [default: 32]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_SEQS_FILEPATH, --input_seqs_filepath=INPUT_SEQS_FILEPATH
                        Path to input sequences file [REQUIRED]
    -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        Path to reference sequences to search against
                        [default: none] [REQUIRED]
Usage: merge_mapping_files.py [options] {-m/--mapping_fps MAPPING_FPS -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script provides a convenient interface for merging mapping files which contain data on different samples.

Example usage: 
Print help message and exit
 merge_mapping_files.py -h

Example: Merge two mapping files into a new mapping file (merged_mapping.txt). In cases where a mapping field is not provided for some samples, add the value 'Data not collected'.
 merge_mapping_files.py -m map_controls.txt,map_fasting.txt -o merged_mapping.txt -n 'Data not collected'

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NO_DATA_VALUE, --no_data_value=NO_DATA_VALUE
                        value to represent missing data (i.e., when all fields
                        are not defined in all mapping files) [default:
                        no_data]
  --case_insensitive    if present the headers will be merged case
                        insensitivly and transformed to upper case [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FPS, --mapping_fps=MAPPING_FPS
                        the input mapping files in a comma-separated list
                        [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output mapping file to write [REQUIRED]
Usage: merge_otu_maps.py [options] {-i/--otu_map_fps OTU_MAP_FPS -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script merges OTU mapping files generated by denoise_wrapper.py and/or pick_otus.py

For example, if otu_map1.txt contains:

=   ====    ====    ====
0   seq1    seq2    seq5
1   seq3    seq4
2   seq6    seq7    seq8
=   ====    ====    ====

and otu_map2.txt contains:

=== =   =
110 0   2
221 1
=== =   =

The resulting OTU map will be:

=== ====    ====    ====    ====    ====    ====
110 seq1    seq2    seq5    seq6    seq7    seq8
221 seq3    seq4
=== ====    ====    ====    ====    ====    ====


Example usage: 
Print help message and exit
 merge_otu_maps.py -h

Expand an OTU map: If the seq_ids in otu_map2.txt are otu_ids in otu_map1.txt, expand the seq_ids in otu_map2.txt to be the full list of associated seq_ids from otu_map1.txt. Write the resulting otu map to otu_map.txt (-o).
 merge_otu_maps.py -i $PWD/otu_map1.txt,$PWD/otu_map2.txt -o $PWD/otu_map_ex1.txt

Expand a failures file: Some OTU pickers (e.g. uclust_ref) will generate a list of failures for sequences which could not be assigned to OTUs. If this occurs in a chained OTU picking process, the failures file will need to be expanded to include the orignal sequence ids. To do this, pass the failures file via -f, and the otu maps up to, but not including, the step that generated the failures file.
 merge_otu_maps.py -i $PWD/otu_map1.txt,$PWD/otu_map2.txt -f $PWD/fail.txt -o $PWD/all_failures.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -f FAILURES_FP, --failures_fp=FAILURES_FP
                        failures filepath, if applicable

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_MAP_FPS, --otu_map_fps=OTU_MAP_FPS
                        the otu map filepaths, comma-separated and ordered as
                        the OTU pickers were run [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        path to write output OTU map [REQUIRED]
Usage: merge_otu_tables.py [options] {-i/--input_fps INPUT_FPS -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script merges two or more OTU tables into a single OTU table. This is useful, for example, when you've created several reference-based OTU tables for different analyses and need to combine them for a larger analysis.

Requirements: It is also very important that your OTUs are consistent across the different OTU tables. For example, you cannot safely merge OTU tables from two independent de novo OTU picking runs. Finally, either all or none of the OTU tables can contain taxonomic information: you can't merge some OTU tables with taxonomic data and some without taxonomic data.

Example usage: 
Print help message and exit
 merge_otu_tables.py -h

Merge two OTU tables into a single OTU table
 merge_otu_tables.py -i otu_table1.biom,otu_table2.biom -o merged_otu_table.biom

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FPS, --input_fps=INPUT_FPS
                        the otu tables in biom format (comma-separated)
                        [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output otu table filepath [REQUIRED]
Usage: multiple_extract_barcodes.py [options] {-i/--input_dir INPUT_DIR -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script runs extract_barcodes.py on data that are already demultiplexed
(split up according to sample, with one sample per file). The script
supports the following types of input:

- a directory containing many files, where each file is named on a per-sample
  basis
- a directory containing many directories, where each directory is named on a
  per-sample basis
 
The script assumes that the leading/trailing characters before/after the read
number indicator (see --read1_indicator) are matched between forward and
reverse reads. For example:

- S0_L001_R1_001.fastq.gz and S0_L001_R2_001.fastq.gz would be matched up reads
- S0_L002_R1_00X.fastq.gz and S0_L002_R2_00X.fastq.gz would be matched up reads

The output directory used for each call to extract_barcodes.py uses the base
name of the input read 1 fastq file (a single directory would be problematic
since the output names for extract_barcodes.py can be the same for different
calls). Use the parameter --include_input_dir_path to also include the input
directory name in the output directory path, which may be preferable in the
case of an input folder of folders, and --remove_filepath_in_name can be used
in this case to prevent the input read 1 fastq file base name from being used
as part of the output directory name.



Example usage: 
Print help message and exit
 multiple_extract_barcodes.py -h

Example 1: Process an input folder of files, with default options used for extract_barcodes.py
 multiple_extract_barcodes.py -i input_files -o output_folder

Example 2: Process an input folder of folders (with the filenames having _forward_ and _reverse_ containing the forward and reverse read filenames, respectively), using the extract_barcodes.py option for paired reads. The individual folder names are included in the output folder names, but not the filenames. Note: it is important to pass the --paired_data option if paired data are to be used in the extract_barcodes.py commands. Additionally, the paired fastq file type for extract_barcodes.py is specified with a qiime_parameters.txt file (with this value specified: extract_barcodes:input_type barcode_paired_end)
 multiple_extract_barcodes.py -i input_folders -o output_folder -p qiime_parameters.txt --paired_data --read1_indicator '_forward_' --read2_indicator '_reverse_' --include_input_dir_path --remove_filepath_in_name

Example 3: To see what commands would be executed by the script without actually running them, use the following command
 multiple_extract_barcodes.py -i input_files -o output_folder -w

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior of extract_barcodes.py. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters [default: extract_barcodes.py
                        defaults will be used]
  --paired_data         Turn this option on if paired data are to be used. The
                        type of paired data for extract_barcodes.py should be
                        specified with -p. Forward and reverse reads will be
                        searched for via the --read1_indicator and
                        --read2_indicator parameters [default: False]
  --read1_indicator=READ1_INDICATOR
                        Substring to search for to indicate read 1 [default:
                        _R1_]
  --read2_indicator=READ2_INDICATOR
                        Substring to search for to indicate read 2 [default:
                        _R2_]
  --leading_text=LEADING_TEXT
                        Leading text to add to each extract_barcodes.py
                        command [default: no leading text added]
  --trailing_text=TRAILING_TEXT
                        Trailing text to add to each extract_barcodes.py
                        command [default: no trailing text added]
  --include_input_dir_path
                        Include the input directory name in the output
                        directory path. Useful in cases where the file names
                        are repeated in input folders [default: False]
  --remove_filepath_in_name
                        Disable inclusion of the input filename in the output
                        directory names. Must use --include_input_dir_path if
                        this option is enabled [default: False]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        Input directory of directories, or directory of paired
                        fastq files. [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Base output directory to write output folders
                        [REQUIRED]
Usage: multiple_join_paired_ends.py [options] {-i/--input_dir INPUT_DIR -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script runs join_paired_ends.py on data that are already demultiplexed
(split up according to sample, with one sample per pair of files). The script
supports the following types of input:

- a directory containing many files, where each file is named on a per-sample
  basis
- a directory containing many directories, where each directory is named on a
  per-sample basis
 
The script assumes that the leading/trailing characters before/after the read
number indicator (see --read1_indicator) are matched between forward and
reverse reads. For example:

- S0_L001_R1_001.fastq.gz and S0_L001_R2_001.fastq.gz would be matched up reads
- S0_L002_R1_00X.fastq.gz and S0_L002_R2_00X.fastq.gz would be matched up reads

If an optional --barcode_indicator file is used, it is searched for in the same
manner that the paired files are searched for, so if the default "_I1_" is
used, S0_L001_R1_001.fastq.gz and S0_L001_R2_001.fastq.gz would be matched up
with S0_L001_I1_001.fastq.gz as the barcode indicator file.

The output directory used for each call to join_paired_ends.py uses the base
name of the input read 1 fastq file (a single directory would be problematic
since the output names for join_paired_ends.py can be the same for different
calls). Use the parameter --include_input_dir_path to also include the input
directory name in the output directory path, which may be preferable in the
case of an input folder of folders, and --remove_filepath_in_name can be used
in this case to prevent the input read 1 fastq file base name from being used
as part of the output directory name.



Example usage: 
Print help message and exit
 multiple_join_paired_ends.py -h

Example 1: Process an input folder of paired-up files (by filename, with the default _R1_ and _R2_ containing the forward and reverse reads filenames, respectively). An optional parameters file is passed with -p. This file can specify an optional parameter for join_paired_ends.py, such as: join_paired_ends:pe_join_method SeqPrep
 multiple_join_paired_ends.py -i input_files -o output_folder -p qiime_parameters.txt

Example 2: Process an input folder of folders (with the filenames having _forward_ and _reverse_ containing the forward and reverse read filenames, respectively). The individual folder names are included in the output folder names, but not the filenames. A matching barcode fastq file (indicated by _barcode_) is also included.
 multiple_join_paired_ends.py -i input_folders -o output_folder --read1_indicator '_forward_' --read2_indicator '_reverse_' --include_input_dir_path --remove_filepath_in_name -b --barcode_indicator '_barcode_'

Example 3: To see what commands would be executed by the script without actually running them, use the following command
 multiple_join_paired_ends.py -i input_files -o output_folder -w

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior of join_paired_ends.py. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters [default: join_paired_ends.py
                        defaults will be used]
  --read1_indicator=READ1_INDICATOR
                        Substring to search for to indicate read 1 [default:
                        _R1_]
  --read2_indicator=READ2_INDICATOR
                        Substring to search for to indicate read 2 [default:
                        _R2_]
  -b, --match_barcodes  Enable searching for matching barcodes [default:
                        False]
  --barcode_indicator=BARCODE_INDICATOR
                        Substring to search for to indicate barcode reads
                        [default: _I1_]
  --leading_text=LEADING_TEXT
                        Leading text to add to each join_paired_ends.py
                        command [default: no leading text added]
  --trailing_text=TRAILING_TEXT
                        Trailing text to add to each join_paired_ends.py
                        command [default: no trailing text added]
  --include_input_dir_path
                        Include the input directory name in the output
                        directory path. Useful in cases where the file names
                        are repeated in input folders [default: False]
  --remove_filepath_in_name
                        Disable inclusion of the input filename in the output
                        directory names. Must use --include_input_dir_path if
                        this option is enabled [default: False]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        Input directory of directories, or directory of paired
                        fastq files. [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Base output directory to write output folders
                        [REQUIRED]
Usage: multiple_rarefactions.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH -m/--min MIN -x/--max MAX -s/--step STEP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

To perform bootstrap, jackknife, and rarefaction analyses, the otu table must be subsampled (rarefied).  This script rarefies, or subsamples, OTU tables.  This does not provide curves of diversity by number of sequences in a sample.  Rather it creates a series of subsampled OTU tables by random sampling (without replacement) of the input OTU table.  Samples that have fewer sequences then the requested rarefaction depth for a given output otu table are omitted from those ouput otu tables.  The pseudo-random number generator used for rarefaction by subsampling is NumPy's default - an implementation of the Mersenne twister PRNG.

Example usage: 
Print help message and exit
 multiple_rarefactions.py -h

Generate rarefied OTU tables: Generate rarefied OTU tables beginning with 10 (-m) sequences/sample through 140 (-x) sequences per sample in steps of of 10 (-s), performing 2 iterations at each sampling depth (-n). All resulting OTU tables will be written to 'rarefied_otu_tables' (-o). Any sample containing fewer sequences in the input file than the requested number of sequences per sample is removed from the output rarefied otu table.
 multiple_rarefactions.py -i otu_table.biom -m 10 -x 140 -s 10 -n 2 -o rarefied_otu_tables/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NUM_REPS, --num_reps=NUM_REPS
                        The number of iterations at each step. [default: 10]
  --lineages_included   Retain taxonomic (lineage) information for each OTU.
                        Note: this will only work if lineage information is in
                        the input OTU table. [default: False]
  -k, --keep_empty_otus
                        Retain OTUs of all zeros, which are usually omitted
                        from the output OTU tables. [default: False]
  --subsample_multinomial
                        subsample using subsampling with replacement [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        Input OTU table filepath. [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        Output directory. [REQUIRED]
    -m MIN, --min=MIN   Minimum number of seqs/sample for rarefaction.
                        [REQUIRED]
    -x MAX, --max=MAX   Maximum number of seqs/sample (inclusive) for
                        rarefaction.  [REQUIRED]
    -s STEP, --step=STEP
                        Size of each steps between the min/max of seqs/sample
                        (e.g. min, min+step... for level <= max). [REQUIRED]
Usage: multiple_rarefactions_even_depth.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH -d/--depth DEPTH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

To perform bootstrap, jackknife, and rarefaction analyses, the otu table must be subsampled (rarefied).  This script rarefies, or subsamples, an OTU table.  This does not provide curves of diversity by number of sequences in a sample.  Rather it creates a subsampled OTU table by random sampling (without replacement) of the input OTU table.  Samples that have fewer sequences then the requested rarefaction depth are omitted from the ouput otu tables.  The pseudo-random number generator used for rarefaction by subsampling is NumPy's default - an implementation of the Mersenne twister PRNG.

Example usage: 
Print help message and exit
 multiple_rarefactions_even_depth.py -h

Example: subsample otu_table.biom at 100 seqs/sample (-d) 10 times (-n) and write results to files (e.g., rarefaction_400_0.biom) in 'rarefied_otu_tables/' (-o).
 multiple_rarefactions_even_depth.py -i otu_table.biom -o rarefied_otu_tables/ -d 100 -n 10

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NUM_REPS, --num_reps=NUM_REPS
                        num iterations at each seqs/sample level [default: 10]
  --lineages_included   output rarefied otu tables will include taxonomic
                        (lineage) information for each otu, if present in
                        input otu table [default: False]
  -k, --keep_empty_otus
                        otus (rows) of all zeros are usually omitted from the
                        output otu tables, with -k they will not be removed
                        from the output files [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input otu table filepath [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        write output rarefied otu tables files to this dir
                        (makes dir if it doesn't exist) [REQUIRED]
    -d DEPTH, --depth=DEPTH
                        sequences per sample to subsample [REQUIRED]
Usage: multiple_split_libraries_fastq.py [options] {-i/--input_dir INPUT_DIR -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script runs split_libraries_fastq.py on data that are already demultiplexed
(split up according to sample, with one sample per file). The script supports
the following types of input:

- a directory containing many files, where each file is named on a per-sample
  basis (with different prefixes before the read number)
- a directory containing many directories, where each directory is named on a
  per-sample basis

This script assumes that the leading characters before the read indicator
(see --read_indicator) are matched between the read, barcode, and mapping files.
For example, sample1_L001_R1_001.fastq.gz, sample1_L001_I1_001.fastq.gz,
sample1_L001_mapping_001.txt would be matched up if "R1" is the read indicator,
"I1" is the barcode indicator, and "mapping" is the mapping file indicator.



Example usage: 
Print help message and exit
 multiple_split_libraries_fastq.py -h

Example 1: Process an input folder of folders, with options specified to pair up reads, barcodes, and mapping files. A qiime_parameters.txt file is included to use the parameter split_libraries_fastq:barcode_type 12
 multiple_split_libraries_fastq.py -i input_folders -o output_folder --demultiplexing_method mapping_barcode_files --read_indicator reads --barcode_indicator barcode --mapping_indicator mapping -p qiime_parameters.txt

Example 2: Process an input folder of files, with the option specified to generate sample ids using the filenames (default behavior is to use all text before the first underscore as the sample id)
 multiple_split_libraries_fastq.py -i input_files -o output_folder --demultiplexing_method sampleid_by_file

Example 3: Process an input folder of folders, with an option specified to use the folder names as the sample ids. In this case, the fastq filenames themselves are not included, only the folder names are used.
 multiple_split_libraries_fastq.py -i input_folders_no_barcodes -o output_folder --demultiplexing_method sampleid_by_file --include_input_dir_path --remove_filepath_in_name

Example 4: To see what commands would be executed by the script without actually running them, use the following command
 multiple_split_libraries_fastq.py -i input_files -o output_folder -w

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -m DEMULTIPLEXING_METHOD, --demultiplexing_method=DEMULTIPLEXING_METHOD
                        Method for demultiplexing. Can either be
                        "sampleid_by_file" or "mapping_barcode_files". With
                        the sampleid_by_file option, each fastq file (and/or
                        directory name) will be used to generate the
                        --sample_ids value passed to split_libraries_fastq.py.
                        The mapping_barcode_files option will search for
                        barcodes and mapping files that match the input read
                        files [default: sampleid_by_file]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior of split_libraries_fastq.py. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters [default: split_libraries_fastq.py
                        defaults will be used]
  --read_indicator=READ_INDICATOR
                        Substring to search for to indicate read files
                        [default: _R1_]
  --barcode_indicator=BARCODE_INDICATOR
                        Substring to search for to indicate barcode files
                        [default: _I1_]
  --mapping_indicator=MAPPING_INDICATOR
                        Substring to search for to indicate mapping files
                        [default: _mapping_]
  --mapping_extensions=MAPPING_EXTENSIONS
                        Comma-separated list of file extensions used to
                        identify mapping files. Only applies when
                        --demultiplexing_method is "mapping_barcode_files"
                        [default: txt,tsv]
  --sampleid_indicator=SAMPLEID_INDICATOR
                        Text in fastq filename before this value will be used
                        as output sample ids [default: _]
  --include_input_dir_path
                        Include the input directory name in the output sample
                        id name. Useful in cases where the file names are
                        repeated in input folders [default: False]
  --remove_filepath_in_name
                        Disable inclusion of the input filename in the output
                        sample id names. Must use --include_input_dir_path if
                        this option is enabled [default: False]
  --leading_text=LEADING_TEXT
                        Leading text to add to each split_libraries_fastq.py
                        command [default: no leading text added]
  --trailing_text=TRAILING_TEXT
                        Trailing text to add to each split_libraries_fastq.py
                        command [default: no trailing text added]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        Input directory of directories or fastq files.
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Output directory to write split_libraries_fastq.py
                        results [REQUIRED]
Usage: neighbor_joining.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The input to this step is a distance matrix (i.e. resulting file from beta_diversity.py).

Example usage: 
Print help message and exit
 neighbor_joining.py -h

neighbor joining (nj) cluster (Single File): To perform nj clustering on a single distance matrix (e.g.: beta_div.txt, a result file from beta_diversity.py) use the following idiom
 neighbor_joining.py -i beta_div.txt -o beta_div_cluster.tre

neighbor joining (Multiple Files): The script also functions in batch mode if a folder is supplied as input. This script operates on every file in the input directory and creates a corresponding neighbor joining tree file in the output directory, e.g.
 neighbor_joining.py -i beta_div_weighted_unifrac/ -o beta_div_weighted_clusters/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input path.  directory for batch processing, filename
                        for single file operation [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path. directory for batch processing, filename
                        for single file operation [REQUIRED]
Usage: nmds.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Nonmetric Multidimensional Scaling (NMDS) is commonly used to compare groups of samples based on phylogenetic or count-based distance metrics (see section on beta_diversity.py).

Example usage: 
Print help message and exit
 nmds.py -h

NMDS (Single File): For this script, the user supplies a distance matrix (i.e. resulting file from beta_diversity.py), along with the output filename (e.g. beta_div_coords.txt), as follows
 nmds.py -i beta_div.txt -o beta_div_coords.txt

NMDS (Dimensions): For this script, the user supplies a distance matrix (i.e. resulting file from beta_diversity.py), the number of dimensions of NMDS space and the output filename (e.g. beta_div_coords.txt), as follows
 nmds.py -i beta_div.txt -d 3 -o beta_div_3_coords.txt

NMDS (Multiple Files): The script also functions in batch mode if a folder is supplied as input (e.g. from beta_diversity.py run in batch). No other files should be present in the input folder - only the distance matrix files to be analyzed. This script operates on every distance matrix file in the input directory and creates a corresponding nmds results file in the output directory, e.g.
 nmds.py -i beta_div_weighted_unifrac/ -o beta_div_weighted_nmds_results/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -d DIMENSIONS, --dimensions=DIMENSIONS
                        number of dimensions of NMDS spacedefault: 3

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        path to the input distance matrix file(s) (i.e., the
                        output from beta_diversity.py). Is a directory for
                        batch processing and a filename for a single file
                        operation. [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path. directory for batch processing, filename
                        for single file operation [REQUIRED]
Usage: normalize_table.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

To perform many downstream analyses after OTU picking (besides
metagenomeSeq's fitZIG and DESeq OTU differential abundance testing), the OTU
matrix must be normalized to account for uneven column (sample) sums that are a
result of most modern sequencing techniques.  These methods attempt to correct
for compositionality too.  Rarefying throws away some data by rarefying to a
constant sum and throwing away extremely low depth samples.

Even with these new normalization techniques, we would recommend throwing away
low depth samples (e.g. less that 1000 sequences/sample).  DESeq/DESeq2 outputs
negative values for lower abundant OTUs as a result of its log transformation.
For most ecologically useful metrics (e.g. UniFrac/Bray Curtis) this presents
problems. No good solution exists at the moment for this issue.  Note that one
is added to the matrix to avoid log(0).  It has been shown that clustering
results can be highly dependent upon the choice of the pseudocount (e.g. should
it be 0.01 instead of 1?), for more information read Costea, P. et al. (2014)
"A fair comparison", Nature Methods.

DESeq/DESeq2 can also have a very slow runtime, especially for larger datasets.
In this script, we implement DESeq2's variance stabilization technique. If you do use these
alternatives to rarefying, we would recommend metagenomeSeq's CSS (cumulative sum
scaling) transformation for those metrics that are abundance-based.  It is not
recommended to use these new methods with presence/absence metrics, for example
binary Jaccard or unweighted UniFrac.

For more on metagenomeSeq's CSS, please see Paulson, JN, et al. 'Differential
abundance analysis for microbial marker-gene surveys' Nature Methods 2013.  For DESeq
please see Anders S, Huber W. 'Differential expression analysis for sequence
count data.' Genome Biology 2010.  For DESeq2 please read Love, MI et al.
'Moderated estimation of fold change and dispersion for RNA-Seq data
with DESeq2,' Genome Biology 2014.  If you use these methods, please CITE the
appropriate reference as well as QIIME.  For any of these methods, clustering by
sequence depth MUST BE CHECKED FOR as a confounding variable, e.g. by coloring
by sequences/sample on a PCoA plot and testing for correlations between
taxa abundances and sequencing depth with e.g. adonis in compare_categories.py,
or observation_metadata_correlation.py.

Note: If the input BIOM table contains observation metadata (e.g., taxonomy
metadata for each OTU), this metadata will not be included in the output
normalized BIOM table when using DESeq2. When using CSS the taxonomy metadata
will be included in the output normalized table but it may not be in the same
format as the input table (e.g., "NA" will be added for missing taxonomic
levels). This discrepancy occurs because the underlying R packages used to
perform the normalization store taxonomy metadata in a different format.

As a workaround, the "biom add-metadata" command can be used to add the
original observation metadata to the output normalized table if desired. For
example, to include the original taxonomy metadata on the output normalized
table, "biom add-metadata" can be used with the representative sequence
taxonomic assignment file output by assign_taxonomy.py.



Example usage: 
Print help message and exit
 normalize_table.py -h

Single File CSS Matrix Normalization: Normalize a raw (non-normalized/non-rarefied) otu_table.biom using CSS
 normalize_table.py -i otu_table.biom -a CSS -o CSS_normalized_otu_table.biom

Single File DESeq2 Matrix Normalization: Normalize a raw (non-normalized/non-rarefied) otu_table.biom using DESeq2
 normalize_table.py -i otu_table.biom -a DESeq2 -o DESeq2_normalized_otu_table.biom

Multiple File Matrix Normalization: Normalize a folder of raw (non-normalized/non-rarefied) otu tables using e.g. DESeq2
 normalize_table.py -i otu_tables/ -a DESeq2 -o normalized_tables/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -i INPUT_PATH, --input_path=INPUT_PATH
                        path to the input BIOM file (e.g., the output from OTU
                        picking) or directory containing input BIOM files for
                        batch processing [REQUIRED if not passing -l]
  -o OUT_PATH, --out_path=OUT_PATH
                        output filename for single file operation, or output
                        directory for batch processing [REQUIRED if not
                        passing -l]
  -s, --output_CSS_statistics
                        output CSS statistics file. This will be a directory
                        for batch processing, and a filename for single file
                        operation [default: False]
  -z, --DESeq_negatives_to_zero
                        replace negative numbers produced by the DESeq
                        normalization technique with zeros [default: False]
  -a ALGORITHM, --algorithm=ALGORITHM
                        normalization algorithm to apply to input BIOM
                        table(s). [default: CSS] Available options are: CSS,
                        DESeq2
  -l, --list_algorithms
                        show available normalization algorithms and exit
                        [default: False]
Usage: observation_metadata_correlation.py [options] {-i/--otu_table_fp OTU_TABLE_FP -o/--output_fp OUTPUT_FP -m/--mapping_fp MAPPING_FP -c/--category CATEGORY}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script calculates correlations between feature (aka observation) abundances
(relative or absolute) and numeric metadata. Several methods are provided to
allow the user to correlate features to sample metadata values including
Spearman's Rho, Pearson, Kendall's Tau, and the C or checkerboard score.
References for these methods are numerous, but good descriptions may be found in
'Biometry' by Sokal and Rolhf. A brief description of the available tests
follows:

- Pearson score: The Pearson score, aka Pearson's Product Moment correlation, is
  a scaled measure of the degree to which two sequences of numbers co-vary. For
  'correlated' sequences, Pearson > 0, and for 'anticorrelated' sequences
  Pearson < 0 (uncorrelated implies Pearson = 0). Pearson is a paramateric
  and linear measure of correlation.

- Spearman's Rho: The Spearman correlation is a non-paramateric measure of
  correlation between two sequences of numbers. Spearman correlation is
  appropriate for data where the values of the observations are not necessarily
  accurate, but for which their relative magnitudes are (see Biometry for more
  details).

- Kendall's Tau: Kendall's Tau is an alternative method of calculating
  correlation between two sequences of numbers. It is slower and less widely
  utilized than Spearman or Pearson scores.

- Cscore: The c-score or 'checkerboard score' is a measure of covariation
  between two sequences that is derived from traditional ecology (Stone and
  Roberts. 1990, Oecologia 85:74-79).

Raw correlation statistics alone reflect only the degree of association between
two sequences of numbers or vectors. Assigning a likelihood to these score via
a p-value can be done with several methods depending on the particular
assumptions that are used. This script allows four methods for calculating
p-values:

- Bootrapping: Bootstrapping is the most robust, but slowest procedure for
  calculating the p-value of a given correlation score. Bootstrapping takes the
  input sequences, shuffles the order of one, and then recomputes the
  correlation score. The p-value is then the number of times (out of the given
  number of permutations) that the score of the permuted sequence pair was more
  extreme than the observed pair. Bootstrapping is good when the underlying
  properties of the distributions are unknown.

- Parametric t distribution: the traditional method for calculating the
  significance of a correlation score, this method assumes that the scores are
  normally distributed and computes a t statistic for each correlation score in
  conjunction with the length of the sequences being correlated.

- Fisher Z transform: Fisher's Z transform is a way to make the distribution of
  correlation scores (especially when there are many highly correlated scores)
  look more normal. It is not to be confused with Fisher's transformation for
  the combination of p-values.

- Kendall's Tau: for the Kendall's Tau calculation, the specific Kendall's Tau
  p-value is provided.

Notes:

- The only supported metric for p-value assignment with the C-score is
  bootstrapping. For more information on the C-score, read Stone and Roberts
  1990 Oecologea paper 85: 74-79. If you don't pass
  pval_assignment_method='bootstrapped' while you have -s cscore, the script
  will raise an error.

- Assigning p-values to Kendall's Tau scores with the bootstrapping method is
  very slow.



Example usage: 
Print help message and exit
 observation_metadata_correlation.py -h

Example 1: Calculate the correlation between OTUs in the table and the pH of the samples from whence they came
 observation_metadata_correlation.py -i otu_table.biom -m map.txt -c pH -s spearman -o spearman_otu_gradient.txt

Example 2: Calculate the correlation between OTUs in the table and the pH of the samples from whence they came using bootstrapping and pearson correlation
 observation_metadata_correlation.py -i otu_table.biom -m map.txt -c pH -s pearson --pval_assignment_method bootstrapped --permutations 100 -o pearson_bootstrapped.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s TEST, --test=TEST  Correlation method to use. Choices are: spearman,
                        pearson, kendall, cscore [default: spearman]
  --pval_assignment_method=PVAL_ASSIGNMENT_METHOD
                        p-value method to use. Choices are:
                        fisher_z_transform, parametric_t_distribution,
                        bootstrapped, kendall [default: fisher_z_transform]
  --metadata_key=METADATA_KEY
                        Key to extract metadata from BIOM table. [default:
                        taxonomy]
  --permutations=PERMUTATIONS
                        Number of permutations to use for bootstrapped p-value
                        calculations. [default: 1000]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        path to input BIOM table [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        path to the output file to be created [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to metadata mapping file [REQUIRED]
    -c CATEGORY, --category=CATEGORY
                        name of the category in the metadata mapping file over
                        which to run the analysis [REQUIRED]

This script has been deprecated in favor of group_significance.py which is more generally applicable. For more details, call:

 group_significance.py -h


Usage: parallel_align_seqs_pynast.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

A wrapper for the align_seqs.py PyNAST option, intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_align_seqs_pynast.py -h

Example: Align the input file (-i) against using PyNAST and write the output (-o) to $PWD/pynast_aligned_seqs/. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_align_seqs_pynast.py -i $PWD/inseqs.fasta -o $PWD/pynast_aligned_seqs/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t TEMPLATE_FP, --template_fp=TEMPLATE_FP
                        Filepath for template alignment [default:
                        /home/vsts/conda/conda-bld/qiime_1630456709534/_test_e
                        nv_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placeh/lib/python2.7
                        /site-packages/qiime_default_reference/gg_13_8_otus/re
                        p_set_aligned/85_otus.pynast.fasta]
  -a PAIRWISE_ALIGNMENT_METHOD, --pairwise_alignment_method=PAIRWISE_ALIGNMENT_METHOD
                        Method to use for pairwise alignments [default:
                        uclust]
  -d BLAST_DB, --blast_db=BLAST_DB
                        Database to blast against [default: created on-the-fly
                        from template_alignment]
  -e MIN_LENGTH, --min_length=MIN_LENGTH
                        Minimum sequence length to include in alignment
                        [default: 75% of the median input sequence length]
  -p MIN_PERCENT_ID, --min_percent_id=MIN_PERCENT_ID
                        Minimum percent sequence identity to closest blast hit
                        to include sequence in alignment, expressed as a real
                        number between 0 and 100 [default: 75.0]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
Usage: parallel_alpha_diversity.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the alpha_diversity.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_alpha_diversity.py -h

Example: Apply the observed_OTUs, chao1, PD_whole_tree metrics (-m) to all otu tables in rarefied_otu_tables/ (-i) and write the resulting output files to adiv/ (-o, will be created if it doesn't exist). Use the rep_set.tre (-t) to compute phylogenetic diversity metrics. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_alpha_diversity.py -i $PWD/rarefied_otu_tables -o $PWD/adiv -m observed_otus,chao1,PD_whole_tree -t $PWD/rep_set.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t TREE_PATH, --tree_path=TREE_PATH
                        path to newick tree file, required for phylogenetic
                        metrics [default: none]
  -m METRICS, --metrics=METRICS
                        metrics to use, comma delimited
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input path, must be directory [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path, must be directory [REQUIRED]
Usage: parallel_assign_taxonomy_blast.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the assign_taxonomy.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_assign_taxonomy_blast.py -h

Example: Assign taxonomy to all sequences in the input file (-i) using BLAST with the id to taxonomy mapping file (-t) and reference sequences file (-r), and write the results (-o) to $PWD/blast_assigned_taxonomy/. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_assign_taxonomy_blast.py -i $PWD/inseqs.fasta -t $PWD/id_to_tax.txt -r $PWD/refseqs.fasta -o $PWD/blast_assigned_taxonomy/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -r REFERENCE_SEQS_FP, --reference_seqs_fp=REFERENCE_SEQS_FP
                        Ref seqs to blast against.  Must provide either
                        --blast_db or --reference_seqs_db for assignment with
                        blast [default: /home/vsts/conda/conda-bld/qiime_16304
                        56709534/_test_env_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placehold_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeh/
                        lib/python2.7/site-packages/qiime_default_reference/gg
                        _13_8_otus/rep_set/97_otus.fasta]
  -b BLAST_DB, --blast_db=BLAST_DB
                        Database to blast against.  Must provide either
                        --blast_db or --reference_seqs_db for assignment with
                        blast [default: none]
  -e E_VALUE, --e_value=E_VALUE
                        Maximum e-value to record an assignment, only used for
                        blast method [default: 0.001]
  -B BLASTMAT_DIR, --blastmat_dir=BLASTMAT_DIR
                        full path to directory containing blastmat file
                        [default: none]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]
  -t ID_TO_TAXONOMY_FP, --id_to_taxonomy_fp=ID_TO_TAXONOMY_FP
                        full path to id_to_taxonomy mapping file [default:
                        /home/vsts/conda/conda-bld/qiime_1630456709534/_test_e
                        nv_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placeh/lib/python2.7
                        /site-packages/qiime_default_reference/gg_13_8_otus/ta
                        xonomy/97_otu_taxonomy.txt]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        full path to input_fasta_fp [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        full path to store output files [REQUIRED]
Usage: parallel_assign_taxonomy_rdp.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the assign_taxonomy.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_assign_taxonomy_rdp.py -h

Example: Assign taxonomy to all sequences in the input file (-i) using the RDP classifier and write the results (-o) to $PWD/rdp_assigned_taxonomy/. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_assign_taxonomy_rdp.py -i $PWD/inseqs.fasta -o $PWD/rdp_assigned_taxonomy/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --rdp_classifier_fp=RDP_CLASSIFIER_FP
                        full path to rdp classifier jar file [default: none]
  -c CONFIDENCE, --confidence=CONFIDENCE
                        Minimum confidence to record an assignment [default:
                        0.5]
  -t ID_TO_TAXONOMY_FP, --id_to_taxonomy_fp=ID_TO_TAXONOMY_FP
                        full path to id_to_taxonomy mapping file [default:
                        /home/vsts/conda/conda-bld/qiime_1630456709534/_test_e
                        nv_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placeh/lib/python2.7
                        /site-packages/qiime_default_reference/gg_13_8_otus/ta
                        xonomy/97_otu_taxonomy.txt]
  -r REFERENCE_SEQS_FP, --reference_seqs_fp=REFERENCE_SEQS_FP
                        Ref seqs to rdp against. [default: /home/vsts/conda
                        /conda-bld/qiime_1630456709534/_test_env_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeho
                        ld_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placeh/lib/python2.7/site-packages/qiim
                        e_default_reference/gg_13_8_otus/rep_set/97_otus.fasta
                        ]
  --rdp_max_memory=RDP_MAX_MEMORY
                        Maximum memory allocation, in MB, for Java virtual
                        machine when using the rdp method.  Increase for large
                        training sets [default: 4000]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        full path to input_fasta_fp [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to store output files [REQUIRED]
Usage: parallel_assign_taxonomy_uclust.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the assign_taxonomy.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_assign_taxonomy_uclust.py -h

Example: Assign taxonomy to all sequences in the input file (-i) using the uclust consensus taxonomy assigner and write the results (-o) to $PWD/uclust_assigned_taxonomy/. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_assign_taxonomy_uclust.py -i $PWD/inseqs.fasta -o $PWD/uclust_assigned_taxonomy/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t ID_TO_TAXONOMY_FP, --id_to_taxonomy_fp=ID_TO_TAXONOMY_FP
                        full path to id_to_taxonomy mapping file [default:
                        /home/vsts/conda/conda-bld/qiime_1630456709534/_test_e
                        nv_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placeh/lib/python2.7
                        /site-packages/qiime_default_reference/gg_13_8_otus/ta
                        xonomy/97_otu_taxonomy.txt]
  -r REFERENCE_SEQS_FP, --reference_seqs_fp=REFERENCE_SEQS_FP
                        Ref seqs to search against. [default: /home/vsts/conda
                        /conda-bld/qiime_1630456709534/_test_env_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeho
                        ld_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placeh/lib/python2.7/site-packages/qiim
                        e_default_reference/gg_13_8_otus/rep_set/97_otus.fasta
                        ]
  --min_consensus_fraction=MIN_CONSENSUS_FRACTION
                        Minimum fraction of database hits that must have a
                        specific taxonomic assignment to assign that taxonomy
                        to a query [default: 0.51]
  --similarity=SIMILARITY
                        Minimum percent similarity to consider a database
                        match a hit, expressed as a fraction between 0 and 1
                        [default: 0.9]
  --uclust_max_accepts=UCLUST_MAX_ACCEPTS
                        Number of database hits to consider when making an
                        assignment [default: 3]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        full path to fasta file containing query sequences
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to store output files [REQUIRED]
Usage: parallel_beta_diversity.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the beta_diversity.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_beta_diversity.py -h

Apply beta_diversity.py in parallel to multiple otu tables: Apply the unweighted_unifrac and weighted_unifrac metrics (modify with -m) to all otu tables in rarefied_otu_tables (-i) and write the resulting output files to bdiv/ (-o, will be created if it doesn't exist). Use the rep_set.tre (-t) to compute phylogenetic diversity metrics. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_beta_diversity.py -i $PWD/rarefied_otu_tables/ -o $PWD/bdiv/ -t $PWD/rep_set.tre

Apply beta_diversity.py in parallel to a single otu table: 
 parallel_beta_diversity.py -i $PWD/otu_table.biom -o $PWD/bdiv_single/ -t $PWD/rep_set.tre

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m METRICS, --metrics=METRICS
                        Beta-diversity metric(s) to use. A comma-separated
                        list should be provided when multiple metrics are
                        specified. [default:
                        unweighted_unifrac,weighted_unifrac]
  -t TREE_PATH, --tree_path=TREE_PATH
                        path to newick tree file, required for phylogenetic
                        metrics [default: none]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -f, --full_tree       By default, each job removes calls _fast_unifrac_setup
                        to remove unused parts of the tree. pass -f if you
                        already have a minimal tree, and this script will run
                        faster

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input path, must be directory [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path, must be directory [REQUIRED]
Usage: parallel_blast.py [options] {-i/--infile_path INFILE_PATH -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script for performing blast while making use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_blast.py -h

Example: BLAST $PWD/inseqs.fasta (-i) against a blast database created from $PWD/refseqs.fasta (-r). Store the results in $PWD/blast_out/ (-o). ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_blast.py -i $PWD/inseqs.fasta -r $PWD/refseqs.fasta -o $PWD/blast_out/ -e 0.001

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -c, --disable_low_complexity_filter
                        disable filtering of low-complexity sequences (i.e.,
                        -F F is passed to blast) [default: False]
  -e E_VALUE, --e_value=E_VALUE
                        E-value threshold for blasts [default: 1e-30]
  -n NUM_HITS, --num_hits=NUM_HITS
                        number of hits per query for blast results [default:
                        1]
  -w WORD_SIZE, --word_size=WORD_SIZE
                        word size for blast searches [default: 30]
  -a BLASTMAT_DIR, --blastmat_dir=BLASTMAT_DIR
                        full path to directory containing blastmat file
                        [default: none]
  -r REFSEQS_PATH, --refseqs_path=REFSEQS_PATH
                        Path to fasta sequences to search against. Required if
                        -b is not provided.
  -b BLAST_DB, --blast_db=BLAST_DB
                        Name of pre-formatted BLAST database. Required if -r
                        is not provided.
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INFILE_PATH, --infile_path=INFILE_PATH
                        Path of sequences to use as queries [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        name of output directory for blast jobs [REQUIRED]
Usage: parallel_identify_chimeric_seqs.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script works like the identify_chimeric_seqs.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_identify_chimeric_seqs.py -h

blast_fragments example: For each sequence provided as input, the blast_fragments method splits the input sequence into n roughly-equal-sized, non-overlapping fragments, and assigns taxonomy to each fragment against a reference database. The BlastTaxonAssigner (implemented in assign_taxonomy.py) is used for this. The taxonomies of the fragments are compared with one another (at a default depth of 4), and if contradictory assignments are returned the sequence is identified as chimeric. For example, if an input sequence was split into 3 fragments, and the following taxon assignments were returned:

==========  ==========================================================
fragment1:  Archaea;Euryarchaeota;Methanobacteriales;Methanobacterium
fragment2:  Archaea;Euryarchaeota;Halobacteriales;uncultured
fragment3:  Archaea;Euryarchaeota;Methanobacteriales;Methanobacterium
==========  ==========================================================

The sequence would be considered chimeric at a depth of 3 (Methanobacteriales vs. Halobacteriales), but non-chimeric at a depth of 2 (all Euryarchaeota).

blast_fragments begins with the assumption that a sequence is non-chimeric, and looks for evidence to the contrary. This is important when, for example, no taxonomy assignment can be made because no blast result is returned. If a sequence is split into three fragments, and only one returns a blast hit, that sequence would be considered non-chimeric. This is because there is no evidence (i.e., contradictory blast assignments) for the sequence being chimeric. This script can be run by the following command, where the resulting data is written to $PWD/blast_fragments_chimeric_seqs.txt and using default parameters (i.e., number of fragments ("-n 3"), taxonomy depth ("-d 4") and maximum E-value ("-e 1e-30")). ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_identify_chimeric_seqs.py -i $PWD/inseqs.fasta -t $PWD/id_to_tax.txt -r $PWD/refseqs.fasta -o $PWD/blast_fragments_chimeric_seqs.txt -m blast_fragments

ChimeraSlayer Example: Identify chimeric sequences using the ChimeraSlayer algorithm against a user provided reference database. The input sequences need to be provided in aligned (Py)Nast format and the reference database needs to be provided as aligned FASTA (-a). Note that the reference database needs to be the same that was used to build the alignment of the input sequences! ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_identify_chimeric_seqs.py -i $PWD/inseqs_aligned.fasta -o $PWD/chimera_slayer_chimeric_seqs.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -a ALIGNED_REFERENCE_SEQS_FP, --aligned_reference_seqs_fp=ALIGNED_REFERENCE_SEQS_FP
                        Path to (Py)Nast aligned reference sequences. REQUIRED
                        when method ChimeraSlayer [default: /home/vsts/conda
                        /conda-bld/qiime_1630456709534/_test_env_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeho
                        ld_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placeh/lib/python2.7/site-packages/qiim
                        e_default_reference/gg_13_8_otus/rep_set_aligned/85_ot
                        us.pynast.fasta]
  -t ID_TO_TAXONOMY_FP, --id_to_taxonomy_fp=ID_TO_TAXONOMY_FP
                        Path to tab-delimited file mapping sequences to
                        assigned taxonomy. Each assigned taxonomy is provided
                        as a comma-separated list. [default: none; REQUIRED
                        when method is blast_fragments]
  -r REFERENCE_SEQS_FP, --reference_seqs_fp=REFERENCE_SEQS_FP
                        Path to reference sequences (used to build a blast db
                        when method blast_fragments). [default: none; REQUIRED
                        when method blast_fragments if no blast_db is
                        provided;]
  -b BLAST_DB, --blast_db=BLAST_DB
                        Database to blast against. Must provide either
                        --blast_db or --reference_seqs_fp when method is
                        blast_fragments [default: none]
  -m CHIMERA_DETECTION_METHOD, --chimera_detection_method=CHIMERA_DETECTION_METHOD
                        Chimera detection method. Choices: blast_fragments or
                        ChimeraSlayer. [default:ChimeraSlayer]
  -n NUM_FRAGMENTS, --num_fragments=NUM_FRAGMENTS
                        Number of fragments to split sequences into (i.e.,
                        number of expected breakpoints + 1) [default: 3]
  -d TAXONOMY_DEPTH, --taxonomy_depth=TAXONOMY_DEPTH
                        Number of taxonomic divisions to consider when
                        comparing taxonomy assignments [default: 4]
  -e MAX_E_VALUE, --max_e_value=MAX_E_VALUE
                        Max e-value to assign taxonomy [default: 1e-30]
  --min_div_ratio=MIN_DIV_RATIO
                        min divergence ratio (passed to ChimeraSlayer). If set
                        to None uses ChimeraSlayer default value.  [default:
                        none]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        Path to store output [default: derived from
                        input_seqs_fp]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
Usage: parallel_map_reads_to_reference.py [options] {-i/--input_seqs_filepath INPUT_SEQS_FILEPATH -o/--output_dir OUTPUT_DIR -r/--refseqs_fp REFSEQS_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 parallel_map_reads_to_reference.py -h


 parallel_map_reads_to_reference.py -i $PWD/query_nt.fasta -r $PWD/refseqs_pr.fasta -o $PWD/usearch_mapped


 parallel_map_reads_to_reference.py -i $PWD/query_nt.fasta -r $PWD/refseqs_pr.fasta -o $PWD/blat_mapped -m blat


 parallel_map_reads_to_reference.py -i $PWD/query_nt.fasta -r $PWD/refseqs_nt.fasta -o $PWD/bwa-short_mapped -m bwa-short

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t OBSERVATION_METADATA_FP, --observation_metadata_fp=OBSERVATION_METADATA_FP
                        Path to observation metadata (e.g., taxonomy, EC, etc)
                        [default: none]
  -m ASSIGNMENT_METHOD, --assignment_method=ASSIGNMENT_METHOD
                        Method for picking OTUs.  Valid choices are: usearch
                        blat bwa-short. [default: usearch]
  -e EVALUE, --evalue=EVALUE
                        Max e-value to consider a match [default: 1e-10]
  -s MIN_PERCENT_ID, --min_percent_id=MIN_PERCENT_ID
                        Min percent id to consider a match, expressed as a
                        fraction between 0 and 1 [default: 0.75]
  --max_diff=MAX_DIFF   maxDiff to consider a match (applicable for -m bwa) --
                        see the aln section of "man bwa" for details [default
                        (defined by bwa): 0.04]
  --queryalnfract=QUERYALNFRACT
                        Min percent of the query seq that must match to
                        consider a match, expressed as a fraction between 0
                        and 1 (usearch only) [default: 0.35]
  --targetalnfract=TARGETALNFRACT
                        Min percent of the target/reference seq that must
                        match to consider a match, expressed as a fraction
                        between 0 and 1 (usearch only) [default: 0.0]
  --max_accepts=MAX_ACCEPTS
                        max_accepts value (usearch only) [default: 1]
  --max_rejects=MAX_REJECTS
                        max_rejects value to (usearch only) [default: 32]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_SEQS_FILEPATH, --input_seqs_filepath=INPUT_SEQS_FILEPATH
                        Path to input sequences file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Directory to store results [REQUIRED]
    -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        Path to reference sequences [REQUIRED]
Usage: parallel_merge_otu_tables.py [options] {-i/--input_fps INPUT_FPS -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script works like the merge_otu_tables.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_merge_otu_tables.py -h

Example: Merge the OTU tables $PWD/t1.biom,$PWD/t2.biom,$PWD/t3.biom,$PWD/t4.biom and write the resulting output table to the $PWD/merged/ directory.
 parallel_merge_otu_tables.py -i $PWD/t1.biom,$PWD/t2.biom,$PWD/t3.biom,$PWD/t4.biom -o $PWD/merged/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -C, --cluster         Submit to a torque cluster
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FPS, --input_fps=INPUT_FPS
                        the otu tables in biom format (comma-separated)
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output otu table directory path [REQUIRED]
Usage: parallel_multiple_rarefactions.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH -m/--min MIN -x/--max MAX}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the multiple_rarefactions.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_multiple_rarefactions.py -h

OTU tables of different depths: Build rarefied otu tables containing 10 (-m) to 140 (-x) sequences in steps of 10 (-s) with 2 (-n) repetions per number of sequences, from otu_table.biom (-i). Write the output files to the rarefied_otu_tables directory (-o, will be created if it doesn't exist). The name of the output files will be of the form rarefaction_<num_seqs>_<reptition_number>.biom. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_multiple_rarefactions.py -o $PWD/rarefied_otu_tables/ -m 10 -x 140 -s 10 -n 2 -i $PWD/otu_table.biom

OTU tables of the same depth: Build 8 rarefied otu tables each containing exactly 100 sequences per sample (even depth rarefaction). ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_multiple_rarefactions.py -o $PWD/even_otu_tables/ -m 100 -x 100 -n 8 -i $PWD/otu_table.biom

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -n NUM_REPS, --num_reps=NUM_REPS
                        num iterations at each seqs/sample level [default: 10]
  --suppress_lineages_included
                        Exclude taxonomic (lineage) information for each OTU.
  -s STEP, --step=STEP  levels: min, min+step... for level <= max [default: 1]
  --subsample_multinomial
                        subsample using subsampling with replacement [default:
                        False]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input filepath, (the otu table) [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        write output rarefied otu tables here makes dir if it
                        doesn't exist [REQUIRED]
    -m MIN, --min=MIN   min seqs/sample [REQUIRED]
    -x MAX, --max=MAX   max seqs/sample (inclusive) [REQUIRED]
Usage: parallel_pick_otus_blast.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the pick_otus.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_pick_otus_blast.py -h

Example: Pick OTUs by blasting $PWD/inseqs.fasta against $PWD/refseqs.fasta and write the output to the $PWD/blast_otus/ directory. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_pick_otus_blast.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/blast_otus/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -e MAX_E_VALUE, --max_e_value=MAX_E_VALUE
                        Max E-value [default: 1e-10]
  -s SIMILARITY, --similarity=SIMILARITY
                        Sequence similarity threshold [default: 0.97]
  -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        full path to template alignment [default: none]
  -b BLAST_DB, --blast_db=BLAST_DB
                        database to blast against [default: none]
  --min_aligned_percent=MIN_ALIGNED_PERCENT
                        Minimum percent of query sequence that can be aligned
                        to consider a hit, expressed as a fraction between 0
                        and 1 (BLAST OTU picker only) [default: 0.5]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        full path to input_fasta_fp [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to store output files [REQUIRED]
Usage: parallel_pick_otus_sortmerna.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR -r/--refseqs_fp REFSEQS_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script works like the pick_otus.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_pick_otus_sortmerna.py -h

Example: Pick OTUs by searching $PWD/inseqs.fasta against $PWD/refseqs.fasta with reference-based sortmerna and write the output to the $PWD/smr_otus/ directory. This is a closed-reference OTU picking process. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_pick_otus_sortmerna.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/smr_otus/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s SIMILARITY, --similarity=SIMILARITY
                        Sequence similarity threshold [default: 0.97]
  --sortmerna_db=SORTMERNA_DB
                        Pre-existing database to search against when using -m
                        sortmerna [default: none]
  --sortmerna_e_value=SORTMERNA_E_VALUE
                        Maximum E-value when clustering [default = 1]
  --sortmerna_coverage=SORTMERNA_COVERAGE
                        Mininum percent query coverage (of an alignment) to
                        consider a hit, expressed as a fraction between 0 and
                        1 [default: 0.97]
  --sortmerna_tabular   Output alignments in the Blast-like tabular format
                        with two additional columns including the CIGAR string
                        and the percent query coverage [default: False]
  --sortmerna_best_N_alignments=SORTMERNA_BEST_N_ALIGNMENTS
                        Must be set together with --sortmerna_tabular. This
                        option specifies how many alignments per read will be
                        written [default: 1]
  --sortmerna_max_pos=SORTMERNA_MAX_POS
                        The maximum number of positions per seed to store  in
                        the indexed database [default: 10000]
  --threads=THREADS     Specify the number of threads to use per job. Use
                        --jobs_to_start to specify the number of
                        jobs.[default: 1]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        Path to input fasta file. [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Directory where output should be written. [REQUIRED]
    -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        Path to reference fasta file. [REQUIRED]
Usage: parallel_pick_otus_trie.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script performs like the pick_otus.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel. The script uses the first p bases of each read to sort all reads into separate buckets and then each buckets is processed separately. Note that in cases of amplicon sequencing we do not expect the buckets to be even sized, but rather a few buckets make up the majority of reads. Thus, not all combination of prefix length p and number of CPUS -O make sense. Good combinations for a small desktop multicore system would be -p 5 (default) and -O 4. For larger clusters, we suggest -p 10 and -O 20. Increasing -p to a value much larger than 10 will lead to lots of temporary files and many small jobs, so likely will not speed up the OTU picking. On the other hand, the max speed-up is bounded by the size of the largest buckets, so adding more cores will not always increase efficiency.

Example usage: 
Print help message and exit
 parallel_pick_otus_trie.py -h

Example: Pick OTUs by building a trie out of $PWD/inseqs.fasta and write the output to the $PWD/trie_otus/ directory. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_pick_otus_trie.py -i $PWD/seqs.fna -o $PWD/trie_otus/

Example: Pick OTUs by building a trie out of $PWD/inseqs.fasta and write the output to the $PWD/trie_otus/ directory. Split the input according to the first 10 bases of each read and process each set independently.
 parallel_pick_otus_trie.py -i $PWD/seqs.fna -o $PWD/trie_otus/ -p 10

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p PREFIX_LENGTH, --prefix_length=PREFIX_LENGTH
                        prefix length used to split the input. Must be smaller
                        than the shortest seq in input! [default: 5]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        full path to input_fasta_fp [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to store output files [REQUIRED]
Usage: parallel_pick_otus_uclust_ref.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR -r/--refseqs_fp REFSEQS_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script works like the pick_otus.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_pick_otus_uclust_ref.py -h

Example: Pick OTUs by searching $PWD/inseqs.fasta against $PWD/refseqs.fasta with reference-based uclust and write the output to the $PWD/blast_otus/ directory. This is a closed-reference OTU picking process. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_pick_otus_uclust_ref.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/ucref_otus/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s SIMILARITY, --similarity=SIMILARITY
                        Sequence similarity threshold [default: 0.97]
  -z, --enable_rev_strand_match
                        Enable reverse strand matching for uclust otu picking,
                        will double the amount of memory used. [default:
                        False]
  -A, --optimal_uclust  Pass the --optimal flag to uclust for uclust otu
                        picking. [default: False]
  -E, --exact_uclust    Pass the --exact flag to uclust for uclust otu
                        picking. [default: False]
  --max_accepts=MAX_ACCEPTS
                        max_accepts value to uclust and uclust_ref [default:
                        1]
  --max_rejects=MAX_REJECTS
                        max_rejects value to uclust and uclust_ref [default:
                        8]
  --stepwords=STEPWORDS
                        stepwords value to uclust and uclust_ref [default: 8]
  --word_length=WORD_LENGTH
                        w value to uclust and uclust_ref [default: 8]
  --uclust_stable_sort  Deprecated: stable sort enabled by default, pass
                        --uclust_suppress_stable_sort to disable [default:
                        True]
  --suppress_uclust_stable_sort
                        Don't pass --stable-sort to uclust [default: False]
  -d, --save_uc_files   Enable preservation of intermediate uclust (.uc) files
                        that are used to generate clusters via uclust.
                        [default: True]
  --denovo_otu_id_prefix=DENOVO_OTU_ID_PREFIX
                        OTU identifier prefix (string) for the de novo uclust
                        OTU picker [default: none, OTU ids are ascending
                        integers]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        full path to input_fasta_fp [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to store output files [REQUIRED]
    -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        full path to reference collection [REQUIRED]
Usage: parallel_pick_otus_usearch61_ref.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -o/--output_dir OUTPUT_DIR -r/--refseqs_fp REFSEQS_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script works like the pick_otus.py script, but is intended to make use of multicore/multiprocessor environments to perform analyses in parallel.

Example usage: 
Print help message and exit
 parallel_pick_otus_usearch61_ref.py -h

Example: Pick OTUs by searching $PWD/inseqs.fasta against $PWD/refseqs.fasta with reference-based usearch and write the output to the $PWD/usearch_ref_otus/ directory. This is a closed-reference OTU picking process. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 parallel_pick_otus_usearch61_ref.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/usearch_ref_otus/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -s SIMILARITY, --similarity=SIMILARITY
                        Sequence similarity threshold [default: 0.97]
  -z, --enable_rev_strand_match
                        Enable reverse strand matching for uclust, uclust_ref,
                        usearch, usearch_ref, usearch61, or usearch61_ref otu
                        picking, will double the amount of memory used.
                        [default: False]
  --max_accepts=MAX_ACCEPTS
                        max_accepts value to uclust, uclust_ref, usearch61,
                        and usearch61_ref.  By default, will use value
                        suggested by method (uclust: 20, usearch61: 1)
                        [default: default]
  --max_rejects=MAX_REJECTS
                        max_rejects value for uclust, uclust_ref, usearch61,
                        and usearch61_ref.  With default settings, will use
                        value recommended by clustering method used (uclust:
                        500, usearch61: 8 for usearch_fast_cluster option, 32
                        for reference and smallmem options) [default: default]
  --word_length=WORD_LENGTH
                        word length value for uclust, uclust_ref, and usearch,
                        usearch_ref, usearch61, and usearch61_ref. With
                        default setting, will use the setting recommended by
                        the method (uclust: 12, usearch: 64, usearch61: 8).
                        int value can be supplied to override this setting.
                        [default: default]
  --minlen=MINLEN       Minimum length of sequence allowed for usearch,
                        usearch_ref, usearch61, and usearch61_ref. [default:
                        64]
  --usearch_fast_cluster
                        Use fast clustering option for usearch or
                        usearch61_ref with new clusters.
                        --enable_rev_strand_match can not be enabled with this
                        option, and the only valid option for
                        usearch61_sort_method is 'length'.  This option uses
                        more memory than the default option for de novo
                        clustering. [default: False]
  --usearch61_sort_method=USEARCH61_SORT_METHOD
                        Sorting method for usearch61 and usearch61_ref.  Valid
                        options are abundance, length, or None.  If the
                        --usearch_fast_cluster option is enabled, the only
                        sorting method allowed in length. [default: abundance]
  --sizeorder           Enable size based preference in clustering with
                        usearch61. Requires that --usearch61_sort_method be
                        abundance. [default: False]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start [default: 1]
  -R, --retain_temp_files
                        retain temporary files after runs complete (useful for
                        debugging) [default: False]
  -S, --suppress_submit_jobs
                        Only split input and write commands file - don't
                        submit jobs [default: False]
  -T, --poll_directly   Poll directly for job completion rather than running
                        poller as a separate job. If -T is specified this
                        script will not return until all jobs have completed.
                        [default: False]
  -U CLUSTER_JOBS_FP, --cluster_jobs_fp=CLUSTER_JOBS_FP
                        path to cluster jobs script (defined in qiime_config)
                        [default: start_parallel_jobs.py]
  -W, --suppress_polling
                        suppress polling of jobs and merging of results upon
                        completion [default: False]
  -X JOB_PREFIX, --job_prefix=JOB_PREFIX
                        job prefix [default: descriptive prefix + random
                        chars]
  -Z SECONDS_TO_SLEEP, --seconds_to_sleep=SECONDS_TO_SLEEP
                        Number of seconds to sleep between checks for run
                        completion when polling runs [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        full path to input_fasta_fp [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to store output files [REQUIRED]
    -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        full path to reference collection [REQUIRED]
Usage: pick_closed_reference_otus.py [options] {-i/--input_fp INPUT_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script picks OTUs using a closed reference and constructs an OTU table.
Taxonomy is assigned using a pre-defined taxonomy map of reference sequence OTU
to taxonomy. If full-length genomes are provided as the reference sequences,
this script applies the Shotgun UniFrac method.

**Note:** If most or all of your sequences are failing to hit the reference,
your sequences may be in the reverse orientation with respect to your reference
database. To address this, you should add the following line to your parameters
file (creating one, if necessary) and pass this file as -p:

pick_otus:enable_rev_strand_match True

Be aware that this doubles the amount of memory used.



Example usage: 
Print help message and exit
 pick_closed_reference_otus.py -h

Pick OTUs, assign taxonomy, and create an OTU table against a reference set of OTUs. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_closed_reference_otus.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/otus_w_tax/ -t $PWD/taxa.txt

Pick OTUs and create an OTU table against a reference set of OTUs without adding taxonomy assignments. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_closed_reference_otus.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/otus/

Pick OTUs, assign taxonomy, and create an OTU table against a reference set of OTUs using usearch_ref. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_closed_reference_otus.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/otus_usearch/ -p $PWD/usearch_params.txt -t $PWD/taxa.txt

Pick OTUs using usearch_ref, assign taxonomy, and create an OTU table against a reference set of OTUs using usearch_ref. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_closed_reference_otus.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/otus_usearch_ref/ -p $PWD/usearch5.2_params.txt -t $PWD/taxa.txt

Pick OTUs, assign taxonomy, and create an OTU table against a reference set of OTUs using sortmerna. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_closed_reference_otus.py -i $PWD/seqs.fna -r $PWD/refseqs.fna -o $PWD/otus_sortmerna/ -p $PWD/sortmerna_params.txt -t $PWD/taxa.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -r REFERENCE_FP, --reference_fp=REFERENCE_FP
                        The reference sequences [default: /home/vsts/conda
                        /conda-bld/qiime_1630456709534/_test_env_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeho
                        ld_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placeh/lib/python2.7/site-packages/qiim
                        e_default_reference/gg_13_8_otus/rep_set/97_otus.fasta
                        ]. NOTE: If you do not pass -r to this script, you
                        will be using QIIME's default reference sequences. In
                        this case, QIIME will copy the corresponding reference
                        tree to the output directory. This is the tree that
                        should be used to perform phylogenetic diversity
                        analyses (e.g., with core_diversity_analyses.py).
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters . [if omitted, default values will
                        be used]
  -t TAXONOMY_FP, --taxonomy_fp=TAXONOMY_FP
                        the taxonomy map [default: /home/vsts/conda/conda-bld/
                        qiime_1630456709534/_test_env_placehold_placehold_plac
                        ehold_placehold_placehold_placehold_placehold_placehol
                        d_placehold_placehold_placehold_placehold_placehold_pl
                        acehold_placehold_placehold_placehold_placehold_placeh
                        old_placeh/lib/python2.7/site-packages/qiime_default_r
                        eference/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt]
  -s, --assign_taxonomy
                        Assign taxonomy to each sequence using
                        assign_taxonomy.py (this will override --taxonomy_fp,
                        if provided) [default: False]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]
  -a, --parallel        Run in parallel where available [default: False]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start. NOTE: you must also pass -a
                        to run in parallel, this defines the number of jobs to
                        be started if and only if -a is passed [default: 1]
  --suppress_taxonomy_assignment
                        skip the taxonomy assignment step, resulting in an OTU
                        table without taxonomy (this will override
                        --taxonomy_fp and --assign_taxonomy, if provided)
                        [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        the input sequences [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: pick_de_novo_otus.py [options] {-i/--input_fp INPUT_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script takes a sequence file and performs all processing steps through building the OTU table.

Example usage: 
Print help message and exit
 pick_de_novo_otus.py -h

Uclust example: The following command will start an analysis on seqs.fna (-i), which is a post-split_libraries fasta file. The sequence identifiers in this file should be of the form <sample_id>_<unique_seq_id>. The following steps, corresponding to the preliminary data preparation, are applied: Pick de novo OTUs at 97%; pick a representative sequence for each OTU (the OTU centroid sequence); align the representative set with PyNAST; assign taxonomy with the uclust consensus taxonomy assigner; filter the alignment prior to tree building - remove positions which are all gaps, and specified as 0 in the lanemask; build a phylogenetic tree with FastTree; build an OTU table. All output files will be written to the directory specified by -o, and subdirectories as appropriate. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_de_novo_otus.py -i $PWD/seqs.fna -o $PWD/uclust_otus/

SumaClust example: The following command will start an analysis on seqs.fna (-i), which is a post-split_libraries fasta file. The sequence identifiers in this file should be of the form <sample_id>_<unique_seq_id>. The following steps, corresponding to the preliminary data preparation, are applied: Pick de novo OTUs at 97%; pick a representative sequence for each OTU (the OTU centroid sequence); align the representative set with PyNAST; assign taxonomy with the RDP consensus taxonomy assigner; filter the alignment prior to tree building - remove positions which are all gaps, and specified as 0 in the lanemask; build a phylogenetic tree with FastTree; build an OTU table. All output files will be written to the directory specified by -o, and subdirectories as appropriate. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_de_novo_otus.py -i $PWD/seqs.fna -o $PWD/sumaclust_otus/ -p $PWD/sumaclust_params.txt

Swarm example: The following command will start an analysis on seqs.fna (-i), which is a post-split_libraries fasta file. The sequence identifiers in this file should be of the form <sample_id>_<unique_seq_id>. The following steps, corresponding to the preliminary data preparation, are applied: Pick de novo OTUs at 97%; pick a representative sequence for each OTU (the OTU centroid sequence); align the representative set with PyNAST; assign taxonomy with the RDP consensus taxonomy assigner; filter the alignment prior to tree building - remove positions which are all gaps, and specified as 0 in the lanemask; build a phylogenetic tree with FastTree; build an OTU table. All output files will be written to the directory specified by -o, and subdirectories as appropriate. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/).
 pick_de_novo_otus.py -i $PWD/seqs.fna -o $PWD/swarm_otus/ -p $PWD/swarm_params.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters . [if omitted, default values will
                        be used]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]
  -a, --parallel        Run in parallel where available [default: False]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start. NOTE: you must also pass -a
                        to run in parallel, this defines the number of jobs to
                        be started if and only if -a is passed [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        the input fasta file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: pick_open_reference_otus.py [options] {-i/--input_fps INPUT_FPS -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


This script is broken down into 4 possible OTU picking steps, and 2 steps
involving the creation of OTU tables and trees. The commands for each step are
described below, including what the input and resulting output files are.
Additionally, the optional specified parameters of this script that can be passed
are referenced.

Step 1) Prefiltering and picking closed reference OTUs
The first step is an optional prefiltering of the input fasta file to remove
sequences that do not hit the reference database with a given sequence
identity (PREFILTER_PERCENT_ID). This step can take a very long time, so is
disabled by default. The prefilter parameters can be changed with the options:
--prefilter_refseqs_fp
--prefilter_percent_id
This filtering is accomplished by picking closed reference OTUs at the specified
prefilter percent id to produce:
prefilter_otus/seqs_otus.log
prefilter_otus/seqs_otus.txt
prefilter_otus/seqs_failures.txt
prefilter_otus/seqs_clusters.uc
Next, the seqs_failures.txt file is used to remove these failed sequences from
the original input fasta file to produce:
prefilter_otus/prefiltered_seqs.fna
This prefiltered_seqs.fna file is then considered to contain the reads
of the marker gene of interest, rather than spurious reads such as host
genomic sequence or sequencing artifacts.

If prefiltering is applied, this step progresses with the prefiltered_seqs.fna.
Otherwise it progresses with the input file. The Step 1 closed reference OTU
picking is done against the supplied reference database. This command produces:
step1_otus/_clusters.uc
step1_otus/_failures.txt
step1_otus/_otus.log
step1_otus/_otus.txt

The representative sequence for each of the Step 1 picked OTUs are selected to
produce:
step1_otus/step1_rep_set.fna

Next, the sequences that failed to hit the reference database in Step 1 are
filtered from the Step 1 input fasta file to produce:
step1_otus/failures.fasta

Then the failures.fasta file is randomly subsampled to PERCENT_SUBSAMPLE of
the sequences to produce:
step1_otus/subsampled_failures.fna.
Modifying PERCENT_SUBSAMPLE can have a big effect on run time for this workflow,
but will not alter the final OTUs.

Step 2) The subsampled_failures.fna are next clustered de novo, and each cluster
centroid is then chosen as a "new reference sequence" for use as the reference
database in Step 3, to produce:
step2_otus/subsampled_seqs_clusters.uc
step2_otus/subsampled_seqs_otus.log
step2_otus/subsampled_seqs_otus.txt
step2_otus/step2_rep_set.fna

Step 3) Pick Closed Reference OTUs against Step 2 de novo OTUs
Closed reference OTU picking is performed using the failures.fasta file created
in Step 1 against the 'reference' de novo database created in Step 2 to produce:
step3_otus/failures_seqs_clusters.uc
step3_otus/failures_seqs_failures.txt
step3_otus/failures_seqs_otus.log
step3_otus/failures_seqs_otus.txt

Assuming the user has NOT passed the --suppress_step4 flag:
The sequences which failed to hit the reference database in Step 3 are removed
from the Step 3 input fasta file to produce:
step3_otus/failures_failures.fasta

Step 4) Additional de novo OTU picking
It is assumed by this point that the majority of sequences have been assigned
to an OTU, and thus the sequence count of failures_failures.fasta is small
enough that de novo OTU picking is computationally feasible. However, depending
on the sequences being used, it might be that the failures_failures.fasta file
is still prohibitively large for de novo clustering, and the jobs might take
too long to finish. In this case it is likely that the user would want to pass
the --suppress_step4 flag to avoid this additional de novo step.

A final round of de novo OTU picking is done on the failures_failures.fasta file
to produce:
step4_otus/failures_failures_cluster.uc
step4_otus/failures_failures_otus.log
step4_otus/failures_failures_otus.txt

A representative sequence for each cluster is chosen to produce:
step4_otus/step4_rep_set.fna

Step 5) Produce the final OTU map and rep set
If Step 4 is completed, the OTU maps from Step 1, Step 3, and Step 4 are
concatenated to produce:
final_otu_map.txt

If Step 4 was not completed, the OTU maps from Steps 1 and Step 3 are
concatenated together to produce:
final_otu_map.txt

Next, the minimum specified OTU size required to keep an OTU is specified with
the --min_otu_size flag. For example, if the user left the --min_otu_size as the
default value of 2, requiring each OTU to contain at least 2 sequences, the any
OTUs which failed to meet this criteria would be removed from the
final_otu_map.txt to produce:
final_otu_map_mc2.txt

If --min_otu_size 10 was passed, it would produce:
final_otu_map_mc10.txt

The final_otu_map_mc2.txt is used to build the final representative set:
rep_set.fna

Step 6) Making the OTU tables and trees
An OTU table is built using the final_otu_map_mc2.txt file to produce:
otu_table_mc2.biom

As long as the --suppress_taxonomy_assignment flag is NOT passed,
then taxonomy will be assigned to each of the representative sequences
in the final rep_set produced in Step 5, producing:
rep_set_tax_assignments.log
rep_set_tax_assignments.txt
This taxonomic metadata is then added to the otu_table_mc2.biom to produce:
otu_table_mc_w_tax.biom

As long as the --suppress_align_and_tree is NOT passed, then the rep_set.fna
file will be used to align the sequences and build the phylogenetic tree,
which includes the de novo OTUs. Any sequences that fail to align are
omitted from the OTU table and tree to produce:
otu_table_mc_no_pynast_failures.biom
rep_set.tre

If both --suppress_taxonomy_assignment and --suppress_align_and_tree are
NOT passed, the script will produce:
otu_table_mc_w_tax_no_pynast_failures.biom

It is important to remember that with a large workflow script like this that
the user can jump into intermediate steps. For example, imagine that for some
reason the script was interrupted on Step 2, and the user did not want to go
through the process of re-picking OTUs as was done in Step 1. They can simply
rerun the script and pass in the:
--step_1_otu_map_fp
--step1_failures_fasta_fp
parameters, and the script will continue with Steps 2 - 4.

**Note:** If most or all of your sequences are failing to hit the reference
during the prefiltering or closed-reference OTU picking steps, your sequences
may be in the reverse orientation with respect to your reference database. To
address this, you should add the following line to your parameters file
(creating one, if necessary) and pass this file as -p:

pick_otus:enable_rev_strand_match True

Be aware that this doubles the amount of memory used in these steps of the
workflow.



Example usage: 
Print help message and exit
 pick_open_reference_otus.py -h

Run the subsampled open-reference OTU picking workflow on seqs1.fna using refseqs.fna as the reference collection and using sortmerna and sumaclust as the OTU picking methods. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will genenerally look like /home/ubuntu/my_analysis/
 pick_open_reference_otus.py -i $PWD/seqs1.fna -r $PWD/refseqs.fna -o $PWD/ucrss_sortmerna_sumaclust/ -p $PWD/ucrss_smr_suma_params.txt -m sortmerna_sumaclust

Run the subsampled open-reference OTU picking workflow on seqs1.fna using refseqs.fna as the reference collection. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/
 pick_open_reference_otus.py -i $PWD/seqs1.fna -r $PWD/refseqs.fna -o $PWD/ucrss/ -s 0.1 -p $PWD/ucrss_params.txt

Run the subsampled open-reference OTU picking workflow on seqs1.fna using refseqs.fna as the reference collection and using usearch61 and usearch61_ref as the OTU picking methods. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/
 pick_open_reference_otus.py -i $PWD/seqs1.fna -r $PWD/refseqs.fna -o $PWD/ucrss_usearch/ -s 0.1 -p $PWD/ucrss_params.txt -m usearch61

Run the subsampled open-reference OTU picking workflow in iterative mode on seqs1.fna and seqs2.fna using refseqs.fna as the initial reference collection. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/
 pick_open_reference_otus.py -i $PWD/seqs1.fna,$PWD/seqs2.fna -r $PWD/refseqs.fna -o $PWD/ucrss_iter/ -s 0.1 -p $PWD/ucrss_params.txt

Run the subsampled open-reference OTU picking workflow in iterative mode on seqs1.fna and seqs2.fna using refseqs.fna as the initial reference collection. This is useful if you're working with marker genes that do not result in useful alignment (e.g., fungal ITS). ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/
 pick_open_reference_otus.py -i $PWD/seqs1.fna,$PWD/seqs2.fna -r $PWD/refseqs.fna -o $PWD/ucrss_iter_no_tree/ -s 0.1 -p $PWD/ucrss_params.txt --suppress_align_and_tree

Run the subsampled open-reference OTU picking workflow in iterative mode on seqs1.fna and seqs2.fna using refseqs.fna as the initial reference collection, suppressing assignment of taxonomy. This is useful if you're working with a reference collection without associated taxonomy. ALWAYS SPECIFY ABSOLUTE FILE PATHS (absolute path represented here as $PWD, but will generally look something like /home/ubuntu/my_analysis/
 pick_open_reference_otus.py -i $PWD/seqs1.fna,$PWD/seqs2.fna -r $PWD/refseqs.fna -o $PWD/ucrss_iter_no_tax/ -s 0.1 -p $PWD/ucrss_params.txt --suppress_taxonomy_assignment

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m OTU_PICKING_METHOD, --otu_picking_method=OTU_PICKING_METHOD
                        The OTU picking method to use for reference and de
                        novo steps. Passing usearch61, for example, means that
                        usearch61 will be used for the de novo steps and
                        usearch61_ref will be used for reference steps.
                        [default: uclust]
  -r REFERENCE_FP, --reference_fp=REFERENCE_FP
                        the reference sequences [default: /home/vsts/conda
                        /conda-bld/qiime_1630456709534/_test_env_placehold_pla
                        cehold_placehold_placehold_placehold_placehold_placeho
                        ld_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placeh/lib/python2.7/site-packages/qiim
                        e_default_reference/gg_13_8_otus/rep_set/97_otus.fasta
                        ]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters . [if omitted, default values will
                        be used]
  --prefilter_refseqs_fp=PREFILTER_REFSEQS_FP
                        the reference sequences to use for the prefilter, if
                        different from the reference sequecnces to use for the
                        OTU picking [default: same as passed for
                        --reference_fp]
  -n NEW_REF_SET_ID, --new_ref_set_id=NEW_REF_SET_ID
                        Unique identifier for OTUs that get created in this
                        ref set (this is useful to support combining of
                        reference sets) [default:New]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  -a, --parallel        Run in parallel where available [default: False]
  -O JOBS_TO_START, --jobs_to_start=JOBS_TO_START
                        Number of jobs to start. NOTE: you must also pass -a
                        to run in parallel, this defines the number of jobs to
                        be started if and only if -a is passed [default: 1]
  -s PERCENT_SUBSAMPLE, --percent_subsample=PERCENT_SUBSAMPLE
                        Percent of failure sequences to include in the
                        subsample to cluster de novo, expressed as a fraction
                        between 0 and 1 (larger numbers should give more
                        comprehensive results but will be slower)
                        [default:0.001]
  --prefilter_percent_id=PREFILTER_PERCENT_ID
                        Sequences are pre-clustered at this percent id
                        (expressed as a fraction between 0 and 1) against the
                        reference and any reads which fail to hit are
                        discarded (a quality filter); pass 0.0 to disable
                        [default:0.0]
  --step1_otu_map_fp=STEP1_OTU_MAP_FP
                        reference OTU picking OTU map, to avoid rebuilding if
                        one has already been built. This must be an OTU map
                        generated by this workflow, not (for example)
                        pick_closed_reference_otus.py.
  --step1_failures_fasta_fp=STEP1_FAILURES_FASTA_FP
                        reference OTU picking failures fasta filepath, to
                        avoid rebuilding if one has already been built. This
                        must be a failures file generated by this workflow,
                        not (for example) pick_closed_reference_otus.py.
  --minimum_failure_threshold=MINIMUM_FAILURE_THRESHOLD
                        The minimum number of sequences that must fail to hit
                        the reference for subsampling to be performed. If
                        fewer than this number of sequences fail to hit the
                        reference, the de novo clustering step will run
                        serially rather than invoking the subsampled open
                        reference approach to improve performance. [default:
                        100000]
  --suppress_step4      suppress the final de novo OTU picking step  (may be
                        necessary for extremely large data sets) [default:
                        False]
  --min_otu_size=MIN_OTU_SIZE
                        the minimum otu size (in number of sequences) to
                        retain the otu [default: 2]
  --suppress_taxonomy_assignment
                        skip the taxonomy assignment step, resulting in an OTU
                        table without taxonomy [default: False]
  --suppress_align_and_tree
                        skip the sequence alignment and tree-building steps
                        [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FPS, --input_fps=INPUT_FPS
                        the input sequences filepath or comma-separated list
                        of filepaths [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: pick_otus.py [options] {-i/--input_seqs_filepath INPUT_SEQS_FILEPATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The OTU picking step assigns similar sequences to operational taxonomic units, or OTUs, by clustering sequences based on a user-defined similarity threshold. Sequences which are similar at or above the threshold level are taken to represent the presence of a taxonomic unit (e.g., a genus, when the similarity threshold is set at 0.94) in the sequence collection.

Currently, the following clustering methods have been implemented in QIIME:

1.  cd-hit (Li & Godzik, 2006; Li, Jaroszewski, & Godzik, 2001), which applies a "longest-sequence-first list removal algorithm" to cluster sequences.

2.  blast (Altschul, Gish, Miller, Myers, & Lipman, 1990), which compares and clusters each sequence against a reference database of sequences.

3.  Mothur (Schloss et al., 2009), which requires an input file of aligned sequences.  The input file of aligned sequences may be generated from an input file like the one described below by running align_seqs.py.  For the Mothur method, the clustering algorithm may be specified as nearest-neighbor, furthest-neighbor, or average-neighbor.  The default algorithm is furthest-neighbor.

4.  prefix/suffix [Qiime team, unpublished], which will collapse sequences which are identical in their first and/or last bases (i.e., their prefix and/or suffix). The prefix and suffix lengths are provided by the user and default to 50 each.

5.  Trie [Qiime team, unpublished], which collapsing identical sequences and sequences which are subsequences of other sequences.

6.  uclust (Edgar, RC 2010), creates "seeds" of sequences which generate clusters based on percent identity.

7.  uclust_ref (Edgar, RC 2010), as uclust, but takes a reference database to use as seeds.  New clusters can be toggled on or off.

8.  usearch (Edgar, RC 2010, version v5.2.236), creates "seeds" of sequences which generate clusters based on percent identity, filters low abundance clusters, performs de novo and reference based chimera detection.

9.  usearch_ref (Edgar, RC 2010, version v5.2.236), as usearch, but takes a reference database to use as seeds.  New clusters can be toggled on or off.

Quality filtering pipeline with usearch 5.X is described as usearch_qf "usearch quality filter", described here: http://qiime.org/tutorials/usearch_quality_filter.html

8.  usearch61 (Edgar, RC 2010, version v6.1.544), creates "seeds" of sequences which generate clusters based on percent identity.

9.  usearch61_ref (Edgar, RC 2010, version v6.1.544), as usearch61, but takes a reference database to use as seeds.  New clusters can be toggled on or off.

10. sumaclust (Mercier, C. et al., 2014, version 1.0), creates "seeds" of sequences which generate clusters based on similarity threshold.

11. sortmerna_v2 (Kopylova, E. et al., 2012), takes a reference database to use as seeds.

12. swarm (Mahe, F. et al., 2014), creates "seeds" of sequences which generate clusters based on a resolution threshold.


Chimera checking with usearch 6.X is implemented in identify_chimeric_seqs.py.  Chimera checking should be done first with usearch 6.X, and the filtered resulting fasta file can then be clustered.


The primary inputs for pick_otus.py are:

1. A FASTA file containing sequences to be clustered

2. An OTU threshold (default is 0.97, roughly corresponding to species-level OTUs);

3. The method to be applied for clustering sequences into OTUs.

pick_otus.py takes a standard fasta file as input.



Example usage: 
Print help message and exit
 pick_otus.py -h

Example (uclust method, default): Using the seqs.fna file generated from split_libraries.py and outputting the results to the directory "picked_otus_default/", while using default parameters (0.97 sequence similarity, no reverse strand matching)
 pick_otus.py -i seqs.fna -o picked_otus_default

To change the percent identity to a lower value, such as 90%, and also enable reverse strand matching, the command would be the following
 pick_otus.py -i seqs.fna -o picked_otus_90_percent_rev/ -s 0.90 -z

Uclust Reference-based OTU picking example: uclust_ref can be passed via -m to pick OTUs against a reference set where sequences within the similarity threshold to a reference sequence will cluster to an OTU defined by that reference sequence, and sequences outside of the similarity threshold to a reference sequence will form new clusters. OTU identifiers will be set to reference sequence identifiers when sequences cluster to reference sequences, and 'qiime_otu_<integer>' for new OTUs. Creation of new clusters can be suppressed by passing -C, in which case sequences outside of the similarity threshold to any reference sequence will be listed as failures in the log file, and not included in any OTU.
 pick_otus.py -i seqs.fna -r refseqs.fasta -m uclust_ref --denovo_otu_id_prefix qiime_otu_

Example (cdhit method): Using the seqs.fna file generated from split_libraries.py and outputting the results to the directory "cdhit_picked_otus/", while using default parameters (0.97 sequence similarity, no prefix filtering)
 pick_otus.py -i seqs.fna -m cdhit -o cdhit_picked_otus/

Currently the cd-hit OTU picker allows for users to perform a pre-filtering step, so that highly similar sequences are clustered prior to OTU picking. This works by collapsing sequences which begin with an identical n-base prefix, where n is specified by the -n parameter. A commonly used value here is 100 (e.g., -n 100). So, if using this filter with -n 100, all sequences which are identical in their first 100 bases will be clustered together, and only one representative sequence from each cluster will be passed to cd-hit. This is used to greatly decrease the run-time of cd-hit-based OTU picking when working with very large sequence collections, as shown by the following command
 pick_otus.py -i seqs.fna -m cdhit -o cdhit_picked_otus_filter/ -n 100

Alternatively, if the user would like to collapse identical sequences, or those which are subsequences of other sequences prior to OTU picking, they can use the trie prefiltering ("-t") option as shown by the following command.

Note: It is highly recommended to use one of the prefiltering methods when analyzing large datasets (>100,000 seqs) to reduce run-time.
 pick_otus.py -i seqs.fna -m cdhit -o cdhit_picked_otus_trie_prefilter/ -t

BLAST OTU-Picking Example: OTUs can be picked against a reference database using the BLAST OTU picker. This is useful, for example, when different regions of the SSU RNA have sequenced and a sequence similarity based approach like cd-hit therefore wouldn't work. When using the BLAST OTU picking method, the user must supply either a reference set of sequences or a reference database to compare against. The OTU identifiers resulting from this step will be the sequence identifiers in the reference database. This allows for use of a pre-existing tree in downstream analyses, which again is useful in cases where different regions of the 16s gene have been sequenced.

The following command can be used to blast against a reference sequence set, using the default E-value and sequence similarity (0.97) parameters
 pick_otus.py -i seqs.fna -o blast_picked_otus/ -m blast -r refseqs.fasta

If you already have a pre-built BLAST database, you can pass the database prefix as shown by the following command
 pick_otus.py -i seqs.fna -o blast_picked_otus_prebuilt_db/ -m blast -b refseqs.fasta

If the user would like to change the sequence similarity ("-s") and/or the E-value ("-e") for the blast method, they can use the following command
 pick_otus.py -i seqs.fna -o blast_picked_otus_90_percent/ -m blast -r refseqs.fasta -s 0.90 -e 1e-30

Prefix-suffix OTU Picking Example: OTUs can be picked by collapsing sequences which begin and/or end with identical bases (i.e., identical prefixes or suffixes).  This OTU picker is currently likely to be of limited use on its own, but will be very useful in collapsing very similar sequences in a chained OTU picking strategy that is currently in development. For example, the user will be able to pick OTUs with this method, followed by representative set picking, and then re-pick OTUs on their representative set. This will allow for highly similar sequences to be collapsed, followed by running a slower OTU picker. This ability to chain OTU pickers is not yet supported in QIIME. The following command illustrates how to pick OTUs by collapsing sequences which are identical in their first 50 and last 25 bases
 pick_otus.py -i seqs.fna -o prefix_suffix_picked_otus/ -m prefix_suffix -p 50 -u 25

Mothur OTU Picking Example: The Mothur program (http://www.mothur.org/) provides three clustering algorithms for OTU formation: furthest-neighbor (complete linkage), average-neighbor (group average), and nearest-neighbor (single linkage). Details on the algorithms may be found on the Mothur website and publications (Schloss et al., 2009). However, the running times of Mothur's clustering algorithms scale with the number of sequences squared, so the program may not be feasible for large data sets.

The following command may be used to create OTUs based on a furthest-neighbor algorithm (the default setting) using aligned sequences as input
 pick_otus.py -i seqs.aligned.fna -o mothur_picked_otus/ -m mothur

If you prefer to use a nearest-neighbor algorithm instead, you may specify this with the '-c' flag
 pick_otus.py -i seqs.aligned.fna -o mothur_picked_otus_nn/ -m mothur -c nearest

The sequence similarity parameter may also be specified. For example, the following command may be used to create OTUs at the level of 90% similarity
 pick_otus.py -i seqs.aligned.fna -o mothur_picked_otus_90_percent/ -m mothur -s 0.90

usearch: Usearch (http://www.drive5.com/usearch/) provides clustering, chimera checking, and quality filtering. The following command specifies a minimum cluster size of 2 to be used during cluster size filtering
 pick_otus.py -i seqs.fna -m usearch --word_length 64 --db_filepath refseqs.fasta -o usearch_qf_results/ --minsize 2

usearch example where reference-based chimera detection is disabled, and minimum cluster size filter is reduced from default (4) to 2: 
 pick_otus.py -i seqs.fna -m usearch --word_length 64 --suppress_reference_chimera_detection --minsize 2 -o usearch_qf_results_no_ref_chim_detection/

Use de novo OTU-picker Swarm: Using the seqs.fna file generated from split_libraries.py and outputting the results to the directory "$PWD/picked_otus_swarm/", while using default parameters (resolution = 1)
 pick_otus.py -i $PWD/seqs.fna -m swarm -o $PWD/picked_otus_swarm

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m OTU_PICKING_METHOD, --otu_picking_method=OTU_PICKING_METHOD
                        Method for picking OTUs.  Valid choices are:
                        sortmerna, mothur, trie, uclust_ref, usearch,
                        usearch_ref, blast, usearch61, usearch61_ref,
                        sumaclust, swarm, prefix_suffix, cdhit, uclust. The
                        mothur method requires an input file of aligned
                        sequences.  usearch will enable the usearch quality
                        filtering pipeline. [default: uclust]
  -c CLUSTERING_ALGORITHM, --clustering_algorithm=CLUSTERING_ALGORITHM
                        Clustering algorithm for mothur otu picking method.
                        Valid choices are: furthest, nearest, average.
                        [default: furthest]
  -M MAX_CDHIT_MEMORY, --max_cdhit_memory=MAX_CDHIT_MEMORY
                        Maximum available memory to cd-hit-est (via the
                        program's -M option) for cdhit OTU picking method
                        (units of Mbyte) [default: 400]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Path to store result file [default:
                        ./<OTU_METHOD>_picked_otus/]
  -r REFSEQS_FP, --refseqs_fp=REFSEQS_FP
                        Path to reference sequences to search against when
                        using -m blast, -m sortmerna, -m uclust_ref, -m
                        usearch_ref, or -m usearch61_ref [default:
                        /home/vsts/conda/conda-bld/qiime_1630456709534/_test_e
                        nv_placehold_placehold_placehold_placehold_placehold_p
                        lacehold_placehold_placehold_placehold_placehold_place
                        hold_placehold_placehold_placehold_placehold_placehold
                        _placehold_placehold_placehold_placeh/lib/python2.7
                        /site-packages/qiime_default_reference/gg_13_8_otus/re
                        p_set/97_otus.fasta]
  -b BLAST_DB, --blast_db=BLAST_DB
                        Pre-existing database to blast against when using -m
                        blast [default: none]
  -e MAX_E_VALUE_BLAST, --max_e_value_blast=MAX_E_VALUE_BLAST
                        Max E-value when clustering with BLAST [default:
                        1e-10]
  --sortmerna_db=SORTMERNA_DB
                        Pre-existing database to search against when using -m
                        sortmerna [default: none]
  --sortmerna_e_value=SORTMERNA_E_VALUE
                        Maximum E-value when clustering [default = 1]
  --sortmerna_coverage=SORTMERNA_COVERAGE
                        Mininum percent query coverage (of an alignment) to
                        consider a hit, expressed as a fraction between 0 and
                        1 [default: 0.97]
  --sortmerna_tabular   Output alignments in the Blast tabular format with two
                        additional columns including the CIGAR string and the
                        percent query coverage [default: False]
  --sortmerna_best_N_alignments=SORTMERNA_BEST_N_ALIGNMENTS
                        Must be set together with --sortmerna_tabular. This
                        option specifies how many alignments per read will be
                        written [default: 1]
  --sortmerna_max_pos=SORTMERNA_MAX_POS
                        The maximum number of positions per seed to store  in
                        the indexed database [default: 10000]
  --min_aligned_percent=MIN_ALIGNED_PERCENT
                        Minimum percent of query sequence that can be aligned
                        to consider a hit, expressed as a fraction between 0
                        and 1 (BLAST OTU picker only) [default: 0.5]
  -s SIMILARITY, --similarity=SIMILARITY
                        Sequence similarity threshold (for blast, cdhit,
                        uclust, uclust_ref, usearch, usearch_ref, usearch61,
                        usearch61_ref, sumaclust, and sortmerna) [default:
                        0.97]
  --sumaclust_exact     A sequence is assigned to the best matching seed
                        rather than the first matching seed passing the
                        similarity threshold [default: False]
  --sumaclust_l         Reference sequence length if the shortest [default:
                        True]
  --denovo_otu_id_prefix=DENOVO_OTU_ID_PREFIX
                        OTU identifier prefix (string) for the de novo OTU
                        pickers (sumaclust, swarm and uclust) [default:
                        denovo, OTU ids are ascendingintegers]
  --swarm_resolution=SWARM_RESOLUTION
                        Maximum number of differences allowed between two
                        amplicons, meaning that two amplicons will be grouped
                        if they have integer (or less) differences (see Swarm
                        manual at https://github.com/torognes/swarm for more
                        details). [default: 1]
  -q, --trie_reverse_seqs
                        Reverse seqs before picking OTUs with the Trie OTU
                        picker for suffix (rather than prefix) collapsing
                        [default: False]
  -n PREFIX_PREFILTER_LENGTH, --prefix_prefilter_length=PREFIX_PREFILTER_LENGTH
                        Prefilter data so seqs with identical first
                        prefix_prefilter_length are automatically grouped into
                        a single OTU.  This is useful for large sequence
                        collections where OTU picking doesn't scale well
                        [default: none; 100 is a good value]
  -t, --trie_prefilter  prefilter data so seqs which are identical prefixes of
                        a longer seq are automatically grouped into a single
                        OTU; useful for large sequence collections where OTU
                        picking doesn't scale well [default: False]
  -p PREFIX_LENGTH, --prefix_length=PREFIX_LENGTH
                        Prefix length when using the prefix_suffix otu picker;
                        WARNING: CURRENTLY DIFFERENT FROM
                        prefix_prefilter_length (-n)! [default: 50]
  -u SUFFIX_LENGTH, --suffix_length=SUFFIX_LENGTH
                        Suffix length when using the prefix_suffix otu picker
                        [default: 50]
  -z, --enable_rev_strand_match
                        Enable reverse strand matching for uclust, uclust_ref,
                        usearch, usearch_ref, usearch61, or usearch61_ref otu
                        picking, will double the amount of memory used.
                        [default: False]
  -D, --suppress_presort_by_abundance_uclust
                        Suppress presorting of sequences by abundance when
                        picking OTUs with uclust or uclust_ref [default:
                        False]
  -A, --optimal_uclust  Pass the --optimal flag to uclust for uclust otu
                        picking. [default: False]
  -E, --exact_uclust    Pass the --exact flag to uclust for uclust otu
                        picking. [default: False]
  -B, --user_sort       Pass the --user_sort flag to uclust for uclust otu
                        picking. [default: False]
  -C, --suppress_new_clusters
                        Suppress creation of new clusters using seqs that
                        don't match reference when using -m uclust_ref, -m
                        usearch61_ref, or -m usearch_ref [default: False]
  --max_accepts=MAX_ACCEPTS
                        max_accepts value to uclust, uclust_ref, usearch61,
                        and usearch61_ref.  By default, will use value
                        suggested by method (uclust: 1, usearch61: 1)
                        [default: default]
  --max_rejects=MAX_REJECTS
                        max_rejects value for uclust, uclust_ref, usearch61,
                        and usearch61_ref.  With default settings, will use
                        value recommended by clustering method used (uclust:
                        8, usearch61: 8 for usearch_fast_cluster option, 32
                        for reference and smallmem options) [default: default]
  --stepwords=STEPWORDS
                        stepwords value to uclust and uclust_ref [default: 8]
  --word_length=WORD_LENGTH
                        word length value for uclust, uclust_ref, and usearch,
                        usearch_ref, usearch61, and usearch61_ref. With
                        default setting, will use the setting recommended by
                        the method (uclust: 8, usearch: 64, usearch61: 8).
                        int value can be supplied to override this setting.
                        [default: default]
  --suppress_uclust_stable_sort
                        Don't pass --stable-sort to uclust [default: False]
  --suppress_prefilter_exact_match
                        Don't collapse exact matches before calling sortmerna,
                        sumaclust or uclust [default: False]
  -d, --save_uc_files   Enable preservation of intermediate uclust (.uc) files
                        that are used to generate clusters via uclust.  Also
                        enables preservation of all intermediate files created
                        by usearch  and usearch61. [default: True]
  -j PERCENT_ID_ERR, --percent_id_err=PERCENT_ID_ERR
                        Percent identity threshold for cluster error detection
                        with usearch, expressed as a fraction between 0 and 1.
                        [default: 0.97]
  -g MINSIZE, --minsize=MINSIZE
                        Minimum cluster size for size filtering with usearch.
                        [default: 4]
  -a ABUNDANCE_SKEW, --abundance_skew=ABUNDANCE_SKEW
                        Abundance skew setting for de novo chimera detection
                        with usearch. [default: 2.0]
  -f DB_FILEPATH, --db_filepath=DB_FILEPATH
                        Reference database of fasta sequences for reference
                        based chimera detection with usearch. [default: none]
  --perc_id_blast=PERC_ID_BLAST
                        Percent ID for mapping OTUs created by usearch back to
                        original sequence IDs [default: 0.97]
  --de_novo_chimera_detection=DE_NOVO_CHIMERA_DETECTION
                        Deprecated:  de novo chimera detection performed by
                        default, pass --suppress_de_novo_chimera_detection to
                        disable. [default: none]
  -k, --suppress_de_novo_chimera_detection
                        Suppress de novo chimera detection in usearch.
                        [default: False]
  --reference_chimera_detection=REFERENCE_CHIMERA_DETECTION
                        Deprecated:  Reference based chimera detection
                        performed by default, pass
                        --supress_reference_chimera_detection to disable
                        [default: none]
  -x, --suppress_reference_chimera_detection
                        Suppress reference based chimera detection in usearch.
                        [default: False]
  --cluster_size_filtering=CLUSTER_SIZE_FILTERING
                        Deprecated, cluster size filtering enabled by default,
                        pass --suppress_cluster_size_filtering to disable.
                        [default: none]
  -l, --suppress_cluster_size_filtering
                        Suppress cluster size filtering in usearch.  [default:
                        False]
  --remove_usearch_logs
                        Disable creation of logs when usearch is called.  Up
                        to nine logs are created, depending on filtering steps
                        enabled.  [default: False]
  --derep_fullseq       Dereplication of full sequences, instead of
                        subsequences. Faster than the default --derep_subseqs
                        in usearch. [default: False]
  -F NON_CHIMERAS_RETENTION, --non_chimeras_retention=NON_CHIMERAS_RETENTION
                        Selects subsets of sequences detected as non-chimeras
                        to retain after de novo and reference based chimera
                        detection.  Options are intersection or union.  union
                        will retain sequences that are flagged as non-chimeric
                        from either filter, while intersection will retain
                        only those sequences that are flagged as non-chimeras
                        from both detection methods. [default: union]
  --minlen=MINLEN       Minimum length of sequence allowed for usearch,
                        usearch_ref, usearch61, and usearch61_ref. [default:
                        64]
  --usearch_fast_cluster
                        Use fast clustering option for usearch or
                        usearch61_ref with new clusters.
                        --enable_rev_strand_match can not be enabled with this
                        option, and the only valid option for
                        usearch61_sort_method is 'length'.  This option uses
                        more memory than the default option for de novo
                        clustering. [default: False]
  --usearch61_sort_method=USEARCH61_SORT_METHOD
                        Sorting method for usearch61 and usearch61_ref.  Valid
                        options are abundance, length, or None.  If the
                        --usearch_fast_cluster option is enabled, the only
                        sorting method allowed in length. [default: abundance]
  --sizeorder           Enable size based preference in clustering with
                        usearch61. Requires that --usearch61_sort_method be
                        abundance. [default: False]
  --threads=THREADS     Specify number of threads (1 thread per core) to be
                        used for usearch61, sortmerna, sumaclust and swarm
                        commands that utilize multithreading. [default: 1]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_SEQS_FILEPATH, --input_seqs_filepath=INPUT_SEQS_FILEPATH
                        Path to input sequences file [REQUIRED]
Usage: pick_rep_set.py [options] {-i/--input_file OTU_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

After picking OTUs, you can then pick a representative set of sequences. For each OTU, you will end up with one sequence that can be used in subsequent analyses.

Example usage: 
Print help message and exit
 pick_rep_set.py -h

Simple example: picking a representative set for de novo-picked OTUs: The script pick_rep_set.py takes as input an 'OTU map' (via the "-i" parameter) which maps OTU identifiers to sequence identifiers. Typically, this will be the output file provided by pick_otus.py. Additionally, a FASTA file is required, via "-f", which contains all of the sequences whose identifiers are listed in the OTU map.
 pick_rep_set.py -i seqs_otus.txt -f seqs.fna -o rep_set1.fna

Picking OTUs with "preferred representative" sequences: Under some circumstances you may have a fasta file of "preferred representative" sequences. An example of this is if you were to pick OTUs against a reference collection with uclust_ref. In this case you may want your representative sequences to be the sequences from the reference collection, rather than the sequences from your sequencing run. To achieve this, you can pass the original reference collection via -r. If you additionally allowed for new clusters (i.e., sequences which don't match a reference sequence are used as seeds for new OTUs) you'll also need to pass the original sequence collection to pick a representative sequence from the sequencing run in that case.
 pick_rep_set.py -i seqs_otus.txt -f seqs.fna -r refseqs.fasta -o rep_set2.fna

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -f FASTA_FP, --fasta_file=FASTA_FP
                        Path to input fasta file [REQUIRED if not picking
                        against a reference set; default: None]
  -m REP_SET_PICKING_METHOD, --rep_set_picking_method=REP_SET_PICKING_METHOD
                        Method for picking representative sets.  Valid choices
                        are random, longest, most_abundant, first [default:
                        first (first chooses cluster seed when picking otus
                        with uclust)]
  -o RESULT_FP, --result_fp=RESULT_FP
                        Path to store result file [default:
                        <input_sequences_filepath>_rep_set.fasta]
  -l LOG_FP, --log_fp=LOG_FP
                        Path to store log file [default: No log file created.]
  -s SORT_BY, --sort_by=SORT_BY
                        sort by otu or seq_id [default: otu]
  -r REFERENCE_SEQS_FP, --reference_seqs_fp=REFERENCE_SEQS_FP
                        collection of preferred representative sequences
                        [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_FP, --input_file=OTU_FP
                        Path to input otu mapping file [REQUIRED]
Usage: plot_rank_abundance_graph.py [options] {-i/--otu_table_fp OTU_TABLE_FP -s/--sample_name SAMPLE_NAME -o/--result_fp RESULT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Plot a set of rank-abundance graphs from an OTU table and a set of sample names. Multiple graphs will be plotted into the same figure, in order to allow for an easy comparison across samples.

Example usage: 
Print help message and exit
 plot_rank_abundance_graph.py -h

Single graph example: Plot the rank-abundance curve of one sample using a linear scale for the x_axis
 plot_rank_abundance_graph.py -i otu_table.biom  -s 'PC.354' -x -v -o single_plot.pdf

multiple graph example: Plot the rank-abundance curve of several samples
 plot_rank_abundance_graph.py -i otu_table.biom  -s 'PC.354,PC.481,PC.636' -x -v -o multi_plot.pdf

multiple graph example: Plot the rank-abundance curve of all samples in an OTU table
 plot_rank_abundance_graph.py -i otu_table.biom  -s '*' -x -f eps -v -o all_plot.eps

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -a, --absolute_counts
                        plot absolute abundance values instead of relative
                        [default: False]
  -n, --no_legend       do not draw a legend [default: False]
  -x, --x_linear_scale  draw x axis in linear scale [default: False]
  -y, --y_linear_scale  draw y axis in linear scale [default: False]
  -f FILE_TYPE, --file_type=FILE_TYPE
                        save plot using this image type. Choice of pdf, svg,
                        png, eps [default: pdf]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        path to the input OTU table (i.e., the output from
                        make_otu_table.py) [REQUIRED]
    -s SAMPLE_NAME, --sample_name=SAMPLE_NAME
                        name of the sample to plot. Use "*" to plot all.
                        [REQUIRED]
    -o RESULT_FP, --result_fp=RESULT_FP
                        Path to store resulting figure file. File extension
                        will be appended if not supplied (e.g.: rankfig ->
                        rankfig.pdf). Additionally, a log file rankfig_log.txt
                        will be created [REQUIRED]
Usage: plot_semivariogram.py [options] {-x/--input_path_x INPUT_PATH_X -y/--input_path_y INPUT_PATH_Y -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Fits a spatial autocorrelation model between two matrices and plots the result. This script will work with two distance matrices but will ignore the 0s at the diagonal and the values that go to N/A. See distance_matrix_from_mapping.py.

Example usage: 
Print help message and exit
 plot_semivariogram.py -h

Fitting: For this script, the user supplies two distance matrices (i.e. resulting file from beta_diversity.py), along with the output filename (e.g. semivariogram), and the model to fit, as follows
 plot_semivariogram.py -x distance.txt -y unifrac.txt -o semivariogram_exponential.png

Modify the the default method to gaussian
 plot_semivariogram.py -x distance.txt -y unifrac.txt --model gaussian -o semivariogram_gaussian.png

Color semivariograms by a category in the metadata mapping file: Using a header name in the mapping file (Time), create two separate semivariograms in the same plot, an accompanying file with the color coding will be created(categories_legend.eps), both the legends and the plot will be in eps format.
 plot_semivariogram.py -y unweighted_unifrac_dm.txt -x time_dm.txt --model gaussian -m Fasting_Map.txt -o categories.eps -c Treatment

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -b BINNING, --binning=BINNING
                        binning ranges. Format: [increment,top_limit], when
                        top_limit is -1=infinitum; you can specify several
                        ranges using the same format, i.e. [2.5,10][50,-1]
                        will set two bins, one from 0-10 using 2.5 size steps
                        and from 10-inf using 50 size steps. Note that the
                        binning is used to clean the plots (reduce number of
                        points) but ignored to fit the model. [default: none]
  --ignore_missing_samples
                        This will overpass the error raised when the matrices
                        have different sizes/samples
  --x_max=X_MAX         x axis max limit [default: auto]
  --x_min=X_MIN         x axis min limit [default: auto]
  --y_max=Y_MAX         y axis max limit [default: auto]
  --y_min=Y_MIN         y axis min limit [default: auto]
  -X X_LABEL, --x_label=X_LABEL
                        Label for the x axis [default: Distance Dissimilarity
                        (m)]
  -Y Y_LABEL, --y_label=Y_LABEL
                        Label for the y axis [default: Community
                        Dissimilarity]
  -t FIG_TITLE, --fig_title=FIG_TITLE
                        Title of the plot [default: Semivariogram]
  --dot_color=DOT_COLOR
                        dot color for plot, more info:
                        http://matplotlib.sourceforge.net/api/pyplot_api.html
                        [default: white]
  --dot_marker=DOT_MARKER
                        dot color for plot, more info:
                        http://matplotlib.sourceforge.net/api/pyplot_api.html
                        [default: o]
  --line_color=LINE_COLOR
                        line color for plot, more info:
                        http://matplotlib.sourceforge.net/api/pyplot_api.html
                        [default: blue]
  --dot_alpha=DOT_ALPHA
                        alpha for dots, more info:
                        http://matplotlib.sourceforge.net/api/pyplot_api.html
                        [default: 1]
  --line_alpha=LINE_ALPHA
                        alpha for dots, more info:
                        http://matplotlib.sourceforge.net/api/pyplot_api.html
                        [default: 1]
  --model=MODEL         model to be fitted to the data. Valid choices
                        are:nugget, exponential, gaussian, periodic, linear.
                        [default: exponential]
  -p, --print_model     Print in the title of the plot the function of the
                        fit. [default: False]
  -c CATEGORY, --category=CATEGORY
                        category to color each of the trajectories when you
                        have multiple treatments [default: none]
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        metadata mapping file, only used when coloring by a
                        category, a file with the legends and color coding
                        will be created with the suffix legend [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -x INPUT_PATH_X, --input_path_x=INPUT_PATH_X
                        path to distance matrix to be displayed in the x axis
                        [REQUIRED]
    -y INPUT_PATH_Y, --input_path_y=INPUT_PATH_Y
                        path to distance matrix to be displayed in the y axis
                        [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path. directory for batch processing, filename
                        for single file operation [REQUIRED]
Usage: plot_taxa_summary.py [options] {-i/--counts_fname COUNTS_FNAME}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script automates the construction of pie, bar and area charts showing the breakdown of taxonomy by given levels. The script creates an html file for each chart type for easy visualization. It uses the taxonomy or category counts from summarize_taxa.py for combined samples by level (-i) and user specified labels for each file passed in (-l). Output will be written to the user specified folder (-o) the, where the default is the current working directory. The user can also specify the number of categories displayed for within a single pie chart, where the rest are grouped together as the 'other category' using the (-n) option, default is 20.


Example usage: 
Print help message and exit
 plot_taxa_summary.py -h

Examples: If you wish to run the code using default parameters, you must supply a counts file (phylum.txt) along with the taxon level label (Phylum), the type(s) of charts to produce, and an output directory, by using the following command
 plot_taxa_summary.py -i phylum.txt -l phylum -c pie,bar,area -o phylum_charts/

If you want to make charts for multiple levels at a time (phylum.txt,class.txt,genus.txt) use the following command
 plot_taxa_summary.py -i phylum.txt,class.txt,genus.txt -l Phylum,Class,Genus -c pie,bar,area -o phylum_class_genus_charts/

Additionally, if you would like to display on a set number of taxa ("-n 10") in the pie charts, you can use the following command
 plot_taxa_summary.py -i class.txt -l Class -c pie -n 10 -o class_pie_n10_charts/

If you would like to display generate pie charts for specific samples, i.e. sample 'PC.636' and sample 'PC.635' that are in the counts file header, you can use the following command
 plot_taxa_summary.py -i class.txt -l Class -b PC.636,PC.635 -o sample_charts/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -l LABELS, --labels=LABELS
                        Comma-separated list of taxonomic levels (e.g.
                        Phylum,Class,Order)  [default=none]
  -n NUM_CATEGORIES, --num_categories=NUM_CATEGORIES
                        The maximum number of taxonomies to show in each pie
                        chart. All additional taxonomies are grouped into an
                        "other" category. NOTE: this functionality only
                        applies to the pie charts. [default: 20]
  -o DIR_PATH, --dir_path=DIR_PATH
                        Output directory
  -b COLORBY, --colorby=COLORBY
                        This is the categories to color by in the plots from
                        the metadata mapping file. The categories must match
                        the name of a  column header in the mapping file
                        exactly and multiple categories can be list by comma
                        separating them without spaces. [default=none]
  -p PREFS_PATH, --prefs_path=PREFS_PATH
                        Input user-generated preferences filepath. NOTE: This
                        is a file with a dictionary containing preferences for
                        the analysis. The key taxonomy_coloring is used for
                        the coloring. [default: none]
  -k BACKGROUND_COLOR, --background_color=BACKGROUND_COLOR
                        This is the background color to use in the plots
                        (black or white) [default: white]
  -d DPI, --dpi=DPI     This is the resolution of the plot. [default: 80]
  -x X_WIDTH, --x_width=X_WIDTH
                        This is the width of the x-axis to use in the plots.
                        [default: 12]
  -y Y_HEIGHT, --y_height=Y_HEIGHT
                        This is the height of the y-axis to use in the plots.
                        [default: 6]
  -w BAR_WIDTH, --bar_width=BAR_WIDTH
                        This the width of the bars in the bar graph and should
                        be a number between 0 and 1. NOTE: this only applies
                        to the bar charts. [default: 0.75]
  -t TYPE_OF_FILE, --type_of_file=TYPE_OF_FILE
                        This is the type of image to produce (i.e.
                        pdf,svg,png). [default: pdf]
  -c CHART_TYPE, --chart_type=CHART_TYPE
                        This is the type of chart to plot (i.e. pie, bar or
                        area). The user has the ability to plot multiple
                        types, by using a comma-separated list (e.g. area,pie)
                        [default: area,bar]
  -r RESIZE_NTH_LABEL, --resize_nth_label=RESIZE_NTH_LABEL
                        Make every nth label larger than the other lables.
                        This is for large area and bar charts where the font
                        on the x-axis is small. This requires an integer value
                        greater than 0. [default: 0]
  -s, --include_html_legend
                        Include HTML legend. If present, the writing of the
                        legend in the html page is included. [default: False]
  -a LABEL_TYPE, --label_type=LABEL_TYPE
                        Label type ("numeric" or "categorical").  If the label
                        type is defined as numeric, the x-axis will be scaled
                        accordingly. Otherwise the x-values will treated
                        categorically and be evenly spaced [default:
                        categorical].

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i COUNTS_FNAME, --counts_fname=COUNTS_FNAME
                        Input comma-separated list of summarized taxa
                        filepaths (i.e results from summarize_taxa.py)
                        [REQUIRED]
Usage: poller.py [options] {-f/--check_run_complete_file CHECK_RUN_COMPLETE_FILE}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Script for polling parallel runs to check completion.

Example usage: 
Print help message and exit
 poller.py -h

Poller example: Runs the poller, which checks for the existence of two input files (file1.txt and file2.txt) and merges their contents. A cleanup file is provided that instructs the poller to remove the newly merged file.
 poller.py -f run_complete.txt -m poller_test_completed.txt -d clean_up.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -r CHECK_RUN_COMPLETE_F, --check_run_complete_f=CHECK_RUN_COMPLETE_F
                        function which returns True when run is completed
                        [default:
                        qiime.parallel.poller.basic_check_run_complete_f]
  -p PROCESS_RUN_RESULTS_F, --process_run_results_f=PROCESS_RUN_RESULTS_F
                        function to be called when runs complete [default:
                        qiime.parallel.poller.basic_process_run_results_f]
  -m PROCESS_RUN_RESULTS_FILE, --process_run_results_file=PROCESS_RUN_RESULTS_FILE
                        path to file containing a map of tmp filepaths which
                        should be written to final output filepaths [default:
                        none]
  -c CLEAN_UP_F, --clean_up_f=CLEAN_UP_F
                        function called after processing result [default:
                        qiime.parallel.poller.basic_clean_up_f]
  -d CLEAN_UP_FILE, --clean_up_file=CLEAN_UP_FILE
                        List of files and directories to remove after run
                        [default: none]
  -t TIME_TO_SLEEP, --time_to_sleep=TIME_TO_SLEEP
                        time to wait between calls to status_callback_f (in
                        seconds) [default: 3]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f CHECK_RUN_COMPLETE_FILE, --check_run_complete_file=CHECK_RUN_COMPLETE_FILE
                        path to file containing a list of files that must
                        exist to declare a run complete [REQUIRED]
Usage: principal_coordinates.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Principal Coordinate Analysis (PCoA) is commonly used to compare groups of samples based on phylogenetic or count-based distance metrics (see section on beta_diversity.py).

Example usage: 
Print help message and exit
 principal_coordinates.py -h

PCoA (Single File): For this script, the user supplies a distance matrix (i.e. resulting file from beta_diversity.py), along with the output filename (e.g.  beta_div_coords.txt), as follows
 principal_coordinates.py -i beta_div.txt -o beta_div_coords.txt

PCoA (Multiple Files): The script also functions in batch mode if a folder is supplied as input (e.g. from beta_diversity.py run in batch). No other files should be present in the input folder - only the distance matrix files to be analyzed. This script operates on every distance matrix file in the input directory and creates a corresponding principal coordinates results file in the output directory, e.g.
 principal_coordinates.py -i beta_div_weighted_unifrac/ -o beta_div_weighted_pcoa_results/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        path to the input distance matrix file(s) (i.e., the
                        output from beta_diversity.py). Is a directory for
                        batch processing and a filename for a single file
                        operation. [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path. directory for batch processing, filename
                        for single file operation [REQUIRED]
Usage: print_metadata_stats.py [options] {-m/--mapping_file MAPPING_FILE -c/--category CATEGORY}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Sum up the number of samples with each category value and print this information.

Example usage: 
Print help message and exit
 print_metadata_stats.py -h

Example: Count the number of samples associated with Treatment
 print_metadata_stats.py -m $PWD/mapping.txt -c Treatment

Example writting the output to a file: Count the number of samples associated with Treatment and save them to a file called stats.txt
 print_metadata_stats.py -m mapping.txt -c Treatment -o stats.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        path where output will be written [default: print to
                        screen]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FILE, --mapping_file=MAPPING_FILE
                        the input metadata file [REQUIRED]
    -c CATEGORY, --category=CATEGORY
                        the category to examine [REQUIRED]
Usage: print_qiime_config.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Print QIIME configuration details and optionally perform tests of the QIIME base or full install.

Example usage: 
Print help message and exit
 print_qiime_config.py -h

Example 1: Print basic QIIME configuration details
 print_qiime_config.py

Example 2: Print basic QIIME configuration details and test the base QIIME installation
 print_qiime_config.py -t

Example 3: Print basic QIIME configuration details and test the full QIIME installation
 print_qiime_config.py -tf

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -t, --test            Test the QIIME install and configuration [default:
                        False]
  -f, --qiime_full_install
                        If passed, report on dependencies required for the
                        QIIME full install. To perform tests of the QIIME full
                        install, you must also pass -t. [default: False]
Usage: process_iseq.py [options] {-i/--input_fps INPUT_FPS -o/--output_dir OUTPUT_DIR -b/--barcode_length BARCODE_LENGTH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 process_iseq.py -h

Generate fastq files from lanes 1 and 2 (read 1 data) where barcodes are contained as the first tweleve bases of the sequences.
 process_qseq.py -i ./s_1_1_sequence.txt,./s_2_1_sequence.txt -b 12 -o ./fastq/

Generate fastq files from the gzipped lanes 1 and 2 (read 1 data) where barcodes are contained as the first tweleve bases of the sequences.
 process_qseq.py -i ./s_1_1_sequence.txt.gz,./s_2_1_sequence.txt.gz -b 12 -o ./fastq/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --barcode_in_header   pass if barcode is in the header index field (rather
                        than at the beginning of the sequence)
  --barcode_qual_c=BARCODE_QUAL_C
                        if no barcode quality string is available, score each
                        base with this quality [default: b]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FPS, --input_fps=INPUT_FPS
                        the input filepaths (either iseq or gzipped iseq
                        format; comma-separated if more than one). See
                        Processing Illumina Data tutorial for a description of
                        the iseq file type. [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
    -b BARCODE_LENGTH, --barcode_length=BARCODE_LENGTH
                        length of the barcode [REQUIRED]
Usage: process_qseq.py [options] {-i/--input_dir INPUT_DIR -o/--output_dir OUTPUT_DIR -r/--read READ}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 process_qseq.py -h

Generate fastq files from all lanes of read 1 data in the current directory.
 process_qseq.py -i ./ -o ./fastq/ -r 1

Generate fastq files from all lanes of read 2 data in the current directory, truncating the sequences after the first 12 bases.
 process_qseq.py -i ./ -o ./fastq/ -r 2 -b 12

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -l LANES, --lanes=LANES
                        the lane numbers to consider, comma-separated [defaut:
                        1,2,3,4,5,6,7,8]
  -b BASES, --bases=BASES
                        the number of bases to include (useful for slicing a
                        barcode) [defaut: all]
  --ignore_pass_filter  ignore the illumina pass filter [default:False; reads
                        with 0 in  pass filter field are discarded]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        the input directory [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
    -r READ, --read=READ
                        the read number to consider [REQUIRED]
Usage: process_sff.py [options] {-i/--input_dir INPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script converts a directory of sff files into FASTA, QUAL and flowgram files.


Example usage: 
Print help message and exit
 process_sff.py -h

Simple example: Convert all the sffs in directory "sffs/" to fasta and qual.
 process_sff.py -i sffs/

Convert a single sff to fasta and qual.
 process_sff.py -i sffs/test.sff

Flowgram example: Convert all the sffs in directory "sffs/" to fasta and qual, along with a flowgram file.
 process_sff.py -i sffs/ -f

Convert a single sff to fasta and qual, along with a flowgram file.
 process_sff.py -i sffs/test.sff -f

Output example: Convert all the sffs in directory "sffs/" to fasta and qual, along with a flowgram file and write them to another directory.
 process_sff.py -i sffs/ -f -o output_dir

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --no_trim             do not trim sequence/qual (requires --use_sfftools
                        option) [default: False]
  -f, --make_flowgram   generate a flowgram file. [default: False]
  -t, --convert_to_FLX  convert Titanium reads to FLX length. [default: False]
  --use_sfftools        use the external programs sfffile and sffinfo for
                        processing, instead of the equivalent python
                        implementation
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Input directory of sff files [default: same as input
                        dir]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DIR, --input_dir=INPUT_DIR
                        Input directory of sff files or a single sff filepath
                        [REQUIRED]
Usage: quality_scores_plot.py [options] {-q/--qual_fp QUAL_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Two plots are generated by this module.
The first shows line plots indicating the average and standard deviations
for the quality scores of the input quality score file,
starting with the first nucleotide and ending with the the final
nucleotide of the largest sequence.

A second histogram shows a line plot with the nucleotide count for each
position, so that one may easily visualize how sequence length drops off.

A dotted line shows the cut-off point for a score to be acceptable (default
is 25).

A text file logging the average, standard deviation, and base count
for each base position is also generated.  These three sections are comma
separated.

The truncate_fasta_qual_files.py module can be used to create truncated
versions of the input fasta and quality score files.  By using this module
to assess the beginning of poor quality base calls, one can determine
the base position to begin truncating sequences at.

Example usage: 
Print help message and exit
 quality_scores_plot.py -h

Example: Generate plots and output to the quality_histograms folder
 quality_scores_plot.py -q seqs.qual -o quality_histograms/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Output directory.  Will be created if does not exist.
                        [default: .]
  -s SCORE_MIN, --score_min=SCORE_MIN
                        Minimum quality score to be considered acceptable.
                        Used to draw dotted line on histogram for easy
                        visualization of poor quality scores. [default: 25]
  -v, --verbose         Turn on this flag to disable verbose output.
                        [default: True]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -q QUAL_FP, --qual_fp=QUAL_FP
                        Quality score file used to generate histogram data.
                        [REQUIRED]
Usage: relatedness.py [options] {-t/--tree_fp TREE_FP -g/--taxa_fp TAXA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script calculates NRI and NTI from a path to a Newick formatted tree and a path to a comma separated list of ids in that tree that form the group whose NRI/NTI you want to test. The tree is not required to have distances. If none are found script will use the number of nodes (self inclusive) as their distance from one another. NRI and NTI are calculated as described in the Phylocom manual (which is a slightly modified version of that found in Webb 2002, and Webb 2000). The Phylocom manual is freely available on the web and Webb 2002 can be found in the Annual Review of Ecology and Systematics: Phylogenies and Community Ecology Webb 2002.

Example usage: 
Print help message and exit
 relatedness.py -h

Calculate both NRI and NTI from the given tree and group of taxa: 
 relatedness.py -t reference.tre -g group1_otus.txt -m nri,nti

Calculate only NRI: 
 relatedness.py -t reference.tre -g group1_otus.txt -m nri

Calculate only NTI using a different number of iterations: 
 relatedness.py -t reference.tre -g group1_otus.txt -m nti -i 100

Calculate only NTI using a different number of iterations and save the results into a file called output.txt: 
 relatedness.py -t reference.tre -g group1_otus.txt -m nti -i 100 -o output.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -i ITERS, --iters=ITERS
                        number of iterations to use for sampling tips without
                        replacement (null model 2 community sampling, see http
                        ://bodegaphylo.wikispot.org/Community_Phylogenetics
                        [default: 1000]
  -m METHODS, --methods=METHODS
                        comma-separated list of metrics to calculate.
                        [default: nri,nti]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        path where output will be written [default: print to
                        screen]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -t TREE_FP, --tree_fp=TREE_FP
                        the tree filepath [REQUIRED]
    -g TAXA_FP, --taxa_fp=TAXA_FP
                        taxa list filepath [REQUIRED]
Usage: shared_phylotypes.py [options] {-i/--otu_table_fp OTU_TABLE_FP -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script computes from an OTU table a matrix with the number of shared phylotypes between all pairs of samples.

Example usage: 
Print help message and exit
 shared_phylotypes.py -h

Single example: Compute shared OTUs on one OTU table for all samples
 shared_phylotypes.py -i otu_table.biom -o shared_otus.txt

Reference sample example: Compute shared OTUs with respect to a reference sample. Computes shared OTUs between all pairs of samples and the reference sample. E.g. in a transplant study this can be used to establish a base line count of shared OTUs with the Donor sample before and after the transplant.
 shared_phylotypes.py -i otu_table.biom -o shared_otus_PC.636.txt -r PC.636

Batch mode example: Compute shared OTUs for a set of OTU tables, e.g. from running multiple_rarefactions.py, with an even number of sequences per sample. The resulting directory can be fed to dissimilarity_mtx_stats.py, which computes mean, median and the standard deviation on the provided tables.
 shared_phylotypes.py -i rarefied_otu_tables/ -o shared_otus/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -r REFERENCE_SAMPLE, --reference_sample=REFERENCE_SAMPLE
                        Name of reference sample to which all pairs of samples
                        should be compared [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        path to the input OTU table in biom format or a
                        directory containing OTU tables [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath [REQUIRED]
Usage: simsam.py [options] {-i/--otu_table OTU_TABLE -t/--tree_file TREE_FILE -o/--output_dir OUTPUT_DIR -d/--dissim DISSIM -n/--num NUM}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script makes n samples related to each sample in an input otu table 
An input OTU table with 3 samples and n=2 will result in an output OTU table with 6 samples total: 3 clusters of 2 related samples.
To simulate each of the new samples, this script uses a sample in the input OTU table, and for each OTU in that sample the script traverses rootward on the tree a distance specified by '-d' to a point x. It then randomly selects a tip that decends from x, (call that new tip 'o2'), and reassigns all observations of the original OTU to the tip/OTU 'o2'.

Example usage: 
Print help message and exit
 simsam.py -h

Create an OTU table with 3 related samples for each sample in otu_table.biom with dissimilarities of 0.001.
 simsam.py -i otu_table.biom -t rep_set.tre -o simsam_out1 -d .001 -n 3

Create OTU tables with 2, 3 and 4 related samples for each sample in otu_table.biom with dissimilarities of 0.001 and 0.01. Additionally create new mapping files with metadata for each of the new samples for use in downstream analyses.
 simsam.py -i otu_table.biom -t rep_set.tre -o simsam_out2 -d .001,.01 -n 2,3,4 -m map.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        the mapping filepath. If provided, an output mapping
                        file containing the replicated sample IDs (with all
                        other metadata columns copied over) will also be
                        created [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE, --otu_table=OTU_TABLE
                        the input otu table [REQUIRED]
    -t TREE_FILE, --tree_file=TREE_FILE
                        tree file [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory [REQUIRED]
    -d DISSIM, --dissim=DISSIM
                        dissimilarity between nodes up the tree, as a single
                        value or comma-separated list of values [REQUIRED]
    -n NUM, --num=NUM   number of simulated samples per input sample, as a
                        single value or comma-separated list of values
                        [REQUIRED]
Usage: single_rarefaction.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH -d/--depth DEPTH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

To perform bootstrap, jackknife, and rarefaction analyses, the otu table must be subsampled (rarefied).  This script rarefies, or subsamples, an OTU table.  This does not provide curves of diversity by number of sequences in a sample. Rather it creates a subsampled OTU table by random sampling (without replacement) of the input OTU table.  Samples that have fewer sequences then the requested rarefaction depth are omitted from the ouput otu tables.  The pseudo-random number generator used for rarefaction by subsampling is NumPy's default - an implementation of the Mersenne twister PRNG.

Example usage: 
Print help message and exit
 single_rarefaction.py -h

Example: subsample otu_table.biom (-i) at 100 seqs/sample (-d), write results to otu_table_even100.txt (-o).
 single_rarefaction.py -i otu_table.biom -o otu_table_even100.biom -d 100

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --lineages_included=LINEAGES_INCLUDED
                        Deprecated: lineages are now included by default. Pass
                        --supress_lineages_included to prevent output OTU
                        tables from including taxonomic (lineage) information
                        for each OTU. Note: this will only work if lineage
                        information is in the input OTU table.
  --suppress_lineages_included
                        Exclude taxonomic (lineage) information for each OTU.
  -k, --keep_empty_otus
                        Retain OTUs of all zeros, which are usually omitted
                        from the output OTU tables. [default: False]
  --subsample_multinomial
                        subsample using subsampling with replacement [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        Input OTU table filepath. [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        Output OTU table filepath. [REQUIRED]
    -d DEPTH, --depth=DEPTH
                        Number of sequences to subsample per sample.
                        [REQUIRED]
Usage: sort_otu_table.py [options] {-i/--input_otu_table INPUT_OTU_TABLE -o/--output_fp OUTPUT_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 sort_otu_table.py -h

Default: case insensitive natural sorting i.e. SAMP0, samp1, SAMP2, samp10, samp12
 sort_otu_table.py -i otu_table.biom -o sorted_otu_table.biom

sort samples by the age field in the mapping file
 sort_otu_table.py -i otu_table.biom -o dob_sorted_otu_table.biom -m Fasting_Map.txt -s DOB

sort samples based on order in a file where each line starts with a sample id
 sort_otu_table.py -i otu_table.biom -o sorted_otu_table.biom -l sample_id_list.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        Input metadata mapping filepath. [default: none]
  -s SORT_FIELD, --sort_field=SORT_FIELD
                        Category to sort OTU table by. [default: none]
  -l SORTED_SAMPLE_IDS_FP, --sorted_sample_ids_fp=SORTED_SAMPLE_IDS_FP
                        Sorted sample id filepath [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_OTU_TABLE, --input_otu_table=INPUT_OTU_TABLE
                        Input OTU table filepath in BIOM format. [REQUIRED]
    -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        Output OTU table filepath. [REQUIRED]
Usage: split_libraries.py [options] {-m/--map MAP_FNAME -f/--fasta FASTA_FNAMES}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Since newer sequencing technologies provide many reads per run (e.g. the 454 GS FLX Titanium series can produce 400-600 million base pairs with 400-500 base pair read lengths) researchers are now finding it useful to combine multiple samples into a single 454 run. This multiplexing is achieved through the application of a pyrosequencing-tailored nucleotide barcode design (described in (Parameswaran et al., 2007)). By assigning individual, unique sample specific barcodes, multiple sequencing runs may be performed in parallel and the resulting reads can later be binned according to sample. The script split_libraries.py performs this task, in addition to several quality filtering steps including user defined cut-offs for: sequence lengths; end-trimming; minimum quality score. To summarize, by using the fasta, mapping, and quality files, the program split_libraries.py will parse sequences that meet user defined quality thresholds and then rename each read with the appropriate Sample ID, thus formatting the sequence data for downstream analysis. If a combination of different sequencing technologies are used in any particular study, split_libraries.py can be used to perform the quality-filtering for each library individually and the output may then be combined.

Sequences from samples that are not found in the mapping file (no corresponding barcode) and sequences without the correct primer sequence will be excluded. Additional scripts can be used to exclude sequences that match a given reference sequence (e.g. the human genome; exclude_seqs_by_blast.py) and/or sequences that are flagged as chimeras (identify_chimeric_seqs.py).


Example usage: 
Print help message and exit
 split_libraries.py -h

Standard Example: Using a single 454 run, which contains a single FASTA, QUAL, and mapping file while using default parameters and outputting the data into the Directory "Split_Library_Output"
 split_libraries.py -m Mapping_File.txt -f 1.TCA.454Reads.fna -q 1.TCA.454Reads.qual -o Split_Library_Output/

Multiple FASTA and QUAL Files Example: For the case where there are multiple FASTA and QUAL files, the user can run the following comma-separated command as long as there are not duplicate barcodes listed in the mapping file
 split_libraries.py -m Mapping_File.txt -f 1.TCA.454Reads.fna,2.TCA.454Reads.fna -q 1.TCA.454Reads.qual,2.TCA.454Reads.qual -o Split_Library_Output_comma_separated/

Duplicate Barcode Example: An example of this situation would be a study with 1200 samples. You wish to have 400 samples per run, so you split the analysis into three runs and reuse barcoded primers (you only have 600). After initial analysis you determine a small subset is underrepresented (<500 sequences per samples) and you boost the number of sequences per sample for this subset by running a fourth run. Since the same sample IDs are in more than one run, it is likely that some sequences will be assigned the same unique identifier by split_libraries.py when it is run separately on the four different runs, each with their own barcode file. This will cause a problem in file concatenation of the four different runs into a single large file. To avoid this, you can use the '-n' parameter which defines a start index for split_libraries.py. From experience, most FLX runs (when combining both files for a single plate) will have 350,000 to 650,000 sequences. Thus, if Run 1 for split_libraries.py uses '-n 1000000', Run 2 uses '-n 2000000', etc., then you are guaranteed to have unique identifiers after concatenating the results of multiple FLX runs. With newer technologies you will just need to make sure that your start index spacing is greater than the potential number of sequences.

To run split_libraries.py, you will need two or more (depending on the number of times the barcodes were reused) separate mapping files (one for each Run, for example one for Run1 and another one for Run2), then you can run split_libraries.py using the FASTA and mapping file for Run1 and FASTA and mapping file for Run2. Once you have run split libraries on each file independently, you can concatenate (e.g. using the 'cat' command) the sequence files that were generated by split_libraries.py. You can also concatenate the mapping files, since the barcodes are not necessary for downstream analyses, unless the same sample IDs are found in multiple mapping files.

Run split_libraries.py on Run 1
 split_libraries.py -m Mapping_File.txt -f 1.TCA.454Reads.fna -q 1.TCA.454Reads.qual -o Split_Library_Run1_Output/ -n 1000000

Run split_libraries.py on Run 2. The resulting FASTA files from Run 1 and Run 2 can then be concatenated using the 'cat' command (e.g. cat Split_Library_Run1_Output/seqs.fna Split_Library_Run2_Output/seqs.fna > Combined_seqs.fna) and used in downstream analyses.
 split_libraries.py -m Mapping_File.txt -f 2.TCA.454Reads.fna -q 2.TCA.454Reads.qual -o Split_Library_Run2_Output/ -n 2000000

Barcode Decoding Example: The standard barcode types supported by split_libraries.py are golay (Length: 12 NTs) and hamming (Length: 8 NTs). For situations where the barcodes are of a different length than golay and hamming, the user can define a generic barcode type "-b" as an integer, where the integer is the length of the barcode used in the study.

Note: When analyzing large datasets (>100,000 seqs), users may want to use a generic barcode type, even for length 8 and 12 NTs, since the golay and hamming decoding processes can be computationally intensive, which causes the script to run slow. Barcode correction can be disabled with the -c option if desired.

For the case where the 8 base pair barcodes were used, you can use the following command
 split_libraries.py -m Mapping_File_8bp_barcodes.txt -f 1.TCA.454Reads.fna  -q 1.TCA.454Reads.qual -o split_Library_output_8bp/ -b 8

Linkers and Primers: The linker and primer sequence (or all the degenerate possibilities) are associated with each barcode from the mapping file. If a barcode cannot be identified, all the possible primers in the mapping file are tested to find a matching sequence. Using truncated forms of the same primer can lead to unexpected results for rare circumstances where the barcode cannot be identified and the sequence following the barcode matches multiple primers.

In many cases, sequence reads are long enough to sequence through the reverse primer and sequencing adapter.  To remove these primers and all following sequences, the -z option can be used.  By default, this option is set to 'disable'.  If it is set to 'truncate_only', split_libraries will trim the primer and any sequence following it if the primer is found.  If the 'truncate_remove' option is set, split_libraries.py will trim the primer if found, and will not write the sequence if the primer is not found. The allowed mismatches for the reverse primer are set with the --reverse_primer_mismatches parameter (default 0).  To use reverse primer removal, one must include a 'ReversePrimer' column in the mapping file, with the reverse primer recorded in the 5' to 3' orientation.

Example reverse primer removal, where primers are trimmed if found, and sequence is written unchanged if not found.  Mismatches are increased to 1 from the default 0
 split_libraries.py -m Mapping_File_reverse_primer.txt -f 1.TCA.454Reads.fna -q 1.TCA.454Reads.qual -o split_libraries_output_revprimer/ --reverse_primer_mismatches 1 -z truncate_only

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -q QUAL_FNAMES, --qual=QUAL_FNAMES
                        names of qual files, comma-delimited [default: none]
  -r, --remove_unassigned
                        DEPRECATED: pass --retain_unassigned_reads to keep
                        unassigned reads  [default: none]
  -l MIN_SEQ_LEN, --min_seq_length=MIN_SEQ_LEN
                        minimum sequence length, in nucleotides [default: 200]
  -L MAX_SEQ_LEN, --max_seq_length=MAX_SEQ_LEN
                        maximum sequence length, in nucleotides [default:
                        1000]
  -t, --trim_seq_length
                        calculate sequence lengths after trimming primers and
                        barcodes [default: False]
  -s MIN_QUAL_SCORE, --min_qual_score=MIN_QUAL_SCORE
                        min average qual score allowed in read [default: 25]
  -k, --keep_primer     do not remove primer from sequences
  -B, --keep_barcode    do not remove barcode from sequences
  -a MAX_AMBIG, --max_ambig=MAX_AMBIG
                        maximum number of ambiguous bases [default: 6]
  -H MAX_HOMOPOLYMER, --max_homopolymer=MAX_HOMOPOLYMER
                        maximum length of homopolymer run [default: 6]
  -M MAX_PRIMER_MM, --max_primer_mismatch=MAX_PRIMER_MM
                        maximum number of primer mismatches [default: 0]
  -b BARCODE_TYPE, --barcode_type=BARCODE_TYPE
                        barcode type, hamming_8, golay_12, variable_length
                        (will disable any barcode correction if
                        variable_length set), or a number representing the
                        length of the barcode, such as -b 4.  [default:
                        golay_12]
  -o DIR_PREFIX, --dir_prefix=DIR_PREFIX
                        directory prefix for output files [default: .]
  -e MAX_BC_ERRORS, --max_barcode_errors=MAX_BC_ERRORS
                        maximum number of errors in barcode [default: 1.5]
  -n START_INDEX, --start_numbering_at=START_INDEX
                        seq id to use for the first sequence [default: 1]
  --retain_unassigned_reads
                        retain sequences which are Unassigned in the output
                        sequence file[default: False]
  -c, --disable_bc_correction
                        Disable attempts to find nearest corrected barcode.
                        Can improve performance. [default: False]
  -w QUAL_SCORE_WINDOW, --qual_score_window=QUAL_SCORE_WINDOW
                        Enable sliding window test of quality scores.  If the
                        average score of a continuous set of w nucleotides
                        falls below the threshold (see -s for default), the
                        sequence is discarded. A good value would be 50. 0
                        (zero) means no filtering. Must pass a .qual file (see
                        -q parameter) if this functionality is enabled.
                        Default behavior for this function is to truncate the
                        sequence at the beginning of the poor quality window,
                        and test for minimal length (-l parameter) of the
                        resulting sequence. [default: 0]
  -g, --discard_bad_windows
                        If the qual_score_window option (-w) is enabled, this
                        will override the default truncation behavior and
                        discard any sequences where a bad window is found.
                        [default: False]
  -p, --disable_primers
                        Disable primer usage when demultiplexing.  Should be
                        enabled for unusual circumstances, such as analyzing
                        Sanger sequence data generated with different primers.
                        [default: False]
  -z REVERSE_PRIMERS, --reverse_primers=REVERSE_PRIMERS
                        Enable removal of the reverse primer and any
                        subsequence sequence from the end of each read.  To
                        enable this, there has to be a "ReversePrimer" column
                        in the mapping file. Primers a required to be in IUPAC
                        format and written in the 5' to  3' direction.  Valid
                        options are 'disable', 'truncate_only', and
                        'truncate_remove'.  'truncate_only' will remove the
                        primer and subsequent sequence data from the output
                        read and will not alter output of sequences where the
                        primer cannot be found. 'truncate_remove' will flag
                        sequences where the primer cannot be found to not be
                        written and will record the quantity of such failed
                        sequences in the log file. [default: disable]
  --reverse_primer_mismatches=REVERSE_PRIMER_MISMATCHES
                        Set number of allowed mismatches for reverse primers
                        (option -z). [default: 0]
  -d, --record_qual_scores
                        Enables recording of quality scores for all sequences
                        that are recorded.  If this option is enabled, a file
                        named seqs_filtered.qual will be created in the output
                        directory, and will contain the same sequence IDs in
                        the seqs.fna file and sequence quality scores matching
                        the bases present in the seqs.fna file. [default:
                        False]
  -i MEDIAN_LENGTH_FILTERING, --median_length_filtering=MEDIAN_LENGTH_FILTERING
                        Disables minimum and maximum sequence length
                        filtering, and instead calculates the median sequence
                        length and filters the sequences based upon the number
                        of median absolute deviations specified by this
                        parameter.  Any sequences with lengths outside the
                        number of deviations will be removed. [default: none]
  -j ADDED_DEMULTIPLEX_FIELD, --added_demultiplex_field=ADDED_DEMULTIPLEX_FIELD
                        Use -j to add a field to use in the mapping file as an
                        additional demultiplexing option to the barcode.  All
                        combinations of barcodes and the values in these
                        fields must be unique. The fields must contain values
                        that can be parsed from the fasta labels such as
                        "plate=R_2008_12_09".  In this case, "plate" would be
                        the column header and "R_2008_12_09" would be the
                        field data (minus quotes) in the mapping file.  To use
                        the run prefix from the fasta label, such as
                        ">FLP3FBN01ELBSX", where "FLP3FBN01" is generated from
                        the run ID, use "-j run_prefix" and set the run prefix
                        to be used as the data under the column headerr
                        "run_prefix".  [default: none]
  -x, --truncate_ambi_bases
                        Enable to truncate at the first "N" character
                        encountered in the sequences.  This will disable
                        testing for ambiguous bases (-a option) [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAP_FNAME, --map=MAP_FNAME
                        name of mapping file. NOTE: Must contain a header line
                        indicating SampleID in the first column and
                        BarcodeSequence in the second, LinkerPrimerSequence in
                        the third. [REQUIRED]
    -f FASTA_FNAMES, --fasta=FASTA_FNAMES
                        names of fasta files, comma-delimited [REQUIRED]
Usage: split_libraries_fastq.py [options] {-i/--sequence_read_fps SEQUENCE_READ_FPS -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 split_libraries_fastq.py -h

Demultiplex and quality filter (at Phred >= Q20) one lane of Illumina fastq data and write results to ./slout_q20.: 
 split_libraries_fastq.py -i lane1_read1.fastq.gz -b lane1_barcode.fastq.gz --rev_comp_mapping_barcodes -o slout_q20/ -m map.txt -q 19

Demultiplex and quality filter (at Phred >= Q20) one lane of Illumina fastq data and write results to ./slout_q20. Store trimmed quality scores in addition to sequence data.: 
 split_libraries_fastq.py -i lane1_read1.fastq.gz -b lane1_barcode.fastq.gz --rev_comp_mapping_barcodes -o slout_q20/ -m map.txt --store_qual_scores -q 19

Demultiplex and quality filter (at Phred >= Q20) two lanes of Illumina fastq data and write results to ./slout_q20.: 
 split_libraries_fastq.py -i lane1_read1.fastq.gz,lane2_read1.fastq.gz -b lane1_barcode.fastq.gz,lane2_barcode.fastq.gz --rev_comp_mapping_barcodes -o slout_q20/ -m map.txt,map.txt --store_qual_scores -q 19

Quality filter (at Phred >= Q20) one non-multiplexed lane of Illumina fastq data and write results to ./slout_single_sample_q20.: 
 split_libraries_fastq.py -i lane1_read1.fastq.gz --sample_ids my.sample.1 -o slout_single_sample_q20/ -q 19 --barcode_type 'not-barcoded'

Quality filter (at Phred >= Q20) two non-multiplexed lanes of Illumina fastq data with different samples in each and write results to ./slout_not_multiplexed_q20.: 
 split_libraries_fastq.py -i lane1_read1.fastq.gz,lane2_read1.fastq.gz --sample_ids my.sample.1,my.sample.2 -o slout_not_multiplexed_q20/ -q 19 --barcode_type 'not-barcoded'

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m MAPPING_FPS, --mapping_fps=MAPPING_FPS
                        metadata mapping files (comma-separated if more than
                        one) [default: none]
  -b BARCODE_READ_FPS, --barcode_read_fps=BARCODE_READ_FPS
                        the barcode read fastq files (comma-separated if more
                        than one) [default: none]
  --store_qual_scores   store qual strings in .qual files [default: False]
  --sample_ids=SAMPLE_IDS
                        comma-separated list of samples ids to be applied to
                        all sequences, must be one per input file path (used
                        when data is not multiplexed) [default: none]
  --store_demultiplexed_fastq
                        write demultiplexed fastq files [default: False]
  --retain_unassigned_reads
                        retain sequences which don't map to a barcode in the
                        mapping file (sample ID will be "Unassigned")
                        [default: False]
  -r MAX_BAD_RUN_LENGTH, --max_bad_run_length=MAX_BAD_RUN_LENGTH
                        max number of consecutive low quality base calls
                        allowed before truncating a read [default: 3]
  -p MIN_PER_READ_LENGTH_FRACTION, --min_per_read_length_fraction=MIN_PER_READ_LENGTH_FRACTION
                        min number of consecutive high quality base calls to
                        include a read (per single end read) as a fraction of
                        the input read length [default: 0.75]
  -n SEQUENCE_MAX_N, --sequence_max_n=SEQUENCE_MAX_N
                        maximum number of N characters allowed in a sequence
                        to retain it -- this is applied after quality
                        trimming, and is total over combined paired end reads
                        if applicable [default: 0]
  -s START_SEQ_ID, --start_seq_id=START_SEQ_ID
                        start seq_ids as ascending integers beginning with
                        start_seq_id [default: 0]
  --rev_comp_barcode    reverse complement barcode reads before lookup
                        [default: False]
  --rev_comp_mapping_barcodes
                        reverse complement barcode in mapping before lookup
                        (useful if barcodes in mapping file are reverse
                        complements of golay codes) [default: False]
  --rev_comp            reverse complement sequence before writing to output
                        file (useful for reverse-orientation reads) [default:
                        False]
  -q PHRED_QUALITY_THRESHOLD, --phred_quality_threshold=PHRED_QUALITY_THRESHOLD
                        the maximum unacceptable Phred quality score (e.g.,
                        for Q20 and better, specify -q 19) [default: 3]
  --last_bad_quality_char=LAST_BAD_QUALITY_CHAR
                        DEPRECATED: use -q instead. This method of setting is
                        not robust to different versions of CASAVA.
  --barcode_type=BARCODE_TYPE
                        The type of barcode used. This can be an integer, e.g.
                        for length 6 barcodes, or "golay_12" for golay error-
                        correcting barcodes. Error correction will only be
                        applied for "golay_12" barcodes. If data is not
                        barcoded, pass "not-barcoded". [default: golay_12]
  --max_barcode_errors=MAX_BARCODE_ERRORS
                        maximum number of errors in barcode [default: 1.5]
  --phred_offset=PHRED_OFFSET
                        the ascii offset to use when decoding phred scores
                        (either 33 or 64). Warning: in most cases you don't
                        need to pass this value [default: determined
                        automatically]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i SEQUENCE_READ_FPS, --sequence_read_fps=SEQUENCE_READ_FPS
                        the sequence read fastq files (comma-separated if more
                        than one) [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory to store output files [REQUIRED]
Usage: split_libraries_lea_seq.py [options] {-i/--sequence_read_fps SEQUENCE_READ_FPS -o/--output_dir OUTPUT_DIR -m/--mapping_fp MAPPING_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


Implements Low-Error Amplicon Sequencing (LEA-Seq) method, described in:

Faith, Jeremiah J., et al.
The long-term stability of the human gut microbiota.Science 341.6141 (2013).

This method is based on redundant sequencing of a set of linear PCR template
extensions of 16S rRNA genes. The oligonucleotide primer that is used for
PCR template extensions is labeled with a random barcode
5' to the universal 16S rRNA primer sequence. This PCR pool is then
amplified with exponential PCR, using primers that specifically
amplify only the linear PCR molecules. An index primer is added to
the amplicons along with a primer specific for each sample.
This exponential PCR pool is then sequenced redundantly (20x coverage).
The resulting sequences are separated by sample, using the index sequence.
The amplicon sequences within each sample are separated by the random
barcodes. The large number of reads for each barcode helps to create an
error-corrected consensus sequence for the initial template molecule.


Example usage: 
Print help message and exit
 split_libraries_lea_seq.py -h

General Example: Specify forward read and reverse read fasta files, use the metadata mapping file map.txt,and output the data to output_dir: output_dir
 split_libraries_lea_seq.py -i fwd_read.fq,rev_read.fq -m map.txt -o output --b 7

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -b BARCODE_TYPE, --barcode_type=BARCODE_TYPE
                        the type of barcode used. This can be an integer, e.g.
                        6 for length 6 barcodes, or golay_12 for golay error-
                        correcting barcodes. Error correction will only be
                        applied for golay_12 barcodes [default: golay_12]
  --max_barcode_errors=MAX_BARCODE_ERRORS
                        the maximum allowable number of errors in the barcode
                        if passing --barcode_type golay_12 [default: 1.5]
  --min_consensus=MIN_CONSENSUS
                        threshold for consensus score: the minimum score
                        allowable at any position in sequence. where the score
                        is calulated as: occurence of base in consensus
                        sequence/ total sequences[default: 6.6]
  --max_cluster_ratio=MAX_CLUSTER_RATIO
                        threshold for cluster ratio: the maximum allowable
                        cluster ratio above which you need to find the
                        consensus sequence for the given sequences.[default:
                        2.5]
  --min_difference_in_bcs=MIN_DIFFERENCE_IN_BCS
                        threshold for selecting unique barcodes: Barcodes that
                        are more similar to each other than this value will be
                        discarded.[default: 0.86]
  --fwd_length=FWD_LENGTH
                        removes phasing from forward readby truncating it to
                        standard length for the region[default: 64]
  --rev_length=REV_LENGTH
                        removes phasing from reverse readby truncating it to
                        standard length for the region[default: 77]
  --min_difference_in_clusters=MIN_DIFFERENCE_IN_CLUSTERS
                        the percent identity threshold while using uclust to
                        cluster sequence reads, which is helpfulin measuring
                        quality of sequencing.[default: 0.98]
  --min_reads_per_random_bc=MIN_READS_PER_RANDOM_BC
                        minimum number of reads per randombarcode, attempts to
                        remove random barcodes that are sequencing errors of
                        true barcodesmight be useful in saving memory and
                        time[default: 1]
  --header_barcode_column=HEADER_BARCODE_COLUMN
                        header of barcode column[default: BarcodeSequence]
  --reverse_primer_column=REVERSE_PRIMER_COLUMN
                        header of reverse primer column[default:
                        ReversePrimer]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i SEQUENCE_READ_FPS, --sequence_read_fps=SEQUENCE_READ_FPS
                        the forward and reverse sequence read fastq files
                        (comma-separated) [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory to store output files [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        metadata mapping file [REQUIRED]
Usage: split_otu_table.py [options] {-i/--biom_table_fp BIOM_TABLE_FP -m/--mapping_fp MAPPING_FP -f/--fields FIELDS -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script splits a biom table based on the cartesian product of the values
found in the mapping fields specified. It accepts any number of mapping fields
to split on. As an example assume the following was your mapping file data:

SampleID       Color       Habitat       Age
S1             Red         Stream        10
S2             Blue        Stream        20
S3             Blue        Lake          30
S4             Red         Stream        30

If we wanted to split a corresponding biom table by the 'Color' and 'Habitat'
fields simultanesouly, we would return 3 biom tables with the following samples
corresponding to the following groups:

(S1, S4): (Red, Stream)
(S2): (Blue, Stream)
(S3): (Blue, Lake)

Combinations which would result in no samples -- in our example (Red, Lake) -- 
are excluded and do not produce (empty) biom tables. The script optionally
produces split mapping files as well. 

The naming convention for split files is (assuming two fields):

input_table.biom -> input_table__field1_value1_field2_value2__.biom
input_mapping.txt -> input_mapping__field1_value1_field2_value2__.txt

So, from our example above:

input_table.biom -> (input_table__Color_Red_Habitat_Stream__.biom,
                     input_table__Color_Blue_Habitat_Stream__.biom,
                     input_table__Color_Blue_Habitat_Lake__.biom)


Example usage: 
Print help message and exit
 split_otu_table.py -h

Split otu_table.biom into per-study OTU tables, and store the results in ./per_study_otu_tables/
 split_otu_table.py -i otu_table.biom -m Fasting_Map.txt -f Treatment -o per_study_otu_tables

Split otu_table.biom into multiple biom tables based on the Treatment and Color of the samples
 split_otu_table.py -i otu_table.biom -m Fasting_Map.txt -f Treatment,Color -o ./per_study_otu_tables/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --suppress_mapping_file_output
                        Do not write out split mapping files.

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i BIOM_TABLE_FP, --biom_table_fp=BIOM_TABLE_FP
                        The input biom table file path. [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        The mapping file path. [REQUIRED]
    -f FIELDS, --fields=FIELDS
                        Mapping columns to split biom table on, comma
                        separated. [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        File path to the output directory to be created.
                        [REQUIRED]
Usage: split_otu_table_by_taxonomy.py [options] {-i/--input_fp INPUT_FP -o/--output_dir OUTPUT_DIR -L/--level LEVEL}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)



Example usage: 
Print help message and exit
 split_otu_table_by_taxonomy.py -h

Split seqs_otu_table.biom into taxon-specific OTU tables based on the third level in the taxonomy, and write the taxon-specific OTU tables to ./L3/
 split_otu_table_by_taxonomy.py -i otu_table.biom -L 3 -o ./L3/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --md_identifier=MD_IDENTIFIER
                        the relevant observation metadat key [default:
                        taxonomy]
  --md_as_string        metadata is included as string [default: metadata is
                        included as list]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FP, --input_fp=INPUT_FP
                        the input otu table in BIOM format [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
    -L LEVEL, --level=LEVEL
                        the taxonomic level to split at [REQUIRED]
Usage: split_sequence_file_on_sample_ids.py [options] {-i/--input_seqs_fp INPUT_SEQS_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Split a single post-split_libraries.py fasta (or post-split_libraries_fastq.py fastq) file into per-sample fasta files. This script requires that the sequences identitifers are in post-split_libraries.py format (i.e., SampleID_SeqID). A file will be created for each unique SampleID.

Example usage: 
Print help message and exit
 split_sequence_file_on_sample_ids.py -h

Split seqs.fna into one fasta file per sample and store the resulting fasta files in 'out'
 split_sequence_file_on_sample_ids.py -i seqs.fna -o out/

Split seqs.fastq into one fastq file per sample and store the resulting fastq files in 'out_fastq'
 split_sequence_file_on_sample_ids.py -i seqs.fastq --file_type fastq -o out_fastq/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  --buffer_size=BUFFER_SIZE
                        the number of sequences to read into memory before
                        writing to file (you usually won't need to change
                        this) [default: 500]
  --file_type=FILE_TYPE
                        Type of file. Either fasta or fastq

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_SEQS_FP, --input_seqs_fp=INPUT_SEQS_FP
                        the input fasta file to split [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [default: none] [REQUIRED]
Usage: start_parallel_jobs.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script is designed to start multiple jobs in parallel on systems with no queueing system, for example a multiple processor or multiple core laptop/desktop machine. This also serves as an example 'cluster_jobs' which users can use as a template to define scripts to start parallel jobs in their environment.

Example usage: 
Print help message and exit
 start_parallel_jobs.py -h

Example: Start each command listed in test_jobs.txt in parallel. The run ID for these jobs will be RUNID.
 start_parallel_jobs.py -ms test_jobs.txt RUNID

Options:
  --version          show program's version number and exit
  -h, --help         show this help message and exit
  -v, --verbose      Print information during execution -- useful for
                     debugging [default: False]
  -m, --make_jobs    make the job files [default: none]
  -s, --submit_jobs  submit the job files [default: none]
Usage: start_parallel_jobs_sc.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Starts multiple jobs in parallel on Sun GridEngine systems. This is designed to work with StarCluster EC2 instances, but may be applicable beyond there.

Example usage: 
Print help message and exit
 start_parallel_jobs_sc.py -h

Job submission example: Start each command listed in test_jobs.txt in parallel. The run ID for these jobs will be RUNID.
 start_parallel_jobs_sc.py -ms test_jobs.txt RUNID

Queue specification example: Submit the commands listed in test_jobs.txt to the specified queue.
 start_parallel_jobs_sc.py -ms test_jobs.txt -q all.q RUNID

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m, --make_jobs       make the job files [default: none]
  -s, --submit_jobs     submit the job files [default: none]
  -q QUEUE_NAME, --queue_name=QUEUE_NAME
                        the queue to submit jobs to [default: all.q]
Usage: start_parallel_jobs_slurm.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script is designed to start multiple jobs in parallel on cluster systems with a slurm based scheduling system.

Example usage: 
Print help message and exit
 start_parallel_jobs_slurm.py -h

Job submission example: Start each command listed in test_jobs.txt in parallel. The run ID for these jobs will be RUNID.
 start_parallel_jobs_slurm.py -ms test_jobs.txt RUNID

Queue specification example: Submit the commands listed in test_jobs.txt to the specified queue.
 start_parallel_jobs_slurm.py -ms test_jobs.txt -q himem RUNID

Jobs output directory specification example: Submit the commands listed in test_jobs.txt, with the jobs put under the my_jobs/ directory.
 start_parallel_jobs_slurm.py -ms test_jobs.txt -j my_jobs/ RUNID

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m, --make_jobs       make the job files [default: False]
  -s, --submit_jobs     submit the job files [default: False]
  -q QUEUE, --queue=QUEUE
                        name of queue to submit to [default: slurm's default]
  -K MEM_PER_CPU, --mem_per_cpu=MEM_PER_CPU
                        megabytes of memory to request per CPU [default:
                        slurm's default]
  -j JOB_DIR, --job_dir=JOB_DIR
                        directory to store the jobs [default: jobs/]
  -t TIME, --time=TIME  run time limit of the jobs in dd-hh:mm:ss format
                        [default: slurm's default]
Usage: start_parallel_jobs_torque.py [options] {}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script is designed to start multiple jobs in parallel on cluster systems with a torque/qsub based scheduling system.

Example usage: 
Print help message and exit
 start_parallel_jobs_torque.py -h

Job submission example: Start each command listed in test_jobs.txt in parallel. The run ID for these jobs will be RUNID.
 start_parallel_jobs_torque.py -ms test_jobs.txt RUNID

Queue specification example: Submit the commands listed in test_jobs.txt to the specified queue.
 start_parallel_jobs_torque.py -ms test_jobs.txt -q friendlyq RUNID

Jobs output directory specification example: Submit the commands listed in test_jobs.txt, with the jobs put under the my_jobs/ directory.
 start_parallel_jobs_torque.py -ms test_jobs.txt -j my_jobs/ RUNID

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -m, --make_jobs       make the job files [default: none]
  -s, --submit_jobs     submit the job files [default: none]
  -q QUEUE, --queue=QUEUE
                        name of queue to submit to [default: friendlyq]
  -j JOB_DIR, --job_dir=JOB_DIR
                        directory to store the jobs [default: jobs/]
  -w MAX_WALLTIME, --max_walltime=MAX_WALLTIME
                        maximum time in hours the job will run for [default:
                        72]
  -c CPUS, --cpus=CPUS  number of CPUs to use [default:1]
  -n NODES, --nodes=NODES
                        number of nodes to use [default:1]
Usage: subsample_fasta.py [options] {-i/--input_fasta_fp INPUT_FASTA_FP -p/--percent_subsample PERCENT_SUBSAMPLE}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Subsample the seqs.fna file, randomly select 5% of the sequences:

Example usage: 
Print help message and exit
 subsample_fasta.py -h

Example: Subsample seqs.fasta to approximately 5%
 subsample_fasta.py -i $PWD/seqs.fna -p 0.05 -o $PWD/subsampled_seqs.fna

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_FP, --output_fp=OUTPUT_FP
                        the output filepath

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
    -p PERCENT_SUBSAMPLE, --percent_subsample=PERCENT_SUBSAMPLE
                        Specify the percentage (as a fraction between 0 and 1)
                        of sequences to subsample [REQUIRED]
summarize_otu_by_cat.py is no longer supported in QIIME. You should instead use collapse_samples.py.
Usage: summarize_taxa.py [options] {-i/--otu_table_fp OTU_TABLE_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The summarize_taxa.py script provides summary information of the representation of taxonomic groups within each sample. It takes an OTU table that contains taxonomic information as input. The taxonomic level for which the summary information is provided is designated with the -L option. The meaning of this level will depend on the format of the taxon strings that are returned from the taxonomy assignment step. The taxonomy strings that are most useful are those that standardize the taxonomic level with the depth in the taxonomic strings. For instance, the Greengenes database uses the following levels: Level 1 = Kingdom (e.g Bacteria), Level 2 = Phylum (e.g Actinobacteria), Level 3 = Class (e.g Actinobacteria), Level 4 = Order (e.g Actinomycetales), Level 5 = Family (e.g Streptomycetaceae), Level 6 = Genus (e.g Streptomyces), Level 7 = Species (e.g mirabilis). By default, the relative abundance of each taxonomic group will be reported, but the raw counts can be returned if -a is passed.

By default, taxa summary tables will be output in both classic (tab-separated) and BIOM formats. The BIOM-formatted taxa summary tables can be used as input to other QIIME scripts that accept BIOM files.

Example usage: 
Print help message and exit
 summarize_taxa.py -h

Examples: Summarize taxa based at taxonomic levels 2, 3, 4, 5, and 6, and write resulting taxa tables to the directory './tax'
 summarize_taxa.py -i otu_table.biom -o ./tax

Examples: Summarize taxa based at taxonomic levels 2, 3, 4, 5, and 6, and write resulting mapping files to the directory './tax'
 summarize_taxa.py -i otu_table.biom -o tax_mapping/ -m Fasting_Map.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -L LEVEL, --level=LEVEL
                        Taxonomic level to summarize by. [default: 2,3,4,5,6]
  -m MAPPING, --mapping=MAPPING
                        Input metadata mapping filepath. If supplied, then the
                        taxon information will be added to this file. This
                        option is useful for coloring PCoA plots by taxon
                        abundance or to perform statistical tests of
                        taxon/mapping associations.
  --md_identifier=MD_IDENTIFIER
                        the relevant observation metadata key [default:
                        taxonomy]
  --md_as_string        metadata is included as string [default: metadata is
                        included as list]
  -d DELIMITER, --delimiter=DELIMITER
                        Delimiter separating taxonomy levels. [default: ;]
  -a, --absolute_abundance
                        If present, the absolute abundance of the lineage in
                        each sample is reported. By default, this script uses
                        relative abundance [default: False]
  -l LOWER_PERCENTAGE, --lower_percentage=LOWER_PERCENTAGE
                        If present, OTUs having higher absolute abundance are
                        trimmed. To remove OTUs that make up more than 5% of
                        the total dataset you would pass 0.05. [default: none]
  -u UPPER_PERCENTAGE, --upper_percentage=UPPER_PERCENTAGE
                        If present, OTUs having lower absolute abundance are
                        trimmed. To remove the OTUs that makes up less than
                        45% of the total dataset you would pass 0.45.
                        [default: none]
  -t, --transposed_output
                        If present, the output will be written transposed from
                        the regular output. This is helpful in cases when you
                        want to use Site Painter to visualize your data
                        [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        path to the output directory
  --suppress_classic_table_output
                        If present, the classic (TSV) format taxon table will
                        not be created in the output directory. This option is
                        ignored if -m/--mapping is present [default: False]
  --suppress_biom_table_output
                        If present, the BIOM-formatted taxon table will not be
                        created in the output directory. This option is
                        ignored if -m/--mapping is present [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        Input OTU table filepath [REQUIRED]
Usage: summarize_taxa_through_plots.py [options] {-i/--otu_table_fp OTU_TABLE_FP -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)


The steps performed by this script are: Summarize OTU by Category (optional, pass -c); Summarize Taxonomy; and Plot Taxonomy Summary

Example usage: 
Print help message and exit
 summarize_taxa_through_plots.py -h

Plot taxa summaries for all samples: 
 summarize_taxa_through_plots.py -o taxa_summary -i otu_table.biom -m Fasting_Map.txt

Plot taxa summaries on a categorical basis: Alternatively, the user can supply a mapping_category, where the OTU is summarized based on a sample metadata category
 summarize_taxa_through_plots.py -o taxa_summary_by_treatment -i otu_table.biom -m Fasting_Map.txt -c Treatment

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p PARAMETER_FP, --parameter_fp=PARAMETER_FP
                        path to the parameter file, which specifies changes to
                        the default behavior. See
                        http://www.qiime.org/documentation/file_formats.html
                        #qiime-parameters. [if omitted, default values will be
                        used]
  -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        path to the mapping file [REQUIRED if passing -c]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  -w, --print_only      Print the commands but don't call them -- useful for
                        debugging [default: False]
  -c MAPPING_CATEGORY, --mapping_category=MAPPING_CATEGORY
                        Summarize OTU table using this category. [default:
                        none]
  -s, --sort            Sort the OTU Table [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i OTU_TABLE_FP, --otu_table_fp=OTU_TABLE_FP
                        the input otu table [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: supervised_learning.py [options] {-i/--input_data INPUT_DATA -m/--mapping_file MAPPING_FILE -c/--category CATEGORY -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script trains a supervised classifier using OTUs (or other continuous input sample x observation data) as predictors, and a mapping file column containing discrete values as the class labels.

Outputs:
    * cv_probabilities.txt: the label probabilities for each of the         given samples. (if available)
    * mislabeling.txt: A convenient presentation of cv_probabilities         for mislabeling detection.
    * confusion_matrix.txt: confusion matrix for hold-out predictions.
    * summary.txt: a summary of the results, including the expected         generalization error of the classifier
    * feature_importance_scores.txt: a list of discriminative OTUs with their         associated importance scores (if available)

It is recommended that you remove low-depth samples and rare OTUs before running this script. This can drastically reduce the run-time, and in many circumstances will not hurt performance. It is also recommended to perform rarefaction to control for sampling effort before running this script. For example, to rarefy at depth 200, then remove OTUs present in < 10 samples run:

single_rarefaction.py -i otu_table.biom -d 200 -o otu_table_rarefied200.biom
filter_otus_from_otu_table.py -i otu_table_rarefied200.biom -s 10 -o otu_table_rarefied200.present10.biom

For an overview of the application of supervised classification to microbiota, see PubMed ID 21039646.

This script also has the ability to collate the supervised learning results produced on an input directory. For example, in order to reduce any variation introduced through producing a rarefied OTU table, the user can run multiple_rarefactions_even_depth.py on the OTU table, and then pass that directory into supervised_learning.py. The user can then pass a -w collate_results filepath to produce a single results file that contains the average estimated generalization error of the classified, and the pooled standard deviation (for cv5 and cv10 errortypes).

This script requires that R be installed and in the search path. To install R visit: http://www.r-project.org/. Once R is installed, run R and excecute the command "install.packages("randomForest")", then type q() to exit.

Example usage: 
Print help message and exit
 supervised_learning.py -h

Simple example of random forests classifier: 
 supervised_learning.py -i otu_table.biom -m Fasting_Map.txt -c BarcodeSequence -o ml

Running with 10-fold cross-validation for improved estimates of generalization error and feature importances: 
 supervised_learning.py -i otu_table.biom -m Fasting_Map.txt -c BarcodeSequence -o ml_cv10 -e cv10

Running with 1,000 trees for improved generalization error: 
 supervised_learning.py -i otu_table.biom -m Fasting_Map.txt -c BarcodeSequence -o ml_ntree1000 --ntree 1000

Run 10-fold cross validation on a directory of OTU tables rarefied at an even depth: 
 supervised_learning.py -i rarefied_tables/ -m Fasting_Map.txt -c Treatment -o sl_rarefied_tables_cv10 -e cv10

Run 10-fold cross validation on a directory of OTU tables rarefied at an even depth and collate the results into a single file: 
 supervised_learning.py -i rarefied_tables/ -m Fasting_Map.txt -c Treatment -o sl_rarefied_tables_cv10_sweep -e cv10 -w sl_cv10_sweep.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -f, --force           Force overwrite of existing output directory (note:
                        existing files in output_dir will not be removed)
                        [default: none]
  --ntree=NTREE         Number of trees in forest (more is better but slower)
                        [default: 500]
  -e ERRORTYPE, --errortype=ERRORTYPE
                        type of error estimation. Valid choices are: oob, loo,
                        cv5, cv10. oob: out-of-bag, fastest, only builds one
                        classifier, use for quick estimates; cv5: 5-fold cross
                        validation, provides mean and standard deviation of
                        error, use for good estimates on very large data sets;
                        cv10: 10-fold cross validation, provides mean and
                        standard deviation of error, use for best estimates;
                        loo: leave-one-out cross validation, use for small
                        data sets (less than ~30-50 samples) [default oob]
  -w COLLATE_RESULTS_FP, --collate_results_fp=COLLATE_RESULTS_FP
                        When passing in a directory of OTU tables that are
                        rarefied at an even depth, this option will collate
                        the results into a single specified output file,
                        averaging the estimated errors and standard
                        deviations. [default: none]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_DATA, --input_data=INPUT_DATA
                        Input data file containing predictors (e.g. otu table)
                        or a directory of otu tables [REQUIRED]
    -m MAPPING_FILE, --mapping_file=MAPPING_FILE
                        File containing meta data (response variables)
                        [REQUIRED]
    -c CATEGORY, --category=CATEGORY
                        Name of meta data category to predict [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: transform_coordinate_matrices.py [options] {-i/--input_fps INPUT_FPS -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This script transforms two or more coordinate matrices (e.g., the output of principal_coordinates.py) using procrustes analysis to minimize the distances between corresponding points. The first coordinate matrix provided is treated as the reference, and all other coordinate matrices are transformed to minimize distances to the reference points. Monte Carlo simulations can additionally be performed (-r random trials are run) to estimate the probability of seeing an M^2 value as extreme as the actual M^2.

Example usage: 
Print help message and exit
 transform_coordinate_matrices.py -h

Write the transformed procrustes matrices to file: 
 transform_coordinate_matrices.py -i unweighted_unifrac_pc.txt,weighted_unifrac_pc.txt -o procrustes_output

Generate transformed procrustes matrices and monte carlo p-values for two principal coordinate matrices: 
 transform_coordinate_matrices.py -i unweighted_unifrac_pc.txt,weighted_unifrac_pc.txt -o mc_procrustes_output_2 -r 1000

Generate transformed procrustes matrices and monte carlo p-values for four principal coordinate matrices: 
 transform_coordinate_matrices.py -i unweighted_unifrac_pc.txt,weighted_unifrac_pc.txt,euclidean_pc.txt,bray_curtis_pc.txt -o mc_procrustes_output_4 -r 1000

Generate transformed procrustes matrices and monte carlo p-values for three principal coordinate matrices where the sample ids must be mapped between matrices: 
 transform_coordinate_matrices.py -i s1_pc.txt,s2_pc.txt,s3_pc.txt -s s1_s2_map.txt,s1_s3_map.txt -o mc_procrustes_output_3 -r 1000

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -r RANDOM_TRIALS, --random_trials=RANDOM_TRIALS
                        Number of random permutations of matrix2 to perform.
                        [default: (no Monte Carlo analysis performed)]
  -d NUM_DIMENSIONS, --num_dimensions=NUM_DIMENSIONS
                        Number of dimensions to include in output matrices
                        [default: 3]
  -s SAMPLE_ID_MAP_FPS, --sample_id_map_fps=SAMPLE_ID_MAP_FPS
                        If sample id maps are provided, there must be exactly
                        one fewer files here than there are coordinate
                        matrices (as each nth sample id map will provide the
                        mapping from the first input coordinate matrix to the
                        n+1th coordinate matrix) [default: none]
  --store_trial_details
                        Store PC matrices for individual trials [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FPS, --input_fps=INPUT_FPS
                        comma-separated list of input coordinate matrices
                        [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        the output directory [REQUIRED]
Usage: tree_compare.py [options] {-m/--master_tree MASTER_TREE -s/--support_dir SUPPORT_DIR -o/--output_dir OUTPUT_DIR}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Compares jackknifed/bootstrapped trees (support trees) with a master tree constructed typically from the entire dataset (e.g: a resulting file from upgma_cluster.py) and outputs support for nodes.

if support trees do not have all tips that master has (e.g. because samples with few sequences were dropped during a jackknifing analysis), the output master tree will have only those tips included in all support trees

if support trees have tips that the master tree does not, those tips will be ignored (removed from the support tree during analysis)

Example usage: 
Print help message and exit
 tree_compare.py -h

Example: Given the sample upgma tree generated by the user for the entire dataset, the directory of bootstrap/jackknife supported trees (e.g.: the resulting directory from upgma_cluster.py) and the directory to write the results for the tree comparisons, the following command compares the support trees with the master
 tree_compare.py -m input_master_tree.tre -s bootstrapped_trees/ -o output/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MASTER_TREE, --master_tree=MASTER_TREE
                        master tree filepath [REQUIRED]
    -s SUPPORT_DIR, --support_dir=SUPPORT_DIR
                        path to dir containing support trees [REQUIRED]
    -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        output directory, writes three files here makes dir if
                        it doesn't exist [REQUIRED]
Usage: trflp_file_to_otu_table.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

The input for this script is a TRLFP text file. The output of this script is an OTU table text file that can be use with QIIME for further analysis 

Example usage: 
Print help message and exit
 trflp_file_to_otu_table.py -h

Usage: You need to pass a TRFLP text, the script will remove not wanted chars sample and otus names, and will add zeros as need it
 trflp_file_to_otu_table.py -i trflp_in.txt -o otu_table.biom

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input path: TRFLP text file [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output file: OTU table [REQUIRED]
Usage: trim_sff_primers.py [options] {-l/--libdir LIBDIR -m/--input_map INPUT_MAP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Finds the technical read regions for each library, and resets the left trim.

Example usage: 
Print help message and exit
 trim_sff_primers.py -h

Simple example: Trim a directory of per-sff files in sff_dir (-l sff_dir/) using an input map (-m input_map.txt). This script uses the sff utility binaries which must be in your path.
 trim_sff_primers.py -l sff_dir/ -m input_map.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -p SFFFILE_PATH, --sfffile_path=SFFFILE_PATH
                        Path to sfffile binary [default: sfffile]
  -q SFFINFO_PATH, --sffinfo_path=SFFINFO_PATH
                        Path to sffinfo binary [default: sffinfo]
  --use_sfftools        Use external sffinfo and sfffile programs instead of
                        equivalent Python implementation.
  --debug               Print command-line output for debugging [default:
                        False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -l LIBDIR, --libdir=LIBDIR
                        The directory containing per-library sff files
                        [REQUIRED]
    -m INPUT_MAP, --input_map=INPUT_MAP
                        Path to the input mapping file describing the
                        libraries [REQUIRED]
Usage: truncate_fasta_qual_files.py [options] {-f/--fasta_fp FASTA_FP -q/--qual_fp QUAL_FP -b/--base_pos BASE_POS}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

This module is designed to remove regions of poor quality in
454 sequence data.  Drops in quality can be visualized with the
quality_scores_plot.py module.  The base position specified will
be used as an index to truncate the sequence and quality scores, and
all data at that base position and to the end of the sequence will be
removed in the output filtered files.

Example usage: 
Print help message and exit
 truncate_fasta_qual_files.py -h

Example: Truncate the input fasta and quality files at base position 100, output to the filtered_seqs directory
 truncate_fasta_qual_files.py -f seqs.fna -q seqs.qual -b 100 -o filtered_seqs/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Output directory.  Will be created if does not exist.
                        [default: .]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f FASTA_FP, --fasta_fp=FASTA_FP
                        Input fasta filepath to be truncated. [REQUIRED]
    -q QUAL_FP, --qual_fp=QUAL_FP
                        Input quality scores filepath to be truncated.
                        [REQUIRED]
    -b BASE_POS, --base_pos=BASE_POS
                        Nucleotide position to truncate the fasta and quality
                        score files at. [REQUIRED]
Usage: truncate_reverse_primer.py [options] {-f/--fasta_fp FASTA_FP -m/--mapping_fp MAPPING_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Takes input mapping file and fasta sequences which have already have been demultiplexed (via split_libraries.py, denoise_wrapper.py, ampliconnoise.py, etc.) with fasta labels that are in QIIME format, i.e., SampleID_#. This script will use the SampleID and a mapping file with a ReversePrimer column to find the reverse primer by local alignment and remove this and any subsequent sequence in a filtered output fasta file.

Example usage: 
Print help message and exit
 truncate_reverse_primer.py -h

Example: Find, truncate reverse primers from the fasta file seqs.fna, with the SampleIDs and reverse primers specified in Mapping_File_Rev_Primer.txt, writes output fasta file to the reverse_primer_removed directory
 truncate_reverse_primer.py -f seqs.fna -m Mapping_File_Rev_Primer.txt -o reverse_primer_removed/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Output directory.  Will be created if does not exist.
                        [default: .]
  -z TRUNCATE_OPTION, --truncate_option=TRUNCATE_OPTION
                        Truncation option.  The default option,
                        "truncate_only" will try to find the reverse primer to
                        truncate, and if not found, will write the sequence
                        unchanged.  If set to "truncate_remove", sequences
                        where the reverse primer is not found will not be
                        written. [default: truncate_only]
  -M PRIMER_MISMATCHES, --primer_mismatches=PRIMER_MISMATCHES
                        Number of mismatches allowed in the reverse primer.
                        [default: 2]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -f FASTA_FP, --fasta_fp=FASTA_FP
                        Fasta file.  Needs to have fasta labels in proper
                        demultiplexed format. [REQUIRED]
    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        Mapping filepath.  ReversePrimer field required.
                        Reverse primers need to be in 5'->3' orientation.
                        [REQUIRED]
Usage: unweight_fasta.py [options] {-i/--input_fasta INPUT_FASTA -o/--output_file OUTPUT_FILE -l/--label LABEL}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

E.g. makes 3 fasta records from a weighted input fasta file containing the following record:
>goodsample1_12_3 bc_val=20
AATGCTTGTCACATCGATGC


Example usage: 
Print help message and exit
 unweight_fasta.py -h

make 3 fasta records from the following record:
>goodsample1_12_3 bc_val=20
AATGCTTGTCACATCGATGC

resulting in:
>goodsample_0
AATGCTTGTCACATCGATGC
>goodsample_1
AATGCTTGTCACATCGATGC
>goodsample_2
AATGCTTGTCACATCGATGC
 unweight_fasta.py -i input.fna -o output.fna -l goodsample

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_FASTA, --input_fasta=INPUT_FASTA
                        the input fasta file [REQUIRED]
    -o OUTPUT_FILE, --output_file=OUTPUT_FILE
                        the output fasta filepath [REQUIRED]
    -l LABEL, --label=LABEL
                        sequence label used for all records. fasta label lines
                        will look like: >label_423 [REQUIRED]
Usage: upgma_cluster.py [options] {-i/--input_path INPUT_PATH -o/--output_path OUTPUT_PATH}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

In addition to using PCoA, it can be useful to cluster samples using UPGMA (Unweighted Pair Group Method with Arithmetic mean, also known as average linkage). As with PCoA, the input to this step is a distance matrix (i.e. resulting file from beta_diversity.py).

Example usage: 
Print help message and exit
 upgma_cluster.py -h

UPGMA Cluster (Single File): To perform UPGMA clustering on a single distance matrix (e.g.: beta_div.txt, a result file from beta_diversity.py) use the following idiom
 upgma_cluster.py -i $PWD/beta_div.txt -o $PWD/beta_div_cluster.tre

UPGMA Cluster (Multiple Files): The script also functions in batch mode if a folder is supplied as input. This script operates on every file in the input directory and creates a corresponding upgma tree file in the output directory, e.g.
 upgma_cluster.py -i $PWD/beta_div_folder -o $PWD/beta_div_folder_results/

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -i INPUT_PATH, --input_path=INPUT_PATH
                        input path.  directory for batch processing, filename
                        for single file operation [REQUIRED]
    -o OUTPUT_PATH, --output_path=OUTPUT_PATH
                        output path. directory for batch processing, filename
                        for single file operation [REQUIRED]
Usage: validate_demultiplexed_fasta.py [options] {-m/--mapping_fp MAPPING_FP -i/--input_fasta_fp INPUT_FASTA_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Checks file is a valid fasta file, does not contain gaps ('.' or '-' characters), contains only valid nucleotide characters, no fasta label is duplicated, SampleIDs match those in a provided mapping file, fasta labels are formatted to have SampleID_X as normally generated by QIIME demultiplexing, and the BarcodeSequence/LinkerPrimerSequences are not found in the fasta sequences.  Optionally this script can also verify that the SampleIDs in the fasta sequences are also present in the tip IDs of a provided newick tree file, can test for equal sequence lengths across all sequences, and can test that all SampleIDs in the mapping file are represented in the fasta file labels.

Example usage: 
Print help message and exit
 validate_demultiplexed_fasta.py -h

Example: 
 validate_demultiplexed_fasta.py -f seqs.fasta -m Mapping_File.txt

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -v, --verbose         Print information during execution -- useful for
                        debugging [default: False]
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        directory prefix for output files [default: .]
  -t TREE_FP, --tree_fp=TREE_FP
                        path to the tree file; Needed to test if sequence IDs
                        are a subset or exact match to the tree tips, options
                        -s and -e  [default: none]
  -s, --tree_subset     Determine if sequence IDs are a subset of the tree
                        tips, newick tree must be passed with the -t option.
                        [default: False]
  -e, --tree_exact_match
                        Determine if sequence IDs are an exact match to tree
                        tips, newick tree must be passed with the -t option.
                        [default: False]
  -l, --same_seq_lens   Determine if sequences are all the same length.
                        [default: False]
  -a, --all_ids_found   Determine if all SampleIDs provided in the mapping
                        file are represented in the fasta file labels.
                        [default: False]
  -b, --suppress_barcode_checks
                        Suppress barcode checks [default: False]
  -p, --suppress_primer_checks
                        Suppress primer checks [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        name of mapping file. NOTE: Must contain a header line
                        indicating SampleID in the first column and
                        BarcodeSequence in the second, LinkerPrimerSequence in
                        the third.  If no barcode or  linkerprimer sequence is
                        present, leave data fields empty. [REQUIRED]
    -i INPUT_FASTA_FP, --input_fasta_fp=INPUT_FASTA_FP
                        path to the input fasta file [REQUIRED]
Usage: validate_mapping_file.py [options] {-m/--mapping_fp MAPPING_FP}

[] indicates optional input (order unimportant)
{} indicates required input (order unimportant)

Specifically, we check that:

    - The BarcodeSequence, LinkerPrimerSequences, and ReversePrimer fields
       have valid IUPAC DNA characters, and BarcodeSequence characters
       are non-degenerate (error)
    - The SampleID, BarcodeSequence, LinkerPrimerSequence, and Description
       headers are present. (error)
    - There are not duplicate header fields (error)
    - There are not duplicate barcodes (error)
    - Barcodes are of the same length.  Suppressed when
       variable_len_barcode flag is passed (warning)
    - The headers do not contain invalid characters (alphanumeric and
       underscore only) (warning)
    - The data fields do not contain invalid characters (alphanumeric,
       underscore, space, and +-%./:,; characters) (warning)
    - SampleID fields are MIENS compliant (only alphanumeric
       and . characters). (warning)
    - There are no duplicates when the primer and variable length
       barcodes are appended (error)
    - There are no duplicates when barcodes and added demultiplex
       fields (-j option) are combined (error)
    - Data fields are not found beyond the Description column (warning)

    Details about the metadata mapping file format can be found here:
    http://www.qiime.org/documentation/file_formats.html#metadata-mapping-files

    Errors and warnings are saved to a log file.  Errors can be caused by
    problems with the headers, invalid characters in barcodes or primers, or
    by duplications in SampleIDs or barcodes.

    Warnings can arise from invalid characters and variable length barcodes that
    are not specified with the --variable_len_barcode.
    Warnings will contain a reference to the cell (row,column) that the
    warning arose from.

    In addition to the log file, a "corrected_mapping" file will be created.
    Any invalid characters will be replaced with '.' characters in
    the SampleID fields (to enforce MIENS compliance) and text in other data
    fields will be replaced with the character specified by the -c parameter,
    which is an underscore "_" by default.

    A html file will be created as well, which will show locations of
    warnings and errors, highlighted in yellow and red respectively.  If no
    errors or warnings were present the file will display a message saying
    such.  Header errors can mask other errors, so these should be corrected
    first.

    If pooled primers are used, separate with a comma.  For instance, a pooled
    set of three 27f primers (used to increase taxonomic coverage) could be
    specified in the LinkerPrimerSequence fields as such:
    AGGGTTCGATTCTGGCTCAG,AGAGTTTGATCCTGGCTTAG,AGAATTTGATCTTGGTTCAG


Example usage: 
Print help message and exit
 validate_mapping_file.py -h

Example: Check the Fasting_Map.txt     mapping file for problems, supplying the required mapping file, and output     the results in the validate_mapping_file_output directory
 validate_mapping_file.py -m     Fasting_Map.txt -o validate_mapping_file_output

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -o OUTPUT_DIR, --output_dir=OUTPUT_DIR
                        Required output directory for log file, corrected
                        mapping file, and html file. [default: ./]
  -v, --verbose         Enable printing information to standard out [default:
                        True]
  -c CHAR_REPLACE, --char_replace=CHAR_REPLACE
                        Changes the default character used to replace invalid
                        characters found in the mapping file.  Must be a valid
                        character (alphanumeric, period, or
                        underscore).[default: _]
  -b, --not_barcoded    Use -b if barcodes are not present.  BarcodeSequence
                        header still required.  [default: False]
  -B, --variable_len_barcodes
                        Use -B if variable length barcodes are present to
                        suppress warnings about barcodes of unequal length.
                        [default: False]
  -p, --disable_primer_check
                        Use -p to disable checks for primers.
                        LinkerPrimerSequence header still required. [default:
                        False]
  -j ADDED_DEMULTIPLEX_FIELD, --added_demultiplex_field=ADDED_DEMULTIPLEX_FIELD
                        Use -j to add a field to use in the mapping file as
                        additional demultiplexing (can be used with or without
                        barcodes).  All combinations of barcodes/primers and
                        the these fields must be unique. The fields must
                        contain values that can be parsed from the fasta
                        labels such as "plate=R_2008_12_09".  In this case,
                        "plate" would be the column header and "R_2008_12_09"
                        would be the field data (minus quotes) in the mapping
                        file.  To use the run prefix from the fasta label,
                        such as ">FLP3FBN01ELBSX", where "FLP3FBN01" is
                        generated from the run ID, use "-j run_prefix" and set
                        the run prefix to be used as the data under the column
                        header "run_prefix".  [default: none]
  -s, --suppress_html   Use -s to disable html file generation, can be useful
                        for extremely large mapping files. [default: False]

  REQUIRED options:
    The following options must be provided under all circumstances.

    -m MAPPING_FP, --mapping_fp=MAPPING_FP
                        Metadata mapping filepath [REQUIRED]

Resource usage statistics from testing qiime:
   Process count: 3
   CPU time: Sys=0:00:05.2, User=0:00:20.6
   Memory: 132.6M
   Disk usage: 36B
   Time elapsed: 0:02:00.3


TEST END: qiime-1.9.1-py_3.tar.bz2
--dirty flag and --keep-old-work not specified. Removing build/test folder after successful build/test.

